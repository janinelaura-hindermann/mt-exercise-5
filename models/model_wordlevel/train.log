2024-05-23 10:41:59,070 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                           cfg.name : wordlevel
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                     cfg.data.train : data_sampled/sampled_train.en-de
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev.en-de
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                      cfg.data.test : data/test.en-de
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                 cfg.data.src.level : word
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 2000
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : de
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                 cfg.data.trg.level : word
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 2000
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -             cfg.training.model_dir : ../models/model_wordlevel
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-23 10:41:59,071 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-23 10:41:59,072 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-23 10:41:59,074 - INFO - joeynmt.data - Building tokenizer...
2024-05-23 10:41:59,074 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2024-05-23 10:41:59,074 - INFO - joeynmt.tokenizers - de tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2024-05-23 10:41:59,074 - INFO - joeynmt.data - Loading train set...
2024-05-23 10:41:59,221 - INFO - joeynmt.data - Building vocabulary...
2024-05-23 10:42:00,114 - INFO - joeynmt.data - Loading dev set...
2024-05-23 10:42:00,117 - INFO - joeynmt.data - Loading test set...
2024-05-23 10:42:00,119 - INFO - joeynmt.data - Data loaded.
2024-05-23 10:42:00,119 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 10:42:00,119 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=888, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 10:42:00,120 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1568, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 10:42:00,120 - INFO - joeynmt.data - First training example:
	[SRC] Al Gore: Averting the climate crisis
	[TRG] Al Gore: Die Abwendung der Klimakatastrophe
2024-05-23 10:42:00,120 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) of (7) a (8) and (9) that
2024-05-23 10:42:00,120 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) die (5) und (6) der (7) das (8) in (9) zu
2024-05-23 10:42:00,120 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2004
2024-05-23 10:42:00,120 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2004
2024-05-23 10:42:00,120 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-23 10:42:00,170 - INFO - joeynmt.model - Enc-dec model built.
2024-05-23 10:42:00,172 - INFO - joeynmt.model - Total params: 3925248
2024-05-23 10:42:00,172 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2024-05-23 10:42:00,172 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-23 10:42:00,172 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-23 10:42:00,172 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-23 10:42:00,172 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-23 10:42:00,172 - INFO - joeynmt.training - EPOCH 1
2024-05-23 10:42:15,093 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.081140, Batch Acc: 0.254311, Tokens per Sec:     4318, Lr: 0.000300
2024-05-23 10:42:29,872 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.767057, Batch Acc: 0.293314, Tokens per Sec:     4481, Lr: 0.000300
2024-05-23 10:42:44,864 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.577559, Batch Acc: 0.312812, Tokens per Sec:     4249, Lr: 0.000300
2024-05-23 10:42:59,558 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.571854, Batch Acc: 0.330895, Tokens per Sec:     4476, Lr: 0.000300
2024-05-23 10:43:14,147 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.485184, Batch Acc: 0.343653, Tokens per Sec:     4492, Lr: 0.000300
2024-05-23 10:43:14,149 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:43:14,149 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:43:51,198 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.50, ppl:  12.14, acc:   0.34, generation: 37.0448[sec], evaluation: 0.0000[sec]
2024-05-23 10:43:51,200 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:43:51,451 - INFO - joeynmt.training - Example #0
2024-05-23 10:43:51,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:43:51,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:43:51,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'ich', '<unk>', '<unk>', 'ich', '<unk>', '<unk>', '<unk>', 'dass', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:43:51,451 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:43:51,451 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:43:51,451 - INFO - joeynmt.training - 	Hypothesis: <unk> ich <unk> <unk> ich <unk> <unk> <unk> dass die <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:43:51,451 - INFO - joeynmt.training - Example #1
2024-05-23 10:43:51,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:43:51,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:43:51,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'weil', 'es', 'es', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:43:51,451 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:43:51,451 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:43:51,451 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> <unk> <unk> <unk> <unk> weil es es <unk> <unk> <unk>
2024-05-23 10:43:51,452 - INFO - joeynmt.training - Example #2
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', '<unk>', 'in', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> <unk> in der <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:43:51,452 - INFO - joeynmt.training - Example #3
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', '<unk>', 'und', 'in', '<unk>', '<unk>', '</s>']
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Hypothesis: Es <unk> <unk> und in <unk> <unk>
2024-05-23 10:43:51,452 - INFO - joeynmt.training - Example #4
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:43:51,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', 'Ich', 'Ich', 'Ich', 'Ich', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:43:51,452 - INFO - joeynmt.training - 	Hypothesis: Die <unk> Ich Ich Ich Ich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:44:07,431 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.475542, Batch Acc: 0.354540, Tokens per Sec:     3973, Lr: 0.000300
2024-05-23 10:44:22,969 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.427644, Batch Acc: 0.365132, Tokens per Sec:     4228, Lr: 0.000300
2024-05-23 10:44:38,565 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.492343, Batch Acc: 0.368160, Tokens per Sec:     4117, Lr: 0.000300
2024-05-23 10:44:55,190 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.231986, Batch Acc: 0.373090, Tokens per Sec:     3996, Lr: 0.000300
2024-05-23 10:45:14,013 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.258629, Batch Acc: 0.381281, Tokens per Sec:     3439, Lr: 0.000300
2024-05-23 10:45:14,013 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:45:14,013 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:45:36,770 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.30, ppl:   9.95, acc:   0.38, generation: 22.7530[sec], evaluation: 0.0000[sec]
2024-05-23 10:45:36,771 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:45:37,235 - INFO - joeynmt.training - Example #0
2024-05-23 10:45:37,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:45:37,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:45:37,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'ich', 'diese', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'dass', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:45:37,236 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:45:37,236 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:45:37,236 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> ich diese <unk> <unk> <unk> <unk> <unk> <unk> dass die <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:45:37,236 - INFO - joeynmt.training - Example #1
2024-05-23 10:45:37,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:45:37,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:45:37,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'das', '<unk>', '<unk>', '<unk>', '', '', '<unk>', 'weil', 'es', 'nicht', '<unk>', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> das <unk> <unk> <unk>   <unk> weil es nicht <unk> der <unk> <unk>
2024-05-23 10:45:37,237 - INFO - joeynmt.training - Example #2
2024-05-23 10:45:37,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:45:37,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:45:37,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ist', 'ein', '<unk>', '<unk>', '<unk>', '<unk>', '', '', '', '', '', '<unk>', '</s>']
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ist ein <unk> <unk> <unk> <unk>      <unk>
2024-05-23 10:45:37,237 - INFO - joeynmt.training - Example #3
2024-05-23 10:45:37,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:45:37,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:45:37,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:45:37,237 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:45:37,238 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:45:37,238 - INFO - joeynmt.training - Example #4
2024-05-23 10:45:37,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:45:37,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:45:37,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ich', 'Ihnen', '<unk>', '<unk>', '', '', '', '<unk>', 'was', 'was', '<unk>', '<unk>', '</s>']
2024-05-23 10:45:37,238 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:45:37,238 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:45:37,238 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ich Ihnen <unk> <unk>    <unk> was was <unk> <unk>
2024-05-23 10:45:58,217 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.225116, Batch Acc: 0.387484, Tokens per Sec:     3082, Lr: 0.000300
2024-05-23 10:46:14,584 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.182861, Batch Acc: 0.394035, Tokens per Sec:     4009, Lr: 0.000300
2024-05-23 10:46:30,525 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.097585, Batch Acc: 0.395469, Tokens per Sec:     4043, Lr: 0.000300
2024-05-23 10:46:47,232 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.116276, Batch Acc: 0.404546, Tokens per Sec:     3866, Lr: 0.000300
2024-05-23 10:47:02,971 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.143670, Batch Acc: 0.409178, Tokens per Sec:     4114, Lr: 0.000300
2024-05-23 10:47:02,973 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:47:02,973 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:47:30,003 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.17, ppl:   8.77, acc:   0.39, generation: 27.0260[sec], evaluation: 0.0000[sec]
2024-05-23 10:47:30,006 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:47:30,275 - INFO - joeynmt.training - Example #0
2024-05-23 10:47:30,275 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:47:30,275 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:47:30,275 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '<unk>', 'Ich', 'habe', 'diese', 'beiden', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '', '', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:47:30,275 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:47:30,275 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:47:30,275 - INFO - joeynmt.training - 	Hypothesis: <unk> <unk> Ich habe diese beiden <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   <unk> <unk> <unk> <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:47:30,275 - INFO - joeynmt.training - Example #1
2024-05-23 10:47:30,275 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:47:30,275 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'das', '<unk>', '<unk>', '<unk>', 'das', 'Problem', 'Problem', '<unk>', '', 'weil', 'es', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> das <unk> <unk> <unk> das Problem Problem <unk>  weil es die <unk> des <unk>
2024-05-23 10:47:30,276 - INFO - joeynmt.training - Example #2
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', 'ist', '<unk>', '<unk>', '<unk>', '', 'der', '<unk>', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Hypothesis: Der <unk> ist <unk> <unk> <unk>  der <unk> <unk> der <unk>
2024-05-23 10:47:30,276 - INFO - joeynmt.training - Example #3
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '</s>']
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>
2024-05-23 10:47:30,276 - INFO - joeynmt.training - Example #4
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:47:30,276 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächsten', 'nächsten', 'nächsten', '<unk>', '', '', 'ein', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:47:30,276 - INFO - joeynmt.training - 	Hypothesis: Die nächsten nächsten nächsten <unk>   ein <unk> <unk> <unk> <unk>
2024-05-23 10:47:47,006 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     1.969730, Batch Acc: 0.413379, Tokens per Sec:     3900, Lr: 0.000300
2024-05-23 10:48:03,172 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.077991, Batch Acc: 0.413102, Tokens per Sec:     3990, Lr: 0.000300
2024-05-23 10:48:19,017 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     1.905872, Batch Acc: 0.423488, Tokens per Sec:     4059, Lr: 0.000300
2024-05-23 10:48:34,697 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.211174, Batch Acc: 0.420578, Tokens per Sec:     4112, Lr: 0.000300
2024-05-23 10:48:50,708 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.000482, Batch Acc: 0.427483, Tokens per Sec:     4024, Lr: 0.000300
2024-05-23 10:48:50,709 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:48:50,709 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:49:07,488 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.06, ppl:   7.88, acc:   0.41, generation: 16.7754[sec], evaluation: 0.0000[sec]
2024-05-23 10:49:07,489 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:49:07,771 - INFO - joeynmt.training - Example #0
2024-05-23 10:49:07,771 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:49:07,771 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:49:07,771 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'ich', '<unk>', '<unk>', '<unk>', 'also', 'also', 'also', '<unk>', '<unk>', '', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '', 'für', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:49:07,771 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:49:07,771 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Hypothesis: <unk> ich <unk> <unk> <unk> also also also <unk> <unk>  <unk> <unk>  <unk> <unk> <unk>  für die <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:49:07,772 - INFO - joeynmt.training - Example #1
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', '<unk>', 'des', '<unk>', '<unk>', '', 'denn', 'es', 'ist', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> <unk> des <unk> <unk>  denn es ist nicht die <unk> des <unk>
2024-05-23 10:49:07,772 - INFO - joeynmt.training - Example #2
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', '<unk>', '', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ist in einem <unk>  <unk>  <unk> der <unk>
2024-05-23 10:49:07,772 - INFO - joeynmt.training - Example #3
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:49:07,772 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:49:07,772 - INFO - joeynmt.training - Example #4
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:49:07,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', '<unk>', 'Ich', 'Ihnen', '<unk>', '', '<unk>', '<unk>', 'was', '<unk>', '<unk>', '</s>']
2024-05-23 10:49:07,773 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:49:07,773 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:49:07,773 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> <unk> Ich Ihnen <unk>  <unk> <unk> was <unk> <unk>
2024-05-23 10:49:23,854 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.097940, Batch Acc: 0.433860, Tokens per Sec:     3930, Lr: 0.000300
2024-05-23 10:49:39,622 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     1.949131, Batch Acc: 0.435365, Tokens per Sec:     4169, Lr: 0.000300
2024-05-23 10:49:55,796 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     1.875726, Batch Acc: 0.437799, Tokens per Sec:     3892, Lr: 0.000300
2024-05-23 10:50:11,682 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     1.894190, Batch Acc: 0.438774, Tokens per Sec:     4077, Lr: 0.000300
2024-05-23 10:50:27,906 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     1.871741, Batch Acc: 0.442011, Tokens per Sec:     3989, Lr: 0.000300
2024-05-23 10:50:27,907 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:50:27,907 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:50:51,267 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.27, acc:   0.43, generation: 23.3559[sec], evaluation: 0.0000[sec]
2024-05-23 10:50:51,269 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:50:51,508 - INFO - joeynmt.training - Example #0
2024-05-23 10:50:51,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:50:51,508 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:50:51,508 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', '', '<unk>', 'die', '<unk>', '<unk>', '<unk>', '', '', '', 'für', 'die', '<unk>', 'von', '<unk>', '<unk>', '<unk>', '', '', '<unk>', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:50:51,508 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:50:51,508 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:50:51,508 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so  <unk> die <unk> <unk> <unk>    für die <unk> von <unk> <unk> <unk>   <unk> der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:50:51,509 - INFO - joeynmt.training - Example #1
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'das', '<unk>', 'des', '<unk>', '<unk>', '', 'denn', 'es', 'ist', 'nicht', 'nicht', 'die', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> das <unk> des <unk> <unk>  denn es ist nicht nicht die <unk> der <unk>
2024-05-23 10:50:51,509 - INFO - joeynmt.training - Example #2
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ist', 'in', 'einer', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ist in einer <unk>  <unk> <unk>
2024-05-23 10:50:51,509 - INFO - joeynmt.training - Example #3
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:50:51,509 - INFO - joeynmt.training - Example #4
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:50:51,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'ich', 'Ihnen', '<unk>', '', '', '<unk>', '<unk>', 'von', '<unk>', '</s>']
2024-05-23 10:50:51,509 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:50:51,510 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:50:51,510 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> ich Ihnen <unk>   <unk> <unk> von <unk>
2024-05-23 10:51:07,125 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     1.887684, Batch Acc: 0.448619, Tokens per Sec:     4109, Lr: 0.000300
2024-05-23 10:51:14,240 - INFO - joeynmt.training - Epoch   1: total training loss 5916.58
2024-05-23 10:51:14,242 - INFO - joeynmt.training - EPOCH 2
2024-05-23 10:51:22,895 - INFO - joeynmt.training - Epoch   2, Step:     2700, Batch Loss:     1.851257, Batch Acc: 0.456218, Tokens per Sec:     4183, Lr: 0.000300
2024-05-23 10:51:38,542 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     1.808888, Batch Acc: 0.454467, Tokens per Sec:     4135, Lr: 0.000300
2024-05-23 10:51:54,731 - INFO - joeynmt.training - Epoch   2, Step:     2900, Batch Loss:     1.749451, Batch Acc: 0.463114, Tokens per Sec:     4054, Lr: 0.000300
2024-05-23 10:52:10,467 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     1.732570, Batch Acc: 0.460994, Tokens per Sec:     4158, Lr: 0.000300
2024-05-23 10:52:10,469 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:52:10,469 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:52:30,078 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.02, acc:   0.43, generation: 19.6050[sec], evaluation: 0.0000[sec]
2024-05-23 10:52:30,079 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:52:30,305 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/500.ckpt
2024-05-23 10:52:30,421 - INFO - joeynmt.training - Example #0
2024-05-23 10:52:30,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:52:30,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:52:30,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', '', '<unk>', 'das', 'das', '<unk>', '<unk>', '', '', '', '', 'für', 'die', 'meisten', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', 'die', '<unk>', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so  <unk> das das <unk> <unk>     für die meisten <unk> <unk> <unk> <unk>  <unk> die <unk> der <unk> <unk> <unk>  <unk> <unk>
2024-05-23 10:52:30,422 - INFO - joeynmt.training - Example #1
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', '', 'denn', 'es', 'gibt', 'nicht', 'die', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem  denn es gibt nicht die <unk> der <unk>
2024-05-23 10:52:30,422 - INFO - joeynmt.training - Example #2
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Hypothesis: Die <unk> <unk> ist in einem <unk>  der <unk> <unk>
2024-05-23 10:52:30,422 - INFO - joeynmt.training - Example #3
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:52:30,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:52:30,422 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:52:30,423 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:52:30,423 - INFO - joeynmt.training - Example #4
2024-05-23 10:52:30,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:52:30,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:52:30,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', '<unk>', 'ich', 'Ihnen', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 10:52:30,423 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:52:30,423 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:52:30,423 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> <unk> ich Ihnen <unk>  <unk> <unk>
2024-05-23 10:52:46,826 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     1.928308, Batch Acc: 0.461447, Tokens per Sec:     3998, Lr: 0.000300
2024-05-23 10:53:02,208 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     1.662353, Batch Acc: 0.463794, Tokens per Sec:     4252, Lr: 0.000300
2024-05-23 10:53:19,133 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     1.704012, Batch Acc: 0.470010, Tokens per Sec:     3771, Lr: 0.000300
2024-05-23 10:53:35,411 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     1.829177, Batch Acc: 0.468095, Tokens per Sec:     3963, Lr: 0.000300
2024-05-23 10:53:51,745 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     1.639068, Batch Acc: 0.467944, Tokens per Sec:     4056, Lr: 0.000300
2024-05-23 10:53:51,747 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:53:51,747 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:54:07,848 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.74, acc:   0.44, generation: 16.0966[sec], evaluation: 0.0000[sec]
2024-05-23 10:54:07,849 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:54:08,140 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/1000.ckpt
2024-05-23 10:54:08,184 - INFO - joeynmt.training - Example #0
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'beiden', '<unk>', 'so', 'dass', 'das', '<unk>', '<unk>', '', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese beiden <unk> so dass das <unk> <unk>  <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:54:08,185 - INFO - joeynmt.training - Example #1
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', '<unk>', 'das', 'Problem', '<unk>', '', 'denn', 'es', 'ist', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> <unk> das Problem <unk>  denn es ist nicht die <unk> des <unk>
2024-05-23 10:54:08,185 - INFO - joeynmt.training - Example #2
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', '<unk>', 'der', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:54:08,185 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist in einem <unk>  <unk> der <unk> des <unk>
2024-05-23 10:54:08,185 - INFO - joeynmt.training - Example #3
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:54:08,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:54:08,186 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:54:08,186 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:54:08,186 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:54:08,186 - INFO - joeynmt.training - Example #4
2024-05-23 10:54:08,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:54:08,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:54:08,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', '<unk>', 'ich', 'Ihnen', 'zeigen', '', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:54:08,186 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:54:08,186 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:54:08,186 - INFO - joeynmt.training - 	Hypothesis: Das nächste <unk> ich Ihnen zeigen  <unk> <unk> <unk> <unk>
2024-05-23 10:54:23,938 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     1.796080, Batch Acc: 0.468117, Tokens per Sec:     4045, Lr: 0.000300
2024-05-23 10:54:39,588 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     1.615669, Batch Acc: 0.472073, Tokens per Sec:     4070, Lr: 0.000300
2024-05-23 10:54:56,032 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     1.752830, Batch Acc: 0.472439, Tokens per Sec:     4045, Lr: 0.000300
2024-05-23 10:55:12,002 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     1.790925, Batch Acc: 0.470463, Tokens per Sec:     4029, Lr: 0.000300
2024-05-23 10:55:27,974 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     1.868358, Batch Acc: 0.471679, Tokens per Sec:     4113, Lr: 0.000300
2024-05-23 10:55:27,975 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:55:27,975 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:55:54,290 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.48, acc:   0.44, generation: 26.3102[sec], evaluation: 0.0000[sec]
2024-05-23 10:55:54,291 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:55:54,520 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/1500.ckpt
2024-05-23 10:55:54,579 - INFO - joeynmt.training - Example #0
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', 'die', '<unk>', '<unk>', '', '', '<unk>', 'die', 'für', 'die', 'meisten', '<unk>', '<unk>', '', '', '<unk>', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', 'hat', 'die', 'Größe', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:55:54,579 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:55:54,579 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:55:54,579 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk>  <unk> die <unk> <unk>   <unk> die für die meisten <unk> <unk>   <unk> die Größe der <unk> <unk> <unk>  <unk> hat die Größe <unk> <unk> <unk>
2024-05-23 10:55:54,579 - INFO - joeynmt.training - Example #1
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'das', '<unk>', 'dieses', 'Problem', '', 'denn', 'es', 'ist', 'nicht', 'die', '<unk>', 'der', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 10:55:54,579 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:55:54,579 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:55:54,579 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> das <unk> dieses Problem  denn es ist nicht die <unk> der <unk> der <unk>
2024-05-23 10:55:54,579 - INFO - joeynmt.training - Example #2
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:55:54,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', 'in', 'einer', '<unk>', '', '<unk>', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist in einer <unk>  <unk> die <unk> des <unk>
2024-05-23 10:55:54,580 - INFO - joeynmt.training - Example #3
2024-05-23 10:55:54,580 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:55:54,580 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:55:54,580 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:55:54,580 - INFO - joeynmt.training - Example #4
2024-05-23 10:55:54,580 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:55:54,580 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:55:54,580 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', '<unk>', 'ich', 'Ihnen', '<unk>', '', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:55:54,580 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> <unk> ich Ihnen <unk>  <unk> <unk> <unk>
2024-05-23 10:56:10,807 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     1.787356, Batch Acc: 0.477378, Tokens per Sec:     3995, Lr: 0.000300
2024-05-23 10:56:27,277 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     2.008971, Batch Acc: 0.473185, Tokens per Sec:     3836, Lr: 0.000300
2024-05-23 10:56:48,762 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     1.695705, Batch Acc: 0.477647, Tokens per Sec:     3102, Lr: 0.000300
2024-05-23 10:57:06,286 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     1.828783, Batch Acc: 0.474932, Tokens per Sec:     3690, Lr: 0.000300
2024-05-23 10:57:23,779 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.952800, Batch Acc: 0.474780, Tokens per Sec:     3670, Lr: 0.000300
2024-05-23 10:57:23,779 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:57:23,779 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:57:50,812 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.22, acc:   0.46, generation: 27.0281[sec], evaluation: 0.0000[sec]
2024-05-23 10:57:50,813 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:57:51,101 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/2000.ckpt
2024-05-23 10:57:51,214 - INFO - joeynmt.training - Example #0
2024-05-23 10:57:51,214 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:57:51,214 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:57:51,214 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'beiden', '<unk>', '<unk>', 'also', 'das', '<unk>', '<unk>', '', '<unk>', 'das', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '', '<unk>', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:57:51,215 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:57:51,215 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:57:51,215 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese beiden <unk> <unk> also das <unk> <unk>  <unk> das <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk>   <unk> die <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:57:51,215 - INFO - joeynmt.training - Example #1
2024-05-23 10:57:51,215 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:57:51,215 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:57:51,215 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', '<unk>', 'das', 'Problem', '<unk>', '', 'denn', 'es', 'ist', 'nicht', 'die', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> <unk> das Problem <unk>  denn es ist nicht die <unk> der <unk>
2024-05-23 10:57:51,216 - INFO - joeynmt.training - Example #2
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '<unk>', '', 'das', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist in einem <unk> <unk>  das <unk> <unk> <unk>
2024-05-23 10:57:51,216 - INFO - joeynmt.training - Example #3
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:57:51,216 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:57:51,216 - INFO - joeynmt.training - Example #4
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:57:51,216 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', '<unk>', 'Ich', 'zeige', 'Ihnen', '<unk>', '', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:57:51,217 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:57:51,217 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:57:51,217 - INFO - joeynmt.training - 	Hypothesis: Das nächste <unk> Ich zeige Ihnen <unk>  <unk> <unk> <unk>
2024-05-23 10:58:08,242 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.734114, Batch Acc: 0.477057, Tokens per Sec:     3659, Lr: 0.000300
2024-05-23 10:58:24,753 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.729603, Batch Acc: 0.477134, Tokens per Sec:     3881, Lr: 0.000300
2024-05-23 10:58:41,973 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.795077, Batch Acc: 0.480402, Tokens per Sec:     3821, Lr: 0.000300
2024-05-23 10:58:58,749 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     1.652340, Batch Acc: 0.484023, Tokens per Sec:     3955, Lr: 0.000300
2024-05-23 10:59:14,990 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.689258, Batch Acc: 0.481038, Tokens per Sec:     4030, Lr: 0.000300
2024-05-23 10:59:14,991 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 10:59:14,991 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 10:59:36,768 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.10, acc:   0.46, generation: 21.7733[sec], evaluation: 0.0000[sec]
2024-05-23 10:59:36,769 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 10:59:37,000 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/2500.ckpt
2024-05-23 10:59:37,027 - INFO - joeynmt.training - Example #0
2024-05-23 10:59:37,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 10:59:37,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 10:59:37,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', 'das', '<unk>', '<unk>', '', '<unk>', 'das', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', 'hat', 'die', 'Größe', 'des', '<unk>', '<unk>', '<unk>', '', '<unk>', 'hat', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk>  <unk> das <unk> <unk>  <unk> das <unk> <unk> <unk>  <unk> <unk> <unk> <unk>  <unk> hat die Größe des <unk> <unk> <unk>  <unk> hat <unk> <unk> <unk> <unk> <unk>
2024-05-23 10:59:37,028 - INFO - joeynmt.training - Example #1
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'das', '<unk>', 'dieses', '<unk>', 'Problem', '<unk>', '', 'denn', 'es', '<unk>', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> das <unk> dieses <unk> Problem <unk>  denn es <unk> nicht die <unk> des <unk>
2024-05-23 10:59:37,028 - INFO - joeynmt.training - Example #2
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', '<unk>', 'das', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  <unk> das <unk> des <unk>
2024-05-23 10:59:37,028 - INFO - joeynmt.training - Example #3
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 10:59:37,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 10:59:37,028 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 10:59:37,029 - INFO - joeynmt.training - Example #4
2024-05-23 10:59:37,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 10:59:37,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 10:59:37,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'ich', 'Ihnen', '<unk>', '', '<unk>', '<unk>', '<unk>', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'ist.', '</s>']
2024-05-23 10:59:37,029 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 10:59:37,029 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 10:59:37,029 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> ich Ihnen <unk>  <unk> <unk> <unk> was über die letzten 25 Jahren ist.
2024-05-23 10:59:56,865 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.748544, Batch Acc: 0.483132, Tokens per Sec:     3241, Lr: 0.000300
2024-05-23 11:00:18,360 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.695585, Batch Acc: 0.484135, Tokens per Sec:     3100, Lr: 0.000300
2024-05-23 11:00:34,111 - INFO - joeynmt.training - Epoch   2: total training loss 4628.11
2024-05-23 11:00:34,111 - INFO - joeynmt.training - EPOCH 3
2024-05-23 11:00:38,227 - INFO - joeynmt.training - Epoch   3, Step:     5300, Batch Loss:     1.571902, Batch Acc: 0.503480, Tokens per Sec:     3142, Lr: 0.000300
2024-05-23 11:00:58,910 - INFO - joeynmt.training - Epoch   3, Step:     5400, Batch Loss:     1.737466, Batch Acc: 0.502679, Tokens per Sec:     3204, Lr: 0.000300
2024-05-23 11:01:19,192 - INFO - joeynmt.training - Epoch   3, Step:     5500, Batch Loss:     1.707744, Batch Acc: 0.499326, Tokens per Sec:     3219, Lr: 0.000300
2024-05-23 11:01:19,192 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:01:19,192 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:01:42,385 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.89, acc:   0.47, generation: 23.1842[sec], evaluation: 0.0000[sec]
2024-05-23 11:01:42,385 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 11:01:42,738 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/3000.ckpt
2024-05-23 11:01:42,782 - INFO - joeynmt.training - Example #0
2024-05-23 11:01:42,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:01:42,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:01:42,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', 'also', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', '<unk>', 'die', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahren', '<unk>', '<unk>', '', '<unk>', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:01:42,782 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:01:42,782 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:01:42,782 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> also <unk> dass das <unk> Eis <unk> <unk> <unk> die für die meisten letzten drei Millionen Jahren <unk> <unk>  <unk> die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:01:42,782 - INFO - joeynmt.training - Example #1
2024-05-23 11:01:42,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:01:42,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:01:42,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieser', '<unk>', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 11:01:42,783 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:01:42,783 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:01:42,783 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieser <unk> <unk>  weil es nicht die <unk> der <unk>
2024-05-23 11:01:42,783 - INFO - joeynmt.training - Example #2
2024-05-23 11:01:42,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:01:42,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:01:42,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', '<unk>', '</s>']
2024-05-23 11:01:42,783 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:01:42,783 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:01:42,783 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  das <unk> <unk>
2024-05-23 11:01:42,783 - INFO - joeynmt.training - Example #3
2024-05-23 11:01:42,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:01:42,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:01:42,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:01:42,784 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:01:42,784 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:01:42,784 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:01:42,784 - INFO - joeynmt.training - Example #4
2024-05-23 11:01:42,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:01:42,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:01:42,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'zeige', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', '<unk>', 'von', 'dem,', 'was', 'vor', '<unk>', '<unk>', '</s>']
2024-05-23 11:01:42,784 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:01:42,784 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:01:42,784 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> zeige ich Ihnen <unk>  ein <unk> <unk> <unk> von dem, was vor <unk> <unk>
2024-05-23 11:02:03,202 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     1.674767, Batch Acc: 0.498885, Tokens per Sec:     3189, Lr: 0.000300
2024-05-23 11:02:23,447 - INFO - joeynmt.training - Epoch   3, Step:     5700, Batch Loss:     1.625579, Batch Acc: 0.498958, Tokens per Sec:     3177, Lr: 0.000300
2024-05-23 11:02:43,687 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     1.490574, Batch Acc: 0.500906, Tokens per Sec:     3162, Lr: 0.000300
2024-05-23 11:03:09,357 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     1.564676, Batch Acc: 0.500099, Tokens per Sec:     2557, Lr: 0.000300
2024-05-23 11:03:29,122 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     1.557197, Batch Acc: 0.505221, Tokens per Sec:     3338, Lr: 0.000300
2024-05-23 11:03:29,122 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:03:29,122 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:03:52,265 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.10, acc:   0.46, generation: 23.1366[sec], evaluation: 0.0000[sec]
2024-05-23 11:03:52,597 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/3500.ckpt
2024-05-23 11:03:52,710 - INFO - joeynmt.training - Example #0
2024-05-23 11:03:52,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:03:52,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:03:52,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', 'das', '<unk>', '<unk>', '', '<unk>', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '</s>']
2024-05-23 11:03:52,710 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:03:52,710 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:03:52,710 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> also das <unk> <unk>  <unk> das für die meisten letzten drei Millionen Jahre <unk>  <unk> <unk> <unk> <unk> <unk> <unk>  <unk>
2024-05-23 11:03:52,710 - INFO - joeynmt.training - Example #1
2024-05-23 11:03:52,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:03:52,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:03:52,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieser', '<unk>', '', 'Denn', 'es', 'ist', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:03:52,710 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:03:52,710 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:03:52,710 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieser <unk>  Denn es ist nicht die <unk> des <unk>
2024-05-23 11:03:52,711 - INFO - joeynmt.training - Example #2
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', 'ist', 'im', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 11:03:52,711 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:03:52,711 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:03:52,711 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> ist im <unk>  der <unk> <unk>
2024-05-23 11:03:52,711 - INFO - joeynmt.training - Example #3
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:03:52,711 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:03:52,711 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:03:52,711 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:03:52,711 - INFO - joeynmt.training - Example #4
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:03:52,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'ich', 'Ihnen', 'Ihnen', 'zeigen', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:03:52,712 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:03:52,712 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:03:52,712 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> ich Ihnen Ihnen zeigen  <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:04:12,297 - INFO - joeynmt.training - Epoch   3, Step:     6100, Batch Loss:     1.735263, Batch Acc: 0.503245, Tokens per Sec:     3307, Lr: 0.000300
2024-05-23 11:04:32,077 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     1.619822, Batch Acc: 0.497034, Tokens per Sec:     3230, Lr: 0.000300
2024-05-23 11:04:52,004 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     1.516991, Batch Acc: 0.497722, Tokens per Sec:     3250, Lr: 0.000300
2024-05-23 11:05:12,260 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     1.729258, Batch Acc: 0.499317, Tokens per Sec:     3289, Lr: 0.000300
2024-05-23 11:05:32,167 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     1.700467, Batch Acc: 0.504229, Tokens per Sec:     3195, Lr: 0.000300
2024-05-23 11:05:32,167 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:05:32,168 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:05:56,712 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.59, acc:   0.48, generation: 24.5386[sec], evaluation: 0.0000[sec]
2024-05-23 11:05:56,714 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 11:05:57,088 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/4000.ckpt
2024-05-23 11:05:57,131 - INFO - joeynmt.training - Example #0
2024-05-23 11:05:57,131 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:05:57,131 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:05:57,131 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', 'also', '<unk>', 'das', 'das', '<unk>', '<unk>', '<unk>', '', '<unk>', 'was', 'für', 'die', 'meisten', '<unk>', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:05:57,131 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:05:57,131 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:05:57,131 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk> also <unk> das das <unk> <unk> <unk>  <unk> was für die meisten <unk> der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:05:57,131 - INFO - joeynmt.training - Example #1
2024-05-23 11:05:57,131 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:05:57,131 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:05:57,131 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieser', '<unk>', '<unk>', '', 'denn', 'es', 'ist', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:05:57,131 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:05:57,131 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:05:57,132 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieser <unk> <unk>  denn es ist nicht die <unk> des <unk>
2024-05-23 11:05:57,132 - INFO - joeynmt.training - Example #2
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:05:57,132 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:05:57,132 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:05:57,132 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist in einem <unk>  das <unk> des <unk>
2024-05-23 11:05:57,132 - INFO - joeynmt.training - Example #3
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:05:57,132 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:05:57,132 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:05:57,132 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:05:57,132 - INFO - joeynmt.training - Example #4
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:05:57,132 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'ich', 'Ihnen', '<unk>', '<unk>', 'werden.', '', '<unk>', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:05:57,133 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:05:57,133 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:05:57,133 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> ich Ihnen <unk> <unk> werden.  <unk> <unk> was in den letzten 25 Jahren <unk>
2024-05-23 11:06:17,708 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     1.629447, Batch Acc: 0.503176, Tokens per Sec:     3105, Lr: 0.000300
2024-05-23 11:06:37,904 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     1.587146, Batch Acc: 0.501447, Tokens per Sec:     3216, Lr: 0.000300
2024-05-23 11:06:59,164 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     1.544914, Batch Acc: 0.502938, Tokens per Sec:     3010, Lr: 0.000300
2024-05-23 11:07:19,829 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     1.667004, Batch Acc: 0.501527, Tokens per Sec:     3216, Lr: 0.000300
2024-05-23 11:07:40,076 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     1.685746, Batch Acc: 0.501375, Tokens per Sec:     3197, Lr: 0.000300
2024-05-23 11:07:40,076 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:07:40,076 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:08:07,348 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.70, acc:   0.47, generation: 27.2654[sec], evaluation: 0.0000[sec]
2024-05-23 11:08:07,672 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/4500.ckpt
2024-05-23 11:08:07,735 - INFO - joeynmt.training - Example #0
2024-05-23 11:08:07,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'beiden', '<unk>', '<unk>', '<unk>', '', '<unk>', 'das', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '', '<unk>', 'die', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', 'hat', '<unk>', '</s>']
2024-05-23 11:08:07,736 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:08:07,736 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:08:07,736 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese beiden <unk> <unk> <unk>  <unk> das <unk> <unk>  <unk> <unk> <unk>  <unk> die für die meisten letzten drei Millionen Jahre <unk> <unk> <unk> <unk> <unk>  <unk> hat <unk>
2024-05-23 11:08:07,736 - INFO - joeynmt.training - Example #1
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieser', '<unk>', '', 'weil', 'es', 'nicht', '<unk>', '<unk>', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 11:08:07,736 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:08:07,736 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:08:07,736 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieser <unk>  weil es nicht <unk> <unk> der <unk> <unk>
2024-05-23 11:08:07,736 - INFO - joeynmt.training - Example #2
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:08:07,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:08:07,737 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:08:07,737 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:08:07,737 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> <unk>
2024-05-23 11:08:07,737 - INFO - joeynmt.training - Example #3
2024-05-23 11:08:07,737 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:08:07,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:08:07,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:08:07,737 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:08:07,737 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:08:07,737 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:08:07,737 - INFO - joeynmt.training - Example #4
2024-05-23 11:08:07,737 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:08:07,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:08:07,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'Ich', 'zeige', 'Ihnen', '<unk>', '<unk>', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:08:07,738 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:08:07,738 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:08:07,738 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> Ich zeige Ihnen <unk> <unk> <unk> was in den letzten 25 Jahren <unk>
2024-05-23 11:08:29,463 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     1.574692, Batch Acc: 0.501634, Tokens per Sec:     2960, Lr: 0.000300
2024-05-23 11:08:53,975 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     1.529586, Batch Acc: 0.502068, Tokens per Sec:     2644, Lr: 0.000300
2024-05-23 11:09:15,145 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     1.594698, Batch Acc: 0.502964, Tokens per Sec:     3044, Lr: 0.000300
2024-05-23 11:09:35,169 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     1.501719, Batch Acc: 0.502235, Tokens per Sec:     3285, Lr: 0.000300
2024-05-23 11:09:54,930 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     1.641714, Batch Acc: 0.504059, Tokens per Sec:     3248, Lr: 0.000300
2024-05-23 11:09:54,930 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:09:54,930 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:10:17,038 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.62, acc:   0.47, generation: 22.1020[sec], evaluation: 0.0000[sec]
2024-05-23 11:10:17,420 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/6000.ckpt
2024-05-23 11:10:17,450 - INFO - joeynmt.training - Example #0
2024-05-23 11:10:17,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:10:17,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:10:17,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', 'das', '<unk>', '', '<unk>', 'das', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:10:17,450 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk> <unk> <unk>  <unk> das <unk>  <unk> das für die meisten <unk> drei Millionen Jahre <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:10:17,451 - INFO - joeynmt.training - Example #1
2024-05-23 11:10:17,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:10:17,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:10:17,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'denn', 'es', '<unk>', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  denn es <unk> <unk> der <unk>
2024-05-23 11:10:17,451 - INFO - joeynmt.training - Example #2
2024-05-23 11:10:17,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:10:17,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:10:17,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:10:17,451 - INFO - joeynmt.training - 	Hypothesis: Der <unk> <unk> ist in einem <unk>  der <unk> des <unk>
2024-05-23 11:10:17,452 - INFO - joeynmt.training - Example #3
2024-05-23 11:10:17,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:10:17,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:10:17,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:10:17,452 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:10:17,452 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:10:17,452 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:10:17,452 - INFO - joeynmt.training - Example #4
2024-05-23 11:10:17,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:10:17,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:10:17,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'ich', 'Ihnen', 'zeigen', 'werde,', 'wird', 'ein', '<unk>', '<unk>', 'der', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahren.', '</s>']
2024-05-23 11:10:17,452 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:10:17,452 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:10:17,452 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> ich Ihnen zeigen werde, wird ein <unk> <unk> der <unk> was in den letzten 25 Jahren.
2024-05-23 11:10:37,335 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.583085, Batch Acc: 0.504657, Tokens per Sec:     3153, Lr: 0.000300
2024-05-23 11:10:59,694 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.526573, Batch Acc: 0.505063, Tokens per Sec:     2906, Lr: 0.000300
2024-05-23 11:11:19,768 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.635296, Batch Acc: 0.502602, Tokens per Sec:     3303, Lr: 0.000300
2024-05-23 11:11:39,725 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.561369, Batch Acc: 0.505294, Tokens per Sec:     3285, Lr: 0.000300
2024-05-23 11:11:43,383 - INFO - joeynmt.training - Epoch   3: total training loss 4260.21
2024-05-23 11:11:43,383 - INFO - joeynmt.training - EPOCH 4
2024-05-23 11:11:59,893 - INFO - joeynmt.training - Epoch   4, Step:     8000, Batch Loss:     1.524454, Batch Acc: 0.519377, Tokens per Sec:     3204, Lr: 0.000300
2024-05-23 11:11:59,894 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:11:59,894 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:12:34,754 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.78, acc:   0.47, generation: 34.8524[sec], evaluation: 0.0000[sec]
2024-05-23 11:12:35,098 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/5000.ckpt
2024-05-23 11:12:35,185 - INFO - joeynmt.training - Example #0
2024-05-23 11:12:35,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:12:35,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:12:35,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', '<unk>', '', '<unk>', 'die', 'die', '<unk>', '<unk>', '', '<unk>', 'die', 'für', 'die', 'meisten', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', 'von', '40', 'Prozent.', '</s>']
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese zwei <unk> <unk> also <unk>  <unk> die die <unk> <unk>  <unk> die für die meisten <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk>  <unk> <unk> von 40 Prozent.
2024-05-23 11:12:35,186 - INFO - joeynmt.training - Example #1
2024-05-23 11:12:35,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:12:35,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:12:35,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '', 'denn', 'es', 'zeigt', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk>  denn es zeigt nicht die <unk> des <unk> <unk>
2024-05-23 11:12:35,186 - INFO - joeynmt.training - Example #2
2024-05-23 11:12:35,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:12:35,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:12:35,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:12:35,186 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> ist in einem <unk>  der <unk> <unk>
2024-05-23 11:12:35,187 - INFO - joeynmt.training - Example #3
2024-05-23 11:12:35,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:12:35,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:12:35,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:12:35,187 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:12:35,187 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:12:35,187 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:12:35,187 - INFO - joeynmt.training - Example #4
2024-05-23 11:12:35,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:12:35,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:12:35,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', '', '<unk>', '<unk>', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahren.', '</s>']
2024-05-23 11:12:35,187 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:12:35,187 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:12:35,187 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen  <unk> <unk> <unk> was in den letzten 25 Jahren.
2024-05-23 11:12:57,654 - INFO - joeynmt.training - Epoch   4, Step:     8100, Batch Loss:     1.516012, Batch Acc: 0.524936, Tokens per Sec:     2893, Lr: 0.000300
2024-05-23 11:13:16,943 - INFO - joeynmt.training - Epoch   4, Step:     8200, Batch Loss:     1.547545, Batch Acc: 0.525172, Tokens per Sec:     3409, Lr: 0.000300
2024-05-23 11:13:36,629 - INFO - joeynmt.training - Epoch   4, Step:     8300, Batch Loss:     1.523541, Batch Acc: 0.519354, Tokens per Sec:     3334, Lr: 0.000300
2024-05-23 11:13:56,755 - INFO - joeynmt.training - Epoch   4, Step:     8400, Batch Loss:     1.416560, Batch Acc: 0.520508, Tokens per Sec:     3266, Lr: 0.000300
2024-05-23 11:14:16,348 - INFO - joeynmt.training - Epoch   4, Step:     8500, Batch Loss:     1.619006, Batch Acc: 0.521134, Tokens per Sec:     3426, Lr: 0.000300
2024-05-23 11:14:16,349 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:14:16,349 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:14:55,194 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.40, acc:   0.48, generation: 38.8385[sec], evaluation: 0.0000[sec]
2024-05-23 11:14:55,195 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 11:14:55,574 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/5500.ckpt
2024-05-23 11:14:55,607 - INFO - joeynmt.training - Example #0
2024-05-23 11:14:55,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:14:55,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:14:55,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', '<unk>', 'das', 'das', '<unk>', '<unk>', '<unk>', '', '<unk>', 'die', '<unk>', 'drei', 'Millionen', 'Jahren', '<unk>', '', '<unk>', 'die', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:14:55,607 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:14:55,607 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:14:55,607 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk> <unk> das das <unk> <unk> <unk>  <unk> die <unk> drei Millionen Jahren <unk>  <unk> die <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:14:55,607 - INFO - joeynmt.training - Example #1
2024-05-23 11:14:55,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:14:55,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:14:55,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '<unk>', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:14:55,608 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:14:55,608 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:14:55,608 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> <unk> <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:14:55,608 - INFO - joeynmt.training - Example #2
2024-05-23 11:14:55,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:14:55,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:14:55,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:14:55,608 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:14:55,608 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:14:55,608 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:14:55,608 - INFO - joeynmt.training - Example #3
2024-05-23 11:14:55,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:14:55,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:14:55,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:14:55,608 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:14:55,609 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:14:55,609 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:14:55,609 - INFO - joeynmt.training - Example #4
2024-05-23 11:14:55,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:14:55,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:14:55,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', '<unk>', 'ich', 'Ihnen', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:14:55,609 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:14:55,609 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:14:55,609 - INFO - joeynmt.training - 	Hypothesis: Das nächste <unk> ich Ihnen <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:15:14,858 - INFO - joeynmt.training - Epoch   4, Step:     8600, Batch Loss:     1.502016, Batch Acc: 0.519277, Tokens per Sec:     3308, Lr: 0.000300
2024-05-23 11:15:35,157 - INFO - joeynmt.training - Epoch   4, Step:     8700, Batch Loss:     1.704684, Batch Acc: 0.519466, Tokens per Sec:     3207, Lr: 0.000300
2024-05-23 11:15:55,648 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     1.607939, Batch Acc: 0.521891, Tokens per Sec:     3198, Lr: 0.000300
2024-05-23 11:16:16,068 - INFO - joeynmt.training - Epoch   4, Step:     8900, Batch Loss:     1.524994, Batch Acc: 0.519869, Tokens per Sec:     3145, Lr: 0.000300
2024-05-23 11:16:36,393 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     1.536293, Batch Acc: 0.520068, Tokens per Sec:     3135, Lr: 0.000300
2024-05-23 11:16:36,393 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:16:36,394 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:17:10,625 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.45, acc:   0.48, generation: 34.2254[sec], evaluation: 0.0000[sec]
2024-05-23 11:17:10,957 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/8000.ckpt
2024-05-23 11:17:10,985 - INFO - joeynmt.training - Example #0
2024-05-23 11:17:10,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:17:10,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:17:10,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'beiden', '<unk>', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', '<unk>', '', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', '</s>']
2024-05-23 11:17:10,986 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:17:10,986 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:17:10,986 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese beiden <unk> <unk>  <unk> dass das <unk> Eis <unk>  <unk>  das für die meisten letzten drei Millionen Jahre <unk>  <unk>
2024-05-23 11:17:10,986 - INFO - joeynmt.training - Example #1
2024-05-23 11:17:10,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:17:10,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:17:10,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieser', '<unk>', '<unk>', '', 'denn', 'es', '<unk>', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:17:10,986 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:17:10,986 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:17:10,986 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieser <unk> <unk>  denn es <unk> nicht die <unk> des <unk>
2024-05-23 11:17:10,986 - INFO - joeynmt.training - Example #2
2024-05-23 11:17:10,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:17:10,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:17:10,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  das <unk> <unk> <unk>
2024-05-23 11:17:10,987 - INFO - joeynmt.training - Example #3
2024-05-23 11:17:10,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:17:10,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:17:10,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:17:10,987 - INFO - joeynmt.training - Example #4
2024-05-23 11:17:10,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:17:10,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:17:10,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'ich', 'Ihnen', 'zeigen', '<unk>', '', 'ein', '<unk>', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:17:10,987 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> ich Ihnen zeigen <unk>  ein <unk> <unk> was in den letzten 25 Jahren <unk>
2024-05-23 11:17:31,190 - INFO - joeynmt.training - Epoch   4, Step:     9100, Batch Loss:     1.465079, Batch Acc: 0.518909, Tokens per Sec:     3253, Lr: 0.000300
2024-05-23 11:17:51,665 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     1.481310, Batch Acc: 0.517812, Tokens per Sec:     3088, Lr: 0.000300
2024-05-23 11:18:11,575 - INFO - joeynmt.training - Epoch   4, Step:     9300, Batch Loss:     1.322116, Batch Acc: 0.518598, Tokens per Sec:     3259, Lr: 0.000300
2024-05-23 11:18:31,719 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     1.518075, Batch Acc: 0.518609, Tokens per Sec:     3197, Lr: 0.000300
2024-05-23 11:18:52,373 - INFO - joeynmt.training - Epoch   4, Step:     9500, Batch Loss:     1.472910, Batch Acc: 0.516694, Tokens per Sec:     3118, Lr: 0.000300
2024-05-23 11:18:52,374 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:18:52,374 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:19:18,827 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.45, acc:   0.48, generation: 26.4465[sec], evaluation: 0.0000[sec]
2024-05-23 11:19:19,160 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/7000.ckpt
2024-05-23 11:19:19,187 - INFO - joeynmt.training - Example #0
2024-05-23 11:19:19,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:19:19,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:19:19,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', '<unk>', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'was', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', 'die', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 11:19:19,187 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:19:19,187 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:19:19,187 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr <unk> ich diese zwei <unk> <unk> also <unk> dass das <unk> Eis <unk>  was für die meisten <unk> drei Millionen Jahre <unk>  <unk> die <unk> <unk> <unk>  <unk> <unk>
2024-05-23 11:19:19,187 - INFO - joeynmt.training - Example #1
2024-05-23 11:19:19,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:19:19,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:19:19,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'ist', 'die', '<unk>', 'dieses', 'Problem', '', 'Weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:19:19,188 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:19:19,188 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:19:19,188 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> ist die <unk> dieses Problem  Weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:19:19,188 - INFO - joeynmt.training - Example #2
2024-05-23 11:19:19,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:19:19,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:19:19,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', '<unk>', '<unk>', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 11:19:19,188 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:19:19,188 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:19:19,188 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist <unk> <unk> <unk>  der <unk> <unk>
2024-05-23 11:19:19,188 - INFO - joeynmt.training - Example #3
2024-05-23 11:19:19,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:19:19,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:19:19,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:19:19,189 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:19:19,189 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:19:19,189 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:19:19,189 - INFO - joeynmt.training - Example #4
2024-05-23 11:19:19,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:19:19,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:19:19,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächsten', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werde,', '', 'ein', '<unk>', '<unk>', 'von', 'dem,', 'was', 'vor', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:19:19,189 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:19:19,189 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:19:19,189 - INFO - joeynmt.training - 	Hypothesis: Die nächsten <unk> die ich Ihnen zeigen werde,  ein <unk> <unk> von dem, was vor den letzten 25 Jahren <unk>
2024-05-23 11:19:39,068 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     1.584707, Batch Acc: 0.518404, Tokens per Sec:     3152, Lr: 0.000300
2024-05-23 11:20:01,452 - INFO - joeynmt.training - Epoch   4, Step:     9700, Batch Loss:     1.654352, Batch Acc: 0.518085, Tokens per Sec:     2936, Lr: 0.000300
2024-05-23 11:20:23,124 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     1.387691, Batch Acc: 0.521607, Tokens per Sec:     2962, Lr: 0.000300
2024-05-23 11:20:45,294 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     1.391937, Batch Acc: 0.517819, Tokens per Sec:     2976, Lr: 0.000300
2024-05-23 11:21:06,306 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     1.590318, Batch Acc: 0.518499, Tokens per Sec:     3099, Lr: 0.000300
2024-05-23 11:21:06,306 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:21:06,306 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:21:40,042 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.51, acc:   0.48, generation: 33.7288[sec], evaluation: 0.0000[sec]
2024-05-23 11:21:40,396 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/7500.ckpt
2024-05-23 11:21:40,466 - INFO - joeynmt.training - Example #0
2024-05-23 11:21:40,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:21:40,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:21:40,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', '', '<unk>', 'das', '<unk>', 'Eis', '<unk>', '', '<unk>', 'das', 'für', 'die', 'meisten', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '', 'hat', 'die', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 11:21:40,467 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:21:40,467 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:21:40,467 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass  <unk> das <unk> Eis <unk>  <unk> das für die meisten <unk> <unk> <unk>  <unk> <unk> <unk>  hat die <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk> <unk>
2024-05-23 11:21:40,467 - INFO - joeynmt.training - Example #1
2024-05-23 11:21:40,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:21:40,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:21:40,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '<unk>', '', 'denn', 'es', '<unk>', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:21:40,467 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:21:40,467 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:21:40,467 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> <unk>  denn es <unk> nicht die <unk> des <unk>
2024-05-23 11:21:40,467 - INFO - joeynmt.training - Example #2
2024-05-23 11:21:40,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:21:40,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:21:40,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'das', '<unk>', 'Herz', 'des', '<unk>', '</s>']
2024-05-23 11:21:40,468 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:21:40,468 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:21:40,468 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  das <unk> Herz des <unk>
2024-05-23 11:21:40,468 - INFO - joeynmt.training - Example #3
2024-05-23 11:21:40,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:21:40,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:21:40,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:21:40,468 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:21:40,468 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:21:40,468 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:21:40,468 - INFO - joeynmt.training - Example #4
2024-05-23 11:21:40,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:21:40,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:21:40,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', '<unk>', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:21:40,469 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:21:40,469 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:21:40,469 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  <unk> <unk> was in den letzten 25 Jahren <unk>
2024-05-23 11:22:05,145 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     1.479647, Batch Acc: 0.516378, Tokens per Sec:     2593, Lr: 0.000300
2024-05-23 11:22:26,880 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     1.536753, Batch Acc: 0.522056, Tokens per Sec:     3009, Lr: 0.000300
2024-05-23 11:22:48,859 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     1.615159, Batch Acc: 0.517227, Tokens per Sec:     2952, Lr: 0.000300
2024-05-23 11:23:09,587 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     1.415532, Batch Acc: 0.516692, Tokens per Sec:     3097, Lr: 0.000300
2024-05-23 11:23:30,896 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     1.344797, Batch Acc: 0.518908, Tokens per Sec:     3073, Lr: 0.000300
2024-05-23 11:23:30,896 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:23:30,896 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:23:55,996 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.49, generation: 25.0932[sec], evaluation: 0.0000[sec]
2024-05-23 11:23:55,996 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 11:23:56,384 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/6500.ckpt
2024-05-23 11:23:56,509 - INFO - joeynmt.training - Example #0
2024-05-23 11:23:56,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:23:56,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:23:56,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', '<unk>', 'was', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:23:56,509 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:23:56,509 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:23:56,509 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> also <unk> dass das <unk> Eis <unk>  <unk> was für die meisten letzten drei Millionen Jahre <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:23:56,509 - INFO - joeynmt.training - Example #1
2024-05-23 11:23:56,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:23:56,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:23:56,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', '', 'denn', 'es', 'zeige', 'nicht', 'die', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 11:23:56,509 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:23:56,509 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:23:56,510 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem  denn es zeige nicht die <unk> der <unk>
2024-05-23 11:23:56,510 - INFO - joeynmt.training - Example #2
2024-05-23 11:23:56,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:23:56,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:23:56,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 11:23:56,510 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:23:56,510 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:23:56,510 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist in einem <unk>  der <unk> <unk>
2024-05-23 11:23:56,510 - INFO - joeynmt.training - Example #3
2024-05-23 11:23:56,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:23:56,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:23:56,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '</s>']
2024-05-23 11:23:56,510 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:23:56,510 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:23:56,510 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>
2024-05-23 11:23:56,510 - INFO - joeynmt.training - Example #4
2024-05-23 11:23:56,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:23:56,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:23:56,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'zeige', 'ich', 'Ihnen', 'Ihnen', '<unk>', '', 'eine', '<unk>', '<unk>', 'von', 'dem', 'letzten', '25', 'Jahre', '<unk>', '</s>']
2024-05-23 11:23:56,511 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:23:56,511 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:23:56,511 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> zeige ich Ihnen Ihnen <unk>  eine <unk> <unk> von dem letzten 25 Jahre <unk>
2024-05-23 11:24:08,407 - INFO - joeynmt.training - Epoch   4: total training loss 4050.90
2024-05-23 11:24:08,407 - INFO - joeynmt.training - EPOCH 5
2024-05-23 11:24:17,395 - INFO - joeynmt.training - Epoch   5, Step:    10600, Batch Loss:     1.575071, Batch Acc: 0.545361, Tokens per Sec:     3129, Lr: 0.000300
2024-05-23 11:24:38,380 - INFO - joeynmt.training - Epoch   5, Step:    10700, Batch Loss:     1.408230, Batch Acc: 0.542277, Tokens per Sec:     3109, Lr: 0.000300
2024-05-23 11:25:01,517 - INFO - joeynmt.training - Epoch   5, Step:    10800, Batch Loss:     1.556986, Batch Acc: 0.540963, Tokens per Sec:     2891, Lr: 0.000300
2024-05-23 11:25:25,496 - INFO - joeynmt.training - Epoch   5, Step:    10900, Batch Loss:     1.407299, Batch Acc: 0.537361, Tokens per Sec:     2645, Lr: 0.000300
2024-05-23 11:25:48,507 - INFO - joeynmt.training - Epoch   5, Step:    11000, Batch Loss:     1.500822, Batch Acc: 0.533281, Tokens per Sec:     2795, Lr: 0.000300
2024-05-23 11:25:48,508 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:25:48,508 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:26:15,092 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.45, acc:   0.48, generation: 26.5768[sec], evaluation: 0.0000[sec]
2024-05-23 11:26:15,477 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/10000.ckpt
2024-05-23 11:26:15,538 - INFO - joeynmt.training - Example #0
2024-05-23 11:26:15,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:26:15,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:26:15,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', 'also', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'das', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '</s>']
2024-05-23 11:26:15,538 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:26:15,538 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:26:15,538 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk> also <unk>  <unk> dass das <unk> Eis <unk>  das für die meisten <unk> drei Millionen Jahre <unk>
2024-05-23 11:26:15,538 - INFO - joeynmt.training - Example #1
2024-05-23 11:26:15,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:26:15,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:26:15,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieser', '<unk>', '<unk>', 'das', '<unk>', 'dieses', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:26:15,538 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Hypothesis: Aber dieser <unk> <unk> das <unk> dieses Problem <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:26:15,539 - INFO - joeynmt.training - Example #2
2024-05-23 11:26:15,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:26:15,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:26:15,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist in einem <unk>  der <unk> Herz des globalen <unk>
2024-05-23 11:26:15,539 - INFO - joeynmt.training - Example #3
2024-05-23 11:26:15,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:26:15,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:26:15,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:26:15,539 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:26:15,539 - INFO - joeynmt.training - Example #4
2024-05-23 11:26:15,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:26:15,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:26:15,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werde,', 'wird', '', 'ein', '<unk>', '<unk>', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 11:26:15,540 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:26:15,540 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:26:15,540 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werde, wird  ein <unk> <unk> <unk> was in den letzten 25 Jahren passiert ist.
2024-05-23 11:26:36,089 - INFO - joeynmt.training - Epoch   5, Step:    11100, Batch Loss:     1.453218, Batch Acc: 0.534079, Tokens per Sec:     3033, Lr: 0.000300
2024-05-23 11:26:56,877 - INFO - joeynmt.training - Epoch   5, Step:    11200, Batch Loss:     1.449504, Batch Acc: 0.530609, Tokens per Sec:     3117, Lr: 0.000300
2024-05-23 11:27:19,392 - INFO - joeynmt.training - Epoch   5, Step:    11300, Batch Loss:     1.424536, Batch Acc: 0.533905, Tokens per Sec:     2877, Lr: 0.000300
2024-05-23 11:27:41,682 - INFO - joeynmt.training - Epoch   5, Step:    11400, Batch Loss:     1.392047, Batch Acc: 0.535285, Tokens per Sec:     2954, Lr: 0.000300
2024-05-23 11:28:03,500 - INFO - joeynmt.training - Epoch   5, Step:    11500, Batch Loss:     1.485343, Batch Acc: 0.530197, Tokens per Sec:     2974, Lr: 0.000300
2024-05-23 11:28:03,501 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:28:03,501 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:28:35,240 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.41, acc:   0.48, generation: 31.7329[sec], evaluation: 0.0000[sec]
2024-05-23 11:28:35,593 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/9500.ckpt
2024-05-23 11:28:35,638 - INFO - joeynmt.training - Example #0
2024-05-23 11:28:35,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:28:35,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:28:35,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', '<unk>', '', '<unk>', 'die', '<unk>', '<unk>', '', '<unk>', 'die', '<unk>', '<unk>', '<unk>', '', 'die', 'Größe', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'hat', 'die', 'Größe', 'des', '<unk>', '<unk>', '<unk>', '', '<unk>', '</s>']
2024-05-23 11:28:35,638 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:28:35,638 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:28:35,638 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk> <unk>  <unk> die <unk> <unk>  <unk> die <unk> <unk> <unk>  die Größe der letzten drei Millionen Jahre <unk>  hat die Größe des <unk> <unk> <unk>  <unk>
2024-05-23 11:28:35,638 - INFO - joeynmt.training - Example #1
2024-05-23 11:28:35,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:28:35,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:28:35,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '<unk>', '', 'denn', 'es', 'zeigt', 'nicht', 'die', '<unk>', 'der', '<unk>', '</s>']
2024-05-23 11:28:35,638 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:28:35,638 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:28:35,638 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> <unk>  denn es zeigt nicht die <unk> der <unk>
2024-05-23 11:28:35,639 - INFO - joeynmt.training - Example #2
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', 'in', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 11:28:35,639 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:28:35,639 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:28:35,639 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist in <unk>  der <unk> <unk>
2024-05-23 11:28:35,639 - INFO - joeynmt.training - Example #3
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:28:35,639 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:28:35,639 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:28:35,639 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:28:35,639 - INFO - joeynmt.training - Example #4
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:28:35,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:28:35,640 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'wird.', '', '<unk>', '<unk>', 'der', 'letzten', '25', 'Jahre', '<unk>', '</s>']
2024-05-23 11:28:35,640 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:28:35,640 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:28:35,640 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen wird.  <unk> <unk> der letzten 25 Jahre <unk>
2024-05-23 11:29:00,147 - INFO - joeynmt.training - Epoch   5, Step:    11600, Batch Loss:     1.595780, Batch Acc: 0.535119, Tokens per Sec:     2584, Lr: 0.000300
2024-05-23 11:29:26,531 - INFO - joeynmt.training - Epoch   5, Step:    11700, Batch Loss:     1.378991, Batch Acc: 0.531705, Tokens per Sec:     2482, Lr: 0.000300
2024-05-23 11:29:53,025 - INFO - joeynmt.training - Epoch   5, Step:    11800, Batch Loss:     1.422549, Batch Acc: 0.527957, Tokens per Sec:     2406, Lr: 0.000300
2024-05-23 11:30:15,215 - INFO - joeynmt.training - Epoch   5, Step:    11900, Batch Loss:     1.417825, Batch Acc: 0.536960, Tokens per Sec:     2984, Lr: 0.000300
2024-05-23 11:30:38,003 - INFO - joeynmt.training - Epoch   5, Step:    12000, Batch Loss:     1.423143, Batch Acc: 0.529555, Tokens per Sec:     2939, Lr: 0.000300
2024-05-23 11:30:38,004 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:30:38,004 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:31:07,633 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.15, acc:   0.50, generation: 29.6225[sec], evaluation: 0.0000[sec]
2024-05-23 11:31:07,635 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 11:31:07,973 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/9000.ckpt
2024-05-23 11:31:08,015 - INFO - joeynmt.training - Example #0
2024-05-23 11:31:08,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:31:08,015 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:31:08,015 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', 'die', '<unk>', '<unk>', '<unk>', '<unk>', 'die', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:31:08,015 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:31:08,015 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:31:08,015 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass die <unk> <unk> <unk> <unk> die für die meisten <unk> drei Millionen Jahre <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:31:08,015 - INFO - joeynmt.training - Example #1
2024-05-23 11:31:08,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:31:08,015 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:31:08,015 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:31:08,015 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:31:08,015 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:31:08,016 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:31:08,016 - INFO - joeynmt.training - Example #2
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', 'ist', 'in', '<unk>', '', 'der', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:31:08,016 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:31:08,016 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:31:08,016 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> ist in <unk>  der <unk> <unk> <unk>
2024-05-23 11:31:08,016 - INFO - joeynmt.training - Example #3
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:31:08,016 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:31:08,016 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:31:08,016 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:31:08,016 - INFO - joeynmt.training - Example #4
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:31:08,016 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', '', 'ein', '<unk>', '<unk>', '<unk>', 'von', 'dem', 'letzten', '25', 'Jahre', '<unk>', '</s>']
2024-05-23 11:31:08,017 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:31:08,017 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:31:08,017 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen  ein <unk> <unk> <unk> von dem letzten 25 Jahre <unk>
2024-05-23 11:31:29,819 - INFO - joeynmt.training - Epoch   5, Step:    12100, Batch Loss:     1.585440, Batch Acc: 0.531648, Tokens per Sec:     2828, Lr: 0.000300
2024-05-23 11:31:51,163 - INFO - joeynmt.training - Epoch   5, Step:    12200, Batch Loss:     1.322171, Batch Acc: 0.531832, Tokens per Sec:     3004, Lr: 0.000300
2024-05-23 11:32:12,821 - INFO - joeynmt.training - Epoch   5, Step:    12300, Batch Loss:     1.536331, Batch Acc: 0.530087, Tokens per Sec:     3038, Lr: 0.000300
2024-05-23 11:32:34,566 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     1.623454, Batch Acc: 0.535492, Tokens per Sec:     2903, Lr: 0.000300
2024-05-23 11:32:56,041 - INFO - joeynmt.training - Epoch   5, Step:    12500, Batch Loss:     1.594084, Batch Acc: 0.529134, Tokens per Sec:     2995, Lr: 0.000300
2024-05-23 11:32:56,042 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:32:56,042 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:33:28,186 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.43, acc:   0.48, generation: 32.1379[sec], evaluation: 0.0000[sec]
2024-05-23 11:33:28,520 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/11000.ckpt
2024-05-23 11:33:28,621 - INFO - joeynmt.training - Example #0
2024-05-23 11:33:28,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:33:28,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:33:28,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', '<unk>', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '</s>']
2024-05-23 11:33:28,622 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:33:28,622 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:33:28,622 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk>  <unk> dass das <unk> Eis <unk>  <unk> die meisten <unk> drei Millionen Jahre <unk>  <unk> <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk>  <unk>
2024-05-23 11:33:28,622 - INFO - joeynmt.training - Example #1
2024-05-23 11:33:28,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:33:28,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:33:28,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', '<unk>', '</s>']
2024-05-23 11:33:28,622 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:33:28,622 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:33:28,622 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem <unk>  weil es nicht die <unk> <unk>
2024-05-23 11:33:28,622 - INFO - joeynmt.training - Example #2
2024-05-23 11:33:28,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:33:28,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:33:28,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', '<unk>', '', 'das', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:33:28,623 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:33:28,623 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:33:28,623 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> <unk>  das <unk> <unk> <unk>
2024-05-23 11:33:28,623 - INFO - joeynmt.training - Example #3
2024-05-23 11:33:28,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:33:28,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:33:28,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:33:28,623 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:33:28,623 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:33:28,623 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:33:28,623 - INFO - joeynmt.training - Example #4
2024-05-23 11:33:28,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:33:28,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:33:28,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'wird', '', 'ein', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:33:28,624 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:33:28,624 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:33:28,624 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen wird  ein <unk> <unk> <unk> <unk>
2024-05-23 11:33:51,977 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     1.341873, Batch Acc: 0.529264, Tokens per Sec:     2778, Lr: 0.000300
2024-05-23 11:34:13,846 - INFO - joeynmt.training - Epoch   5, Step:    12700, Batch Loss:     1.477742, Batch Acc: 0.530709, Tokens per Sec:     2977, Lr: 0.000300
2024-05-23 11:34:35,850 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     1.550589, Batch Acc: 0.528846, Tokens per Sec:     2963, Lr: 0.000300
2024-05-23 11:35:00,939 - INFO - joeynmt.training - Epoch   5, Step:    12900, Batch Loss:     1.413513, Batch Acc: 0.529948, Tokens per Sec:     2592, Lr: 0.000300
2024-05-23 11:35:23,102 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     1.489453, Batch Acc: 0.536348, Tokens per Sec:     2932, Lr: 0.000300
2024-05-23 11:35:23,103 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:35:23,103 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:35:58,807 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.13, acc:   0.49, generation: 35.6972[sec], evaluation: 0.0000[sec]
2024-05-23 11:35:58,808 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 11:35:59,221 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/12500.ckpt
2024-05-23 11:35:59,254 - INFO - joeynmt.training - Example #0
2024-05-23 11:35:59,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:35:59,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:35:59,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', 'die', 'die', '<unk>', '<unk>', '<unk>', '', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk> die die <unk> <unk> <unk>  die meisten <unk> drei Millionen Jahre <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:35:59,255 - INFO - joeynmt.training - Example #1
2024-05-23 11:35:59,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:35:59,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:35:59,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'das', '<unk>', 'dieses', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> das <unk> dieses Problem <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:35:59,255 - INFO - joeynmt.training - Example #2
2024-05-23 11:35:59,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:35:59,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:35:59,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:35:59,255 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:35:59,256 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> ist in einem <unk>  das <unk> Herz des globalen <unk>
2024-05-23 11:35:59,256 - INFO - joeynmt.training - Example #3
2024-05-23 11:35:59,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:35:59,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:35:59,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:35:59,256 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:35:59,256 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:35:59,256 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:35:59,256 - INFO - joeynmt.training - Example #4
2024-05-23 11:35:59,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:35:59,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:35:59,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werde,', '', 'ein', '<unk>', '<unk>', 'was', 'in', 'den', 'letzten', '25', 'Jahre', '<unk>', '</s>']
2024-05-23 11:35:59,256 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:35:59,256 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:35:59,256 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werde,  ein <unk> <unk> was in den letzten 25 Jahre <unk>
2024-05-23 11:36:20,508 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     1.379416, Batch Acc: 0.533515, Tokens per Sec:     2999, Lr: 0.000300
2024-05-23 11:36:41,687 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     1.435461, Batch Acc: 0.531817, Tokens per Sec:     3055, Lr: 0.000300
2024-05-23 11:36:42,561 - INFO - joeynmt.training - Epoch   5: total training loss 3911.93
2024-05-23 11:36:42,562 - INFO - joeynmt.training - EPOCH 6
2024-05-23 11:37:03,067 - INFO - joeynmt.training - Epoch   6, Step:    13300, Batch Loss:     1.460916, Batch Acc: 0.551856, Tokens per Sec:     3133, Lr: 0.000300
2024-05-23 11:37:23,862 - INFO - joeynmt.training - Epoch   6, Step:    13400, Batch Loss:     1.429071, Batch Acc: 0.551059, Tokens per Sec:     3024, Lr: 0.000300
2024-05-23 11:37:54,254 - INFO - joeynmt.training - Epoch   6, Step:    13500, Batch Loss:     1.448137, Batch Acc: 0.554919, Tokens per Sec:     2164, Lr: 0.000300
2024-05-23 11:37:54,255 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:37:54,255 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:38:45,612 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.48, generation: 51.3473[sec], evaluation: 0.0000[sec]
2024-05-23 11:38:46,011 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/11500.ckpt
2024-05-23 11:38:46,080 - INFO - joeynmt.training - Example #0
2024-05-23 11:38:46,080 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:38:46,080 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:38:46,080 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', 'die', 'die', '<unk>', '<unk>', '<unk>', '', 'die', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', 'hat', 'die', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', 'von', '40', 'Prozent', '<unk>', '</s>']
2024-05-23 11:38:46,080 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:38:46,081 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:38:46,081 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk> die die <unk> <unk> <unk>  die für die meisten <unk> drei Millionen Jahre <unk>  <unk> <unk> <unk> <unk> <unk>  hat die <unk> <unk> <unk>  <unk> <unk> <unk> von 40 Prozent <unk>
2024-05-23 11:38:46,081 - INFO - joeynmt.training - Example #1
2024-05-23 11:38:46,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:38:46,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:38:46,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'den', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:38:46,081 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:38:46,081 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:38:46,081 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem <unk>  weil es nicht den <unk> des <unk> <unk>
2024-05-23 11:38:46,081 - INFO - joeynmt.training - Example #2
2024-05-23 11:38:46,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:38:46,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:38:46,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 11:38:46,081 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:38:46,081 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:38:46,082 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  das <unk> Herz des globalen <unk>
2024-05-23 11:38:46,082 - INFO - joeynmt.training - Example #3
2024-05-23 11:38:46,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:38:46,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:38:46,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:38:46,082 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:38:46,082 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:38:46,082 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:38:46,082 - INFO - joeynmt.training - Example #4
2024-05-23 11:38:46,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:38:46,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:38:46,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werden.', '', 'eine', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:38:46,082 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:38:46,082 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:38:46,082 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werden.  eine <unk> <unk> von dem, was in den letzten 25 Jahren <unk>
2024-05-23 11:39:11,738 - INFO - joeynmt.training - Epoch   6, Step:    13600, Batch Loss:     1.452508, Batch Acc: 0.548553, Tokens per Sec:     2537, Lr: 0.000300
2024-05-23 11:39:38,777 - INFO - joeynmt.training - Epoch   6, Step:    13700, Batch Loss:     1.247377, Batch Acc: 0.548666, Tokens per Sec:     2460, Lr: 0.000300
2024-05-23 11:40:01,237 - INFO - joeynmt.training - Epoch   6, Step:    13800, Batch Loss:     1.390249, Batch Acc: 0.549618, Tokens per Sec:     2946, Lr: 0.000300
2024-05-23 11:40:27,984 - INFO - joeynmt.training - Epoch   6, Step:    13900, Batch Loss:     1.503860, Batch Acc: 0.548388, Tokens per Sec:     2443, Lr: 0.000300
2024-05-23 11:40:50,994 - INFO - joeynmt.training - Epoch   6, Step:    14000, Batch Loss:     1.447703, Batch Acc: 0.550989, Tokens per Sec:     2847, Lr: 0.000300
2024-05-23 11:40:50,995 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:40:50,995 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:41:20,621 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.23, acc:   0.49, generation: 29.6193[sec], evaluation: 0.0000[sec]
2024-05-23 11:41:20,985 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/8500.ckpt
2024-05-23 11:41:21,083 - INFO - joeynmt.training - Example #0
2024-05-23 11:41:21,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:41:21,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:41:21,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', '', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'das', 'ist', 'der', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:41:21,084 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:41:21,084 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:41:21,084 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk>  <unk> dass das <unk> Eis <unk> <unk>  die meisten drei Millionen Jahre <unk>  das ist der Größe der <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:41:21,084 - INFO - joeynmt.training - Example #1
2024-05-23 11:41:21,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:41:21,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:41:21,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', '', 'denn', 'es', 'zeigt', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:41:21,084 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:41:21,084 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:41:21,084 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem  denn es zeigt nicht die <unk> des <unk>
2024-05-23 11:41:21,084 - INFO - joeynmt.training - Example #2
2024-05-23 11:41:21,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:41:21,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:41:21,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Hypothesis: Das <unk> <unk> <unk> <unk> <unk>  das <unk> Herz des globalen <unk>
2024-05-23 11:41:21,085 - INFO - joeynmt.training - Example #3
2024-05-23 11:41:21,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:41:21,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:41:21,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:41:21,085 - INFO - joeynmt.training - Example #4
2024-05-23 11:41:21,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:41:21,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:41:21,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'wird', '', 'ein', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:41:21,085 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:41:21,086 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen wird  ein <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 11:41:43,064 - INFO - joeynmt.training - Epoch   6, Step:    14100, Batch Loss:     1.257338, Batch Acc: 0.546522, Tokens per Sec:     2961, Lr: 0.000300
2024-05-23 11:42:04,520 - INFO - joeynmt.training - Epoch   6, Step:    14200, Batch Loss:     1.369918, Batch Acc: 0.545464, Tokens per Sec:     2993, Lr: 0.000300
2024-05-23 11:42:26,965 - INFO - joeynmt.training - Epoch   6, Step:    14300, Batch Loss:     1.269681, Batch Acc: 0.549523, Tokens per Sec:     2960, Lr: 0.000300
2024-05-23 11:42:50,761 - INFO - joeynmt.training - Epoch   6, Step:    14400, Batch Loss:     1.445545, Batch Acc: 0.544794, Tokens per Sec:     2694, Lr: 0.000300
2024-05-23 11:43:18,018 - INFO - joeynmt.training - Epoch   6, Step:    14500, Batch Loss:     1.359774, Batch Acc: 0.547645, Tokens per Sec:     2388, Lr: 0.000300
2024-05-23 11:43:18,019 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:43:18,019 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:43:53,504 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.31, acc:   0.49, generation: 35.4779[sec], evaluation: 0.0000[sec]
2024-05-23 11:43:53,507 - INFO - joeynmt.training - Example #0
2024-05-23 11:43:53,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:43:53,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:43:53,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', 'so', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'das', '<unk>', '<unk>', '', 'das', '<unk>', '<unk>', '<unk>', '', 'das', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 11:43:53,507 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:43:53,507 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:43:53,507 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> so  <unk> dass das <unk> Eis <unk>  das <unk> <unk>  das <unk> <unk> <unk>  das <unk> <unk> <unk>  <unk> <unk> <unk> <unk>  <unk> <unk>
2024-05-23 11:43:53,507 - INFO - joeynmt.training - Example #1
2024-05-23 11:43:53,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:43:53,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:43:53,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:43:53,508 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:43:53,508 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:43:53,508 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:43:53,508 - INFO - joeynmt.training - Example #2
2024-05-23 11:43:53,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:43:53,508 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:43:53,508 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'im', '<unk>', '', 'das', '<unk>', 'Herz', 'der', '<unk>', '</s>']
2024-05-23 11:43:53,508 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:43:53,508 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:43:53,508 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist im <unk>  das <unk> Herz der <unk>
2024-05-23 11:43:53,508 - INFO - joeynmt.training - Example #3
2024-05-23 11:43:53,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:43:53,508 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:43:53,508 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:43:53,509 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:43:53,509 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:43:53,509 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:43:53,509 - INFO - joeynmt.training - Example #4
2024-05-23 11:43:53,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:43:53,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:43:53,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werden.', '', 'ein', '<unk>', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'ist.', '</s>']
2024-05-23 11:43:53,509 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:43:53,509 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:43:53,509 - INFO - joeynmt.training - 	Hypothesis: Das nächste <unk> die ich Ihnen zeigen werden.  ein <unk> <unk> <unk> von dem, was in den letzten 25 Jahren ist.
2024-05-23 11:44:15,738 - INFO - joeynmt.training - Epoch   6, Step:    14600, Batch Loss:     1.466513, Batch Acc: 0.541183, Tokens per Sec:     2967, Lr: 0.000300
2024-05-23 11:44:37,898 - INFO - joeynmt.training - Epoch   6, Step:    14700, Batch Loss:     1.385658, Batch Acc: 0.541376, Tokens per Sec:     2935, Lr: 0.000300
2024-05-23 11:44:59,325 - INFO - joeynmt.training - Epoch   6, Step:    14800, Batch Loss:     1.355814, Batch Acc: 0.540869, Tokens per Sec:     3002, Lr: 0.000300
2024-05-23 11:45:21,238 - INFO - joeynmt.training - Epoch   6, Step:    14900, Batch Loss:     1.427722, Batch Acc: 0.543689, Tokens per Sec:     3015, Lr: 0.000300
2024-05-23 11:45:43,904 - INFO - joeynmt.training - Epoch   6, Step:    15000, Batch Loss:     1.393227, Batch Acc: 0.539868, Tokens per Sec:     2830, Lr: 0.000300
2024-05-23 11:45:43,905 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:45:43,905 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:46:09,695 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.32, acc:   0.48, generation: 25.7839[sec], evaluation: 0.0000[sec]
2024-05-23 11:46:09,697 - INFO - joeynmt.training - Example #0
2024-05-23 11:46:09,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:46:09,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:46:09,697 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'also', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', '<unk>', '<unk>', '', 'welche', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:46:09,698 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:46:09,698 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:46:09,698 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> also <unk>  <unk> dass das <unk> <unk> <unk>  welche die meisten letzten drei Millionen Jahre <unk>  war die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:46:09,698 - INFO - joeynmt.training - Example #1
2024-05-23 11:46:09,698 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:46:09,698 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:46:09,698 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieser', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:46:09,698 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:46:09,698 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:46:09,698 - INFO - joeynmt.training - 	Hypothesis: Aber dieser <unk> die <unk> dieses <unk> <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:46:09,698 - INFO - joeynmt.training - Example #2
2024-05-23 11:46:09,698 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:46:09,698 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:46:09,698 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 11:46:09,699 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:46:09,699 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:46:09,699 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  das <unk> Herz des globalen <unk>
2024-05-23 11:46:09,699 - INFO - joeynmt.training - Example #3
2024-05-23 11:46:09,699 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:46:09,699 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:46:09,699 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '</s>']
2024-05-23 11:46:09,699 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:46:09,699 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:46:09,699 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>
2024-05-23 11:46:09,699 - INFO - joeynmt.training - Example #4
2024-05-23 11:46:09,699 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:46:09,699 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:46:09,699 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren.', '</s>']
2024-05-23 11:46:09,700 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:46:09,700 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:46:09,700 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die Ihnen <unk>  ein <unk> <unk> von dem, was in den letzten 25 Jahren.
2024-05-23 11:46:30,885 - INFO - joeynmt.training - Epoch   6, Step:    15100, Batch Loss:     1.392467, Batch Acc: 0.539011, Tokens per Sec:     3078, Lr: 0.000300
2024-05-23 11:46:52,848 - INFO - joeynmt.training - Epoch   6, Step:    15200, Batch Loss:     1.367363, Batch Acc: 0.542544, Tokens per Sec:     2955, Lr: 0.000300
2024-05-23 11:47:14,437 - INFO - joeynmt.training - Epoch   6, Step:    15300, Batch Loss:     1.473036, Batch Acc: 0.538659, Tokens per Sec:     2983, Lr: 0.000300
2024-05-23 11:47:35,759 - INFO - joeynmt.training - Epoch   6, Step:    15400, Batch Loss:     1.382461, Batch Acc: 0.541052, Tokens per Sec:     3060, Lr: 0.000300
2024-05-23 11:47:58,033 - INFO - joeynmt.training - Epoch   6, Step:    15500, Batch Loss:     1.468066, Batch Acc: 0.543957, Tokens per Sec:     2870, Lr: 0.000300
2024-05-23 11:47:58,035 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:47:58,035 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:48:39,105 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.05, acc:   0.50, generation: 41.0634[sec], evaluation: 0.0000[sec]
2024-05-23 11:48:39,107 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 11:48:39,545 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/13500.ckpt
2024-05-23 11:48:39,577 - INFO - joeynmt.training - Example #0
2024-05-23 11:48:39,577 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:48:39,577 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:48:39,577 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', '<unk>', 'was', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', 'hat', 'die', '<unk>', '<unk>', '</s>']
2024-05-23 11:48:39,577 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:48:39,577 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:48:39,577 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk> dass das <unk> Eis <unk> <unk> <unk> was für die meisten <unk> drei Millionen Jahre <unk>  hat die Größe der <unk> <unk> <unk>  hat die Größe der <unk> <unk> <unk>  hat die <unk> <unk>
2024-05-23 11:48:39,577 - INFO - joeynmt.training - Example #1
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'das', '<unk>', 'dieses', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 11:48:39,578 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:48:39,578 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:48:39,578 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> das <unk> dieses Problem <unk>  weil es nicht die <unk> der <unk> <unk>
2024-05-23 11:48:39,578 - INFO - joeynmt.training - Example #2
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'im', '<unk>', '', 'das', '<unk>', 'Herz', 'des', '<unk>', '</s>']
2024-05-23 11:48:39,578 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:48:39,578 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:48:39,578 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist im <unk>  das <unk> Herz des <unk>
2024-05-23 11:48:39,578 - INFO - joeynmt.training - Example #3
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:48:39,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '</s>']
2024-05-23 11:48:39,579 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:48:39,579 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:48:39,579 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>
2024-05-23 11:48:39,579 - INFO - joeynmt.training - Example #4
2024-05-23 11:48:39,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:48:39,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:48:39,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werde,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 11:48:39,579 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:48:39,579 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:48:39,579 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werde, was in den letzten 25 Jahren passiert ist.
2024-05-23 11:49:00,007 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     1.525109, Batch Acc: 0.538927, Tokens per Sec:     3128, Lr: 0.000300
2024-05-23 11:49:20,439 - INFO - joeynmt.training - Epoch   6, Step:    15700, Batch Loss:     1.384648, Batch Acc: 0.536606, Tokens per Sec:     3161, Lr: 0.000300
2024-05-23 11:49:43,571 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     1.466022, Batch Acc: 0.541892, Tokens per Sec:     2808, Lr: 0.000300
2024-05-23 11:49:53,879 - INFO - joeynmt.training - Epoch   6: total training loss 3771.25
2024-05-23 11:49:53,879 - INFO - joeynmt.training - EPOCH 7
2024-05-23 11:50:08,823 - INFO - joeynmt.training - Epoch   7, Step:    15900, Batch Loss:     1.371960, Batch Acc: 0.563080, Tokens per Sec:     2720, Lr: 0.000300
2024-05-23 11:50:29,344 - INFO - joeynmt.training - Epoch   7, Step:    16000, Batch Loss:     1.244289, Batch Acc: 0.562750, Tokens per Sec:     3112, Lr: 0.000300
2024-05-23 11:50:29,346 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:50:29,346 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:51:02,380 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.37, acc:   0.49, generation: 33.0268[sec], evaluation: 0.0000[sec]
2024-05-23 11:51:02,382 - INFO - joeynmt.training - Example #0
2024-05-23 11:51:02,382 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:51:02,383 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:51:02,383 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', '', '<unk>', 'das', '<unk>', '<unk>', '', '<unk>', 'der', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'war', 'die', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '</s>']
2024-05-23 11:51:02,383 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:51:02,383 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:51:02,383 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass  <unk> das <unk> <unk>  <unk> der <unk> drei Millionen Jahre <unk>  war die <unk> <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk>  <unk>
2024-05-23 11:51:02,383 - INFO - joeynmt.training - Example #1
2024-05-23 11:51:02,383 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:51:02,383 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:51:02,383 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:51:02,383 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:51:02,383 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:51:02,384 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:51:02,384 - INFO - joeynmt.training - Example #2
2024-05-23 11:51:02,384 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:51:02,384 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:51:02,384 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'das', '<unk>', '', 'das', '<unk>', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 11:51:02,384 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:51:02,384 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:51:02,384 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  das <unk>  das <unk> des globalen <unk>
2024-05-23 11:51:02,384 - INFO - joeynmt.training - Example #3
2024-05-23 11:51:02,384 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:51:02,384 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:51:02,384 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '</s>']
2024-05-23 11:51:02,384 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:51:02,384 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:51:02,385 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>
2024-05-23 11:51:02,385 - INFO - joeynmt.training - Example #4
2024-05-23 11:51:02,385 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:51:02,385 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:51:02,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'eine', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahre', '<unk>', 'ist.', '</s>']
2024-05-23 11:51:02,385 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:51:02,385 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:51:02,385 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  eine <unk> <unk> von dem, was in den letzten 25 Jahre <unk> ist.
2024-05-23 11:51:23,466 - INFO - joeynmt.training - Epoch   7, Step:    16100, Batch Loss:     1.344892, Batch Acc: 0.562755, Tokens per Sec:     3029, Lr: 0.000300
2024-05-23 11:51:44,922 - INFO - joeynmt.training - Epoch   7, Step:    16200, Batch Loss:     1.366404, Batch Acc: 0.562761, Tokens per Sec:     3025, Lr: 0.000300
2024-05-23 11:52:06,370 - INFO - joeynmt.training - Epoch   7, Step:    16300, Batch Loss:     1.448334, Batch Acc: 0.561545, Tokens per Sec:     3005, Lr: 0.000300
2024-05-23 11:52:28,218 - INFO - joeynmt.training - Epoch   7, Step:    16400, Batch Loss:     1.415640, Batch Acc: 0.560599, Tokens per Sec:     2943, Lr: 0.000300
2024-05-23 11:52:49,482 - INFO - joeynmt.training - Epoch   7, Step:    16500, Batch Loss:     1.468884, Batch Acc: 0.558189, Tokens per Sec:     3078, Lr: 0.000300
2024-05-23 11:52:49,484 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:52:49,484 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:53:29,391 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.23, acc:   0.49, generation: 39.8999[sec], evaluation: 0.0000[sec]
2024-05-23 11:53:29,790 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/10500.ckpt
2024-05-23 11:53:29,868 - INFO - joeynmt.training - Example #0
2024-05-23 11:53:29,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:53:29,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:53:29,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', 'die', 'die', '<unk>', '<unk>', '', '<unk>', 'die', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:53:29,869 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:53:29,869 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:53:29,869 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk> die die <unk> <unk>  <unk> die für die meisten <unk> drei Millionen Jahre  <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:53:29,869 - INFO - joeynmt.training - Example #1
2024-05-23 11:53:29,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:53:29,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:53:29,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 11:53:29,870 - INFO - joeynmt.training - Example #2
2024-05-23 11:53:29,870 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:53:29,870 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:53:29,870 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'der', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  der <unk> <unk> <unk>
2024-05-23 11:53:29,870 - INFO - joeynmt.training - Example #3
2024-05-23 11:53:29,870 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:53:29,870 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:53:29,870 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:53:29,870 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:53:29,871 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:53:29,871 - INFO - joeynmt.training - Example #4
2024-05-23 11:53:29,871 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:53:29,871 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:53:29,871 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werde,', '', 'eine', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:53:29,871 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:53:29,871 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:53:29,871 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werde,  eine <unk> <unk> von dem, was in den letzten 25 Jahren <unk>
2024-05-23 11:53:51,750 - INFO - joeynmt.training - Epoch   7, Step:    16600, Batch Loss:     1.492180, Batch Acc: 0.560602, Tokens per Sec:     3012, Lr: 0.000300
2024-05-23 11:54:15,275 - INFO - joeynmt.training - Epoch   7, Step:    16700, Batch Loss:     1.490734, Batch Acc: 0.555557, Tokens per Sec:     2787, Lr: 0.000300
2024-05-23 11:54:38,776 - INFO - joeynmt.training - Epoch   7, Step:    16800, Batch Loss:     1.474388, Batch Acc: 0.555436, Tokens per Sec:     2762, Lr: 0.000300
2024-05-23 11:55:01,066 - INFO - joeynmt.training - Epoch   7, Step:    16900, Batch Loss:     1.559022, Batch Acc: 0.555894, Tokens per Sec:     2917, Lr: 0.000300
2024-05-23 11:55:23,423 - INFO - joeynmt.training - Epoch   7, Step:    17000, Batch Loss:     1.467393, Batch Acc: 0.554170, Tokens per Sec:     2866, Lr: 0.000300
2024-05-23 11:55:23,424 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:55:23,424 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:56:04,448 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.32, acc:   0.49, generation: 41.0170[sec], evaluation: 0.0000[sec]
2024-05-23 11:56:04,450 - INFO - joeynmt.training - Example #0
2024-05-23 11:56:04,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:56:04,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:56:04,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', 'die', 'das', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '', '<unk>', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', 'hat.', '', '<unk>', 'hat', 'sich', '<unk>', '40', 'Prozent', '<unk>', '</s>']
2024-05-23 11:56:04,450 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:56:04,450 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:56:04,450 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk> die das <unk> <unk>  <unk> <unk> <unk>  <unk> der letzten drei Millionen Jahre <unk>  war die Größe der <unk> <unk>  <unk> <unk> <unk> hat.  <unk> hat sich <unk> 40 Prozent <unk>
2024-05-23 11:56:04,451 - INFO - joeynmt.training - Example #1
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 11:56:04,451 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:56:04,451 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:56:04,451 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> <unk>  weil es nicht die <unk> des <unk>
2024-05-23 11:56:04,451 - INFO - joeynmt.training - Example #2
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'das', '<unk>', 'Herz', 'des', '<unk>', '</s>']
2024-05-23 11:56:04,451 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:56:04,451 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:56:04,451 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  das <unk> Herz des <unk>
2024-05-23 11:56:04,451 - INFO - joeynmt.training - Example #3
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:56:04,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '</s>']
2024-05-23 11:56:04,452 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:56:04,452 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:56:04,452 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk>
2024-05-23 11:56:04,452 - INFO - joeynmt.training - Example #4
2024-05-23 11:56:04,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:56:04,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:56:04,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 11:56:04,452 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:56:04,452 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:56:04,452 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  <unk> <unk> von dem, was in den letzten 25 Jahren <unk>
2024-05-23 11:56:28,028 - INFO - joeynmt.training - Epoch   7, Step:    17100, Batch Loss:     1.514599, Batch Acc: 0.555226, Tokens per Sec:     2777, Lr: 0.000300
2024-05-23 11:56:50,220 - INFO - joeynmt.training - Epoch   7, Step:    17200, Batch Loss:     1.417376, Batch Acc: 0.556894, Tokens per Sec:     2915, Lr: 0.000300
2024-05-23 11:57:11,504 - INFO - joeynmt.training - Epoch   7, Step:    17300, Batch Loss:     1.461534, Batch Acc: 0.550505, Tokens per Sec:     3052, Lr: 0.000300
2024-05-23 11:57:32,925 - INFO - joeynmt.training - Epoch   7, Step:    17400, Batch Loss:     1.294388, Batch Acc: 0.550300, Tokens per Sec:     3092, Lr: 0.000300
2024-05-23 11:57:54,646 - INFO - joeynmt.training - Epoch   7, Step:    17500, Batch Loss:     1.407483, Batch Acc: 0.555347, Tokens per Sec:     2990, Lr: 0.000300
2024-05-23 11:57:54,646 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 11:57:54,646 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 11:58:27,507 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.14, acc:   0.49, generation: 32.8538[sec], evaluation: 0.0000[sec]
2024-05-23 11:58:27,923 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/16500.ckpt
2024-05-23 11:58:27,948 - INFO - joeynmt.helpers - delete /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_wordlevel/16500.ckpt
2024-05-23 11:58:27,948 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_wordlevel/16500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_wordlevel/16500.ckpt')
2024-05-23 11:58:27,948 - INFO - joeynmt.training - Example #0
2024-05-23 11:58:27,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 11:58:27,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 11:58:27,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'welche', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 11:58:27,949 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 11:58:27,949 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 11:58:27,949 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>  <unk> dass das <unk> Eis <unk>  welche für die meisten letzten drei Millionen Jahre <unk>  war die Größe der <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 11:58:27,949 - INFO - joeynmt.training - Example #1
2024-05-23 11:58:27,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 11:58:27,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 11:58:27,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'den', '<unk>', '<unk>', '</s>']
2024-05-23 11:58:27,949 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 11:58:27,949 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 11:58:27,949 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  weil es nicht den <unk> <unk>
2024-05-23 11:58:27,949 - INFO - joeynmt.training - Example #2
2024-05-23 11:58:27,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 11:58:27,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 11:58:27,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', 'Herz', 'des', '<unk>', '</s>']
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  das <unk> Herz des <unk>
2024-05-23 11:58:27,950 - INFO - joeynmt.training - Example #3
2024-05-23 11:58:27,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 11:58:27,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 11:58:27,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 11:58:27,950 - INFO - joeynmt.training - Example #4
2024-05-23 11:58:27,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 11:58:27,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 11:58:27,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', 'werden.', '', 'ein', '<unk>', '<unk>', 'von', 'dem', '<unk>', 'in', 'den', 'letzten', '25', 'Jahre', '<unk>', '</s>']
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 11:58:27,950 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 11:58:27,951 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk> werden.  ein <unk> <unk> von dem <unk> in den letzten 25 Jahre <unk>
2024-05-23 11:58:51,641 - INFO - joeynmt.training - Epoch   7, Step:    17600, Batch Loss:     1.310919, Batch Acc: 0.555059, Tokens per Sec:     2742, Lr: 0.000300
2024-05-23 11:59:12,613 - INFO - joeynmt.training - Epoch   7, Step:    17700, Batch Loss:     1.502664, Batch Acc: 0.547907, Tokens per Sec:     3089, Lr: 0.000300
2024-05-23 11:59:34,854 - INFO - joeynmt.training - Epoch   7, Step:    17800, Batch Loss:     1.386607, Batch Acc: 0.550379, Tokens per Sec:     2920, Lr: 0.000300
2024-05-23 11:59:56,767 - INFO - joeynmt.training - Epoch   7, Step:    17900, Batch Loss:     1.386547, Batch Acc: 0.548463, Tokens per Sec:     2954, Lr: 0.000300
2024-05-23 12:00:18,243 - INFO - joeynmt.training - Epoch   7, Step:    18000, Batch Loss:     1.491314, Batch Acc: 0.549288, Tokens per Sec:     3092, Lr: 0.000300
2024-05-23 12:00:18,245 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:00:18,245 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:00:56,855 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.27, acc:   0.49, generation: 38.6029[sec], evaluation: 0.0000[sec]
2024-05-23 12:00:56,858 - INFO - joeynmt.training - Example #0
2024-05-23 12:00:56,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:00:56,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:00:56,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', '', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', 'die', 'meisten', '<unk>', '<unk>', '', '<unk>', 'der', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:00:56,858 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:00:56,858 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:00:56,858 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass  <unk> <unk> <unk> <unk>  <unk> die meisten <unk> <unk>  <unk> der Größe der <unk> <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 12:00:56,858 - INFO - joeynmt.training - Example #1
2024-05-23 12:00:56,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:00:56,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:00:56,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 12:00:56,859 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:00:56,859 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:00:56,859 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 12:00:56,859 - INFO - joeynmt.training - Example #2
2024-05-23 12:00:56,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:00:56,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:00:56,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', '<unk>', '<unk>', '', 'der', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 12:00:56,859 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:00:56,859 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:00:56,859 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist <unk> <unk>  der <unk> des <unk> <unk>
2024-05-23 12:00:56,859 - INFO - joeynmt.training - Example #3
2024-05-23 12:00:56,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:00:56,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:00:56,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:00:56,860 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:00:56,860 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:00:56,860 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:00:56,860 - INFO - joeynmt.training - Example #4
2024-05-23 12:00:56,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:00:56,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:00:56,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:00:56,860 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:00:56,860 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:00:56,860 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 12:01:19,067 - INFO - joeynmt.training - Epoch   7, Step:    18100, Batch Loss:     1.488217, Batch Acc: 0.547300, Tokens per Sec:     2912, Lr: 0.000300
2024-05-23 12:01:40,812 - INFO - joeynmt.training - Epoch   7, Step:    18200, Batch Loss:     1.496822, Batch Acc: 0.547633, Tokens per Sec:     3023, Lr: 0.000300
2024-05-23 12:02:03,301 - INFO - joeynmt.training - Epoch   7, Step:    18300, Batch Loss:     1.429661, Batch Acc: 0.549925, Tokens per Sec:     2889, Lr: 0.000300
2024-05-23 12:02:24,749 - INFO - joeynmt.training - Epoch   7, Step:    18400, Batch Loss:     1.360531, Batch Acc: 0.548096, Tokens per Sec:     3042, Lr: 0.000300
2024-05-23 12:02:41,206 - INFO - joeynmt.training - Epoch   7: total training loss 3676.03
2024-05-23 12:02:41,207 - INFO - joeynmt.training - EPOCH 8
2024-05-23 12:02:46,322 - INFO - joeynmt.training - Epoch   8, Step:    18500, Batch Loss:     1.169083, Batch Acc: 0.579425, Tokens per Sec:     3164, Lr: 0.000300
2024-05-23 12:02:46,323 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:02:46,323 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:03:14,028 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.11, acc:   0.49, generation: 27.6981[sec], evaluation: 0.0000[sec]
2024-05-23 12:03:14,376 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/14000.ckpt
2024-05-23 12:03:14,448 - INFO - joeynmt.training - Example #0
2024-05-23 12:03:14,448 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:03:14,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:03:14,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', '', '<unk>', '<unk>', '<unk>', '<unk>', 'das', '<unk>', '<unk>', '<unk>', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'das', '<unk>', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 12:03:14,449 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:03:14,449 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:03:14,449 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass  <unk> <unk> <unk> <unk> das <unk> <unk> <unk> das für die meisten letzten drei Millionen Jahre <unk>  das <unk> der <unk> <unk> <unk>  <unk> <unk>
2024-05-23 12:03:14,449 - INFO - joeynmt.training - Example #1
2024-05-23 12:03:14,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:03:14,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:03:14,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 12:03:14,449 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> Problem <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 12:03:14,450 - INFO - joeynmt.training - Example #2
2024-05-23 12:03:14,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:03:14,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:03:14,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', '<unk>', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist <unk> <unk>  das <unk> Herz des globalen <unk>
2024-05-23 12:03:14,450 - INFO - joeynmt.training - Example #3
2024-05-23 12:03:14,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:03:14,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:03:14,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '</s>']
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:03:14,450 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>
2024-05-23 12:03:14,450 - INFO - joeynmt.training - Example #4
2024-05-23 12:03:14,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:03:14,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:03:14,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'eine', '<unk>', '<unk>', 'von', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:03:14,451 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:03:14,451 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:03:14,451 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  eine <unk> <unk> von <unk> <unk> <unk>
2024-05-23 12:03:38,632 - INFO - joeynmt.training - Epoch   8, Step:    18600, Batch Loss:     1.271474, Batch Acc: 0.572524, Tokens per Sec:     2640, Lr: 0.000300
2024-05-23 12:04:01,199 - INFO - joeynmt.training - Epoch   8, Step:    18700, Batch Loss:     1.370598, Batch Acc: 0.573465, Tokens per Sec:     2869, Lr: 0.000300
2024-05-23 12:04:23,820 - INFO - joeynmt.training - Epoch   8, Step:    18800, Batch Loss:     1.351258, Batch Acc: 0.572343, Tokens per Sec:     2881, Lr: 0.000300
2024-05-23 12:04:46,410 - INFO - joeynmt.training - Epoch   8, Step:    18900, Batch Loss:     1.470876, Batch Acc: 0.566916, Tokens per Sec:     2880, Lr: 0.000300
2024-05-23 12:05:09,159 - INFO - joeynmt.training - Epoch   8, Step:    19000, Batch Loss:     1.325270, Batch Acc: 0.572232, Tokens per Sec:     2876, Lr: 0.000300
2024-05-23 12:05:09,160 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:05:09,160 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:05:39,746 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.30, acc:   0.48, generation: 30.5791[sec], evaluation: 0.0000[sec]
2024-05-23 12:05:39,748 - INFO - joeynmt.training - Example #0
2024-05-23 12:05:39,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:05:39,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:05:39,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', 'die', 'die', 'meisten', '<unk>', '<unk>', '', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'war', 'die', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 12:05:39,749 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:05:39,749 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:05:39,749 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk> die die meisten <unk> <unk>  die meisten letzten drei Millionen Jahre <unk>  war die <unk> <unk> <unk>  <unk> <unk> <unk> <unk>  <unk> <unk>
2024-05-23 12:05:39,749 - INFO - joeynmt.training - Example #1
2024-05-23 12:05:39,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:05:39,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:05:39,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 12:05:39,749 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:05:39,749 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:05:39,749 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> Problem <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 12:05:39,750 - INFO - joeynmt.training - Example #2
2024-05-23 12:05:39,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:05:39,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:05:39,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'der', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:05:39,750 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:05:39,750 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:05:39,750 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  der <unk> <unk> <unk>
2024-05-23 12:05:39,750 - INFO - joeynmt.training - Example #3
2024-05-23 12:05:39,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:05:39,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:05:39,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:05:39,750 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:05:39,750 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:05:39,750 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:05:39,750 - INFO - joeynmt.training - Example #4
2024-05-23 12:05:39,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:05:39,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:05:39,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werden.', '', 'eine', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 12:05:39,751 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:05:39,751 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:05:39,751 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werden.  eine <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 12:06:05,770 - INFO - joeynmt.training - Epoch   8, Step:    19100, Batch Loss:     1.273634, Batch Acc: 0.568929, Tokens per Sec:     2479, Lr: 0.000300
2024-05-23 12:06:28,631 - INFO - joeynmt.training - Epoch   8, Step:    19200, Batch Loss:     1.287019, Batch Acc: 0.564538, Tokens per Sec:     2882, Lr: 0.000300
2024-05-23 12:06:53,630 - INFO - joeynmt.training - Epoch   8, Step:    19300, Batch Loss:     1.421948, Batch Acc: 0.565642, Tokens per Sec:     2649, Lr: 0.000300
2024-05-23 12:07:16,090 - INFO - joeynmt.training - Epoch   8, Step:    19400, Batch Loss:     1.271344, Batch Acc: 0.567487, Tokens per Sec:     2995, Lr: 0.000300
2024-05-23 12:07:38,689 - INFO - joeynmt.training - Epoch   8, Step:    19500, Batch Loss:     1.396087, Batch Acc: 0.568099, Tokens per Sec:     2862, Lr: 0.000300
2024-05-23 12:07:38,690 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:07:38,690 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:08:22,686 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.08, acc:   0.50, generation: 43.9886[sec], evaluation: 0.0000[sec]
2024-05-23 12:08:23,076 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/12000.ckpt
2024-05-23 12:08:23,115 - INFO - joeynmt.training - Example #0
2024-05-23 12:08:23,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:08:23,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:08:23,116 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', 'so', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', '', 'welches', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'ist', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', 'hat', '<unk>', '40', 'Prozent.', '</s>']
2024-05-23 12:08:23,116 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:08:23,116 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:08:23,116 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> so <unk>  <unk> dass das <unk> Eis <unk> <unk>  welches für die meisten letzten drei Millionen Jahre <unk>  ist die Größe der <unk> <unk> <unk>  hat <unk> 40 Prozent.
2024-05-23 12:08:23,116 - INFO - joeynmt.training - Example #1
2024-05-23 12:08:23,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:08:23,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:08:23,116 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'das', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 12:08:23,117 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:08:23,117 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:08:23,117 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> das <unk> dieses <unk> Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 12:08:23,117 - INFO - joeynmt.training - Example #2
2024-05-23 12:08:23,117 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:08:23,117 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:08:23,117 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 12:08:23,117 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:08:23,117 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:08:23,117 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk> <unk>  der <unk> <unk>
2024-05-23 12:08:23,117 - INFO - joeynmt.training - Example #3
2024-05-23 12:08:23,117 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:08:23,117 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:08:23,117 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:08:23,117 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:08:23,118 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:08:23,118 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:08:23,118 - INFO - joeynmt.training - Example #4
2024-05-23 12:08:23,118 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:08:23,118 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:08:23,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', 'von', 'dem', '<unk>', '<unk>', '</s>']
2024-05-23 12:08:23,118 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:08:23,118 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:08:23,118 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> <unk> von dem <unk> <unk>
2024-05-23 12:08:45,820 - INFO - joeynmt.training - Epoch   8, Step:    19600, Batch Loss:     1.370445, Batch Acc: 0.562000, Tokens per Sec:     2892, Lr: 0.000300
2024-05-23 12:09:09,411 - INFO - joeynmt.training - Epoch   8, Step:    19700, Batch Loss:     1.310791, Batch Acc: 0.564403, Tokens per Sec:     2724, Lr: 0.000300
2024-05-23 12:09:32,438 - INFO - joeynmt.training - Epoch   8, Step:    19800, Batch Loss:     1.366604, Batch Acc: 0.558582, Tokens per Sec:     2824, Lr: 0.000300
2024-05-23 12:09:57,577 - INFO - joeynmt.training - Epoch   8, Step:    19900, Batch Loss:     1.404170, Batch Acc: 0.562403, Tokens per Sec:     2607, Lr: 0.000300
2024-05-23 12:10:20,414 - INFO - joeynmt.training - Epoch   8, Step:    20000, Batch Loss:     1.399749, Batch Acc: 0.558863, Tokens per Sec:     2850, Lr: 0.000300
2024-05-23 12:10:20,415 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:10:20,415 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:11:00,278 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.11, acc:   0.49, generation: 39.8564[sec], evaluation: 0.0000[sec]
2024-05-23 12:11:00,653 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/17500.ckpt
2024-05-23 12:11:00,740 - INFO - joeynmt.training - Example #0
2024-05-23 12:11:00,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:11:00,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:11:00,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', '<unk>', '<unk>', 'die', 'das', '<unk>', 'Eis', '<unk>', '', '<unk>', 'das', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '', '<unk>', 'ist', 'die', 'Größe', 'der', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:11:00,741 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:11:00,741 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:11:00,741 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> <unk> <unk> die das <unk> Eis <unk>  <unk> das für die meisten <unk> drei Millionen Jahre  <unk> ist die Größe der <unk> <unk>  <unk> <unk> <unk> <unk>
2024-05-23 12:11:00,741 - INFO - joeynmt.training - Example #1
2024-05-23 12:11:00,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:11:00,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:11:00,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'das', '<unk>', 'dieses', '<unk>', '<unk>', '', 'denn', 'es', '<unk>', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> das <unk> dieses <unk> <unk>  denn es <unk> nicht die <unk> des <unk>
2024-05-23 12:11:00,742 - INFO - joeynmt.training - Example #2
2024-05-23 12:11:00,742 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:11:00,742 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:11:00,742 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '<unk>', '', 'der', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk> <unk>  der <unk> des <unk> <unk>
2024-05-23 12:11:00,742 - INFO - joeynmt.training - Example #3
2024-05-23 12:11:00,742 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:11:00,742 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:11:00,742 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:11:00,742 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:11:00,743 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:11:00,743 - INFO - joeynmt.training - Example #4
2024-05-23 12:11:00,743 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:11:00,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:11:00,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', '<unk>', '<unk>', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 12:11:00,743 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:11:00,743 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:11:00,743 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  <unk> <unk> in den letzten 25 Jahren passiert ist.
2024-05-23 12:11:23,635 - INFO - joeynmt.training - Epoch   8, Step:    20100, Batch Loss:     1.454808, Batch Acc: 0.562511, Tokens per Sec:     2720, Lr: 0.000210
2024-05-23 12:11:46,794 - INFO - joeynmt.training - Epoch   8, Step:    20200, Batch Loss:     1.272578, Batch Acc: 0.566370, Tokens per Sec:     2784, Lr: 0.000210
2024-05-23 12:12:12,049 - INFO - joeynmt.training - Epoch   8, Step:    20300, Batch Loss:     1.358921, Batch Acc: 0.567035, Tokens per Sec:     2534, Lr: 0.000210
2024-05-23 12:12:39,145 - INFO - joeynmt.training - Epoch   8, Step:    20400, Batch Loss:     1.326094, Batch Acc: 0.569230, Tokens per Sec:     2460, Lr: 0.000210
2024-05-23 12:13:05,192 - INFO - joeynmt.training - Epoch   8, Step:    20500, Batch Loss:     1.385503, Batch Acc: 0.570001, Tokens per Sec:     2479, Lr: 0.000210
2024-05-23 12:13:05,194 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:13:05,194 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:13:39,844 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.10, acc:   0.50, generation: 34.6426[sec], evaluation: 0.0000[sec]
2024-05-23 12:13:40,219 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/13000.ckpt
2024-05-23 12:13:40,349 - INFO - joeynmt.training - Example #0
2024-05-23 12:13:40,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:13:40,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:13:40,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'so', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'das', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'ist', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', 'hat', 'sich', '40', 'Prozent', '<unk>', '</s>']
2024-05-23 12:13:40,350 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:13:40,350 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:13:40,350 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> so  <unk> dass das <unk> Eis <unk>  das für die meisten <unk> drei Millionen Jahre <unk>  ist die Größe der <unk> <unk> <unk>  hat sich 40 Prozent <unk>
2024-05-23 12:13:40,350 - INFO - joeynmt.training - Example #1
2024-05-23 12:13:40,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:13:40,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:13:40,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 12:13:40,350 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:13:40,350 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:13:40,350 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 12:13:40,350 - INFO - joeynmt.training - Example #2
2024-05-23 12:13:40,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'der', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 12:13:40,351 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:13:40,351 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:13:40,351 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  der <unk>  der <unk> <unk>
2024-05-23 12:13:40,351 - INFO - joeynmt.training - Example #3
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:13:40,351 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:13:40,351 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:13:40,351 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:13:40,351 - INFO - joeynmt.training - Example #4
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:13:40,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 12:13:40,352 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:13:40,352 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:13:40,352 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 12:14:06,286 - INFO - joeynmt.training - Epoch   8, Step:    20600, Batch Loss:     1.346769, Batch Acc: 0.566686, Tokens per Sec:     2482, Lr: 0.000210
2024-05-23 12:14:31,175 - INFO - joeynmt.training - Epoch   8, Step:    20700, Batch Loss:     1.404345, Batch Acc: 0.569379, Tokens per Sec:     2568, Lr: 0.000210
2024-05-23 12:14:54,408 - INFO - joeynmt.training - Epoch   8, Step:    20800, Batch Loss:     1.334100, Batch Acc: 0.567376, Tokens per Sec:     2740, Lr: 0.000210
2024-05-23 12:15:18,074 - INFO - joeynmt.training - Epoch   8, Step:    20900, Batch Loss:     1.262131, Batch Acc: 0.567170, Tokens per Sec:     2732, Lr: 0.000210
2024-05-23 12:15:41,663 - INFO - joeynmt.training - Epoch   8, Step:    21000, Batch Loss:     1.399650, Batch Acc: 0.566473, Tokens per Sec:     2756, Lr: 0.000210
2024-05-23 12:15:41,664 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:15:41,664 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:16:11,942 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.01, acc:   0.50, generation: 30.2711[sec], evaluation: 0.0000[sec]
2024-05-23 12:16:11,944 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 12:16:12,367 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/20000.ckpt
2024-05-23 12:16:12,423 - INFO - joeynmt.training - Example #0
2024-05-23 12:16:12,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:16:12,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:16:12,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'so', 'dass', '', '<unk>', 'das', '<unk>', 'Eis', '<unk>', '', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:16:12,424 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:16:12,424 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:16:12,424 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> so dass  <unk> das <unk> Eis <unk>  das für die meisten letzten drei Millionen Jahre  war die Größe der <unk> <unk> <unk>  <unk> <unk> <unk> <unk>
2024-05-23 12:16:12,424 - INFO - joeynmt.training - Example #1
2024-05-23 12:16:12,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:16:12,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:16:12,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'der', '<unk>', 'nicht', '<unk>', '</s>']
2024-05-23 12:16:12,424 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:16:12,424 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:16:12,425 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  weil es nicht die <unk> der <unk> nicht <unk>
2024-05-23 12:16:12,425 - INFO - joeynmt.training - Example #2
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 12:16:12,425 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:16:12,425 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:16:12,425 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  das <unk> Herz des globalen <unk>
2024-05-23 12:16:12,425 - INFO - joeynmt.training - Example #3
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:16:12,425 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:16:12,425 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:16:12,425 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:16:12,425 - INFO - joeynmt.training - Example #4
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:16:12,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', 'von', 'dem', '<unk>', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 12:16:12,426 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:16:12,426 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:16:12,426 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> <unk> von dem <unk> in den letzten 25 Jahren passiert ist.
2024-05-23 12:16:36,136 - INFO - joeynmt.training - Epoch   8, Step:    21100, Batch Loss:     1.366345, Batch Acc: 0.568344, Tokens per Sec:     2747, Lr: 0.000210
2024-05-23 12:16:39,020 - INFO - joeynmt.training - Epoch   8: total training loss 3556.91
2024-05-23 12:16:39,020 - INFO - joeynmt.training - EPOCH 9
2024-05-23 12:16:58,502 - INFO - joeynmt.training - Epoch   9, Step:    21200, Batch Loss:     1.257267, Batch Acc: 0.600295, Tokens per Sec:     2959, Lr: 0.000210
2024-05-23 12:17:21,459 - INFO - joeynmt.training - Epoch   9, Step:    21300, Batch Loss:     1.391164, Batch Acc: 0.598994, Tokens per Sec:     2831, Lr: 0.000210
2024-05-23 12:17:45,351 - INFO - joeynmt.training - Epoch   9, Step:    21400, Batch Loss:     1.206929, Batch Acc: 0.592265, Tokens per Sec:     2720, Lr: 0.000210
2024-05-23 12:18:07,196 - INFO - joeynmt.training - Epoch   9, Step:    21500, Batch Loss:     1.238896, Batch Acc: 0.595235, Tokens per Sec:     2992, Lr: 0.000210
2024-05-23 12:18:07,197 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:18:07,198 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:18:41,236 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.97, acc:   0.50, generation: 34.0305[sec], evaluation: 0.0000[sec]
2024-05-23 12:18:41,238 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 12:18:41,663 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/18500.ckpt
2024-05-23 12:18:41,702 - INFO - joeynmt.training - Example #0
2024-05-23 12:18:41,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:18:41,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:18:41,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'hat.', '</s>']
2024-05-23 12:18:41,703 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:18:41,703 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:18:41,703 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> also <unk>  <unk> dass das <unk> Eis <unk> <unk> der letzten drei Millionen Jahre <unk>  war die Größe der <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> hat.
2024-05-23 12:18:41,703 - INFO - joeynmt.training - Example #1
2024-05-23 12:18:41,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:18:41,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:18:41,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'das', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 12:18:41,703 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> das <unk> Problem  weil es nicht die <unk> des <unk>
2024-05-23 12:18:41,704 - INFO - joeynmt.training - Example #2
2024-05-23 12:18:41,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:18:41,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:18:41,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  der <unk> <unk> <unk>
2024-05-23 12:18:41,704 - INFO - joeynmt.training - Example #3
2024-05-23 12:18:41,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:18:41,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:18:41,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:18:41,704 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:18:41,705 - INFO - joeynmt.training - Example #4
2024-05-23 12:18:41,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:18:41,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:18:41,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werde,', '', 'ein', '<unk>', '<unk>', 'von', 'dem', '<unk>', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 12:18:41,705 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:18:41,705 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:18:41,705 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werde,  ein <unk> <unk> von dem <unk> in den letzten 25 Jahren <unk>
2024-05-23 12:19:03,509 - INFO - joeynmt.training - Epoch   9, Step:    21600, Batch Loss:     1.156618, Batch Acc: 0.587476, Tokens per Sec:     2886, Lr: 0.000210
2024-05-23 12:19:26,605 - INFO - joeynmt.training - Epoch   9, Step:    21700, Batch Loss:     1.248173, Batch Acc: 0.590659, Tokens per Sec:     2779, Lr: 0.000210
2024-05-23 12:19:49,541 - INFO - joeynmt.training - Epoch   9, Step:    21800, Batch Loss:     1.267224, Batch Acc: 0.584701, Tokens per Sec:     2762, Lr: 0.000210
2024-05-23 12:20:13,487 - INFO - joeynmt.training - Epoch   9, Step:    21900, Batch Loss:     1.334883, Batch Acc: 0.584400, Tokens per Sec:     2716, Lr: 0.000210
2024-05-23 12:20:37,415 - INFO - joeynmt.training - Epoch   9, Step:    22000, Batch Loss:     1.272200, Batch Acc: 0.584069, Tokens per Sec:     2747, Lr: 0.000210
2024-05-23 12:20:37,416 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:20:37,416 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:21:06,191 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.08, acc:   0.49, generation: 28.7678[sec], evaluation: 0.0000[sec]
2024-05-23 12:21:06,540 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/20500.ckpt
2024-05-23 12:21:06,628 - INFO - joeynmt.training - Example #0
2024-05-23 12:21:06,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:21:06,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:21:06,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', 'das', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', '', 'war', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:21:06,629 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:21:06,629 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:21:06,629 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> also <unk> dass das <unk> Eis <unk> <unk> das für die meisten drei Millionen Jahre  war die Größe der <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 12:21:06,629 - INFO - joeynmt.training - Example #1
2024-05-23 12:21:06,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:21:06,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:21:06,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', '<unk>', '</s>']
2024-05-23 12:21:06,629 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:21:06,629 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:21:06,629 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> Problem <unk>  weil es nicht die <unk> des <unk> nicht <unk>
2024-05-23 12:21:06,629 - INFO - joeynmt.training - Example #2
2024-05-23 12:21:06,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:21:06,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:21:06,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', '<unk>', '</s>']
2024-05-23 12:21:06,630 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:21:06,630 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:21:06,630 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  der <unk> <unk>
2024-05-23 12:21:06,630 - INFO - joeynmt.training - Example #3
2024-05-23 12:21:06,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:21:06,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:21:06,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:21:06,630 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:21:06,630 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:21:06,630 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:21:06,630 - INFO - joeynmt.training - Example #4
2024-05-23 12:21:06,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:21:06,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:21:06,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werden.', '', 'ein', '<unk>', '<unk>', '<unk>', 'von', 'dem', '<unk>', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 12:21:06,630 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:21:06,631 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:21:06,631 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werden.  ein <unk> <unk> <unk> von dem <unk> in den letzten 25 Jahren passiert ist.
2024-05-23 12:21:27,871 - INFO - joeynmt.training - Epoch   9, Step:    22100, Batch Loss:     1.275284, Batch Acc: 0.588481, Tokens per Sec:     2978, Lr: 0.000210
2024-05-23 12:21:48,993 - INFO - joeynmt.training - Epoch   9, Step:    22200, Batch Loss:     1.117261, Batch Acc: 0.585622, Tokens per Sec:     3057, Lr: 0.000210
2024-05-23 12:22:10,305 - INFO - joeynmt.training - Epoch   9, Step:    22300, Batch Loss:     1.291812, Batch Acc: 0.586113, Tokens per Sec:     3014, Lr: 0.000210
2024-05-23 12:22:31,418 - INFO - joeynmt.training - Epoch   9, Step:    22400, Batch Loss:     1.094627, Batch Acc: 0.583695, Tokens per Sec:     3100, Lr: 0.000210
2024-05-23 12:22:53,096 - INFO - joeynmt.training - Epoch   9, Step:    22500, Batch Loss:     1.283455, Batch Acc: 0.582198, Tokens per Sec:     3016, Lr: 0.000210
2024-05-23 12:22:53,098 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:22:53,098 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:23:26,028 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.04, acc:   0.50, generation: 32.9232[sec], evaluation: 0.0000[sec]
2024-05-23 12:23:26,408 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/22000.ckpt
2024-05-23 12:23:26,455 - INFO - joeynmt.helpers - delete /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_wordlevel/22000.ckpt
2024-05-23 12:23:26,455 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_wordlevel/22000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_wordlevel/22000.ckpt')
2024-05-23 12:23:26,455 - INFO - joeynmt.training - Example #0
2024-05-23 12:23:26,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:23:26,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:23:26,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', '<unk>', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', '<unk>', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', 'hat', '<unk>', '40', 'Prozent', '<unk>', '</s>']
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> also <unk> <unk> dass das <unk> Eis <unk>  <unk> das für die meisten letzten drei Millionen Jahre <unk>  hat die Größe der <unk> <unk> <unk>  <unk> hat <unk> 40 Prozent <unk>
2024-05-23 12:23:26,456 - INFO - joeynmt.training - Example #1
2024-05-23 12:23:26,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:23:26,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:23:26,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'nicht', '<unk>', '</s>']
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses Problem  weil es nicht die <unk> nicht <unk>
2024-05-23 12:23:26,456 - INFO - joeynmt.training - Example #2
2024-05-23 12:23:26,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:23:26,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:23:26,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '<unk>', '', 'das', '<unk>', 'Herz', 'des', '<unk>', '</s>']
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:23:26,456 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk> <unk>  das <unk> Herz des <unk>
2024-05-23 12:23:26,457 - INFO - joeynmt.training - Example #3
2024-05-23 12:23:26,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:23:26,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:23:26,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 12:23:26,457 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:23:26,457 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:23:26,457 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 12:23:26,457 - INFO - joeynmt.training - Example #4
2024-05-23 12:23:26,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:23:26,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:23:26,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', 'werde,', 'wird', 'ein', '<unk>', '<unk>', '<unk>', 'von', 'dem', '<unk>', '<unk>', '</s>']
2024-05-23 12:23:26,457 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:23:26,457 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:23:26,457 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen werde, wird ein <unk> <unk> <unk> von dem <unk> <unk>
2024-05-23 12:23:48,121 - INFO - joeynmt.training - Epoch   9, Step:    22600, Batch Loss:     1.313669, Batch Acc: 0.583009, Tokens per Sec:     3013, Lr: 0.000210
2024-05-23 12:24:08,708 - INFO - joeynmt.training - Epoch   9, Step:    22700, Batch Loss:     1.350590, Batch Acc: 0.583183, Tokens per Sec:     3202, Lr: 0.000210
2024-05-23 12:24:29,931 - INFO - joeynmt.training - Epoch   9, Step:    22800, Batch Loss:     1.105967, Batch Acc: 0.586693, Tokens per Sec:     3037, Lr: 0.000210
2024-05-23 12:24:51,921 - INFO - joeynmt.training - Epoch   9, Step:    22900, Batch Loss:     1.346352, Batch Acc: 0.585410, Tokens per Sec:     2867, Lr: 0.000210
2024-05-23 12:25:13,437 - INFO - joeynmt.training - Epoch   9, Step:    23000, Batch Loss:     1.367385, Batch Acc: 0.585815, Tokens per Sec:     3109, Lr: 0.000210
2024-05-23 12:25:13,438 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 12:25:13,438 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 12:25:50,093 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.08, acc:   0.50, generation: 36.6481[sec], evaluation: 0.0000[sec]
2024-05-23 12:25:50,094 - INFO - joeynmt.training - Example #0
2024-05-23 12:25:50,095 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 12:25:50,095 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 12:25:50,095 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', 'die', '<unk>', '<unk>', '<unk>', '<unk>', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'ist', 'die', 'Größe', 'der', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 12:25:50,095 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 12:25:50,095 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 12:25:50,095 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass die <unk> <unk> <unk> <unk> das für die meisten letzten drei Millionen Jahre <unk>  ist die Größe der <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>
2024-05-23 12:25:50,095 - INFO - joeynmt.training - Example #1
2024-05-23 12:25:50,095 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 12:25:50,095 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 12:25:50,095 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', '<unk>', '</s>']
2024-05-23 12:25:50,095 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 12:25:50,095 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 12:25:50,095 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> dieses <unk> Problem  weil es nicht die <unk> <unk>
2024-05-23 12:25:50,096 - INFO - joeynmt.training - Example #2
2024-05-23 12:25:50,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 12:25:50,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 12:25:50,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 12:25:50,096 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 12:25:50,096 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 12:25:50,096 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  das <unk> Herz des globalen <unk>
2024-05-23 12:25:50,096 - INFO - joeynmt.training - Example #3
2024-05-23 12:25:50,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 12:25:50,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 12:25:50,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '</s>']
2024-05-23 12:25:50,096 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 12:25:50,096 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 12:25:50,096 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>
2024-05-23 12:25:50,097 - INFO - joeynmt.training - Example #4
2024-05-23 12:25:50,097 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 12:25:50,097 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 12:25:50,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 12:25:50,097 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 12:25:50,097 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 12:25:50,097 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 12:26:12,124 - INFO - joeynmt.training - Epoch   9, Step:    23100, Batch Loss:     1.416501, Batch Acc: 0.581413, Tokens per Sec:     2959, Lr: 0.000210
2024-05-23 13:25:27,066 - INFO - joeynmt.training - Epoch   9, Step:    23200, Batch Loss:     1.313096, Batch Acc: 0.579879, Tokens per Sec:       18, Lr: 0.000210
2024-05-23 13:25:44,036 - INFO - joeynmt.training - Epoch   9, Step:    23300, Batch Loss:     1.329304, Batch Acc: 0.581768, Tokens per Sec:     3934, Lr: 0.000210
2024-05-23 13:26:01,373 - INFO - joeynmt.training - Epoch   9, Step:    23400, Batch Loss:     1.374272, Batch Acc: 0.583363, Tokens per Sec:     3727, Lr: 0.000210
2024-05-23 13:26:18,897 - INFO - joeynmt.training - Epoch   9, Step:    23500, Batch Loss:     1.347926, Batch Acc: 0.582239, Tokens per Sec:     3665, Lr: 0.000210
2024-05-23 13:26:18,899 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:26:18,899 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:26:42,771 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.03, acc:   0.50, generation: 23.8675[sec], evaluation: 0.0000[sec]
2024-05-23 13:26:42,993 - INFO - joeynmt.helpers - delete ../models/model_wordlevel/19500.ckpt
2024-05-23 13:26:43,041 - INFO - joeynmt.training - Example #0
2024-05-23 13:26:43,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 13:26:43,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 13:26:43,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', 'so', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'das', '<unk>', 'Eis', '<unk>', '', 'das', 'für', 'die', 'meisten', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'hat', 'die', '<unk>', '<unk>', '<unk>', '', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'hat.', '</s>']
2024-05-23 13:26:43,041 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 13:26:43,041 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 13:26:43,041 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> so dass das <unk> Eis <unk>  das <unk> Eis <unk>  das für die meisten <unk> drei Millionen Jahre <unk>  hat die <unk> <unk> <unk>  <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> hat.
2024-05-23 13:26:43,041 - INFO - joeynmt.training - Example #1
2024-05-23 13:26:43,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 13:26:43,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 13:26:43,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', 'nicht', '<unk>', '</s>']
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> <unk>  weil es nicht die <unk> des <unk> nicht <unk>
2024-05-23 13:26:43,042 - INFO - joeynmt.training - Example #2
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'der', '<unk>', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  der <unk> <unk> des <unk>
2024-05-23 13:26:43,042 - INFO - joeynmt.training - Example #3
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 13:26:43,042 - INFO - joeynmt.training - Example #4
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 13:26:43,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', 'der', '<unk>', 'der', 'letzten', '25', 'Jahre', '<unk>', '</s>']
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 13:26:43,042 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> <unk> der <unk> der letzten 25 Jahre <unk>
2024-05-23 13:27:01,677 - INFO - joeynmt.training - Epoch   9, Step:    23600, Batch Loss:     1.335297, Batch Acc: 0.577105, Tokens per Sec:     3379, Lr: 0.000210
2024-05-23 13:27:18,744 - INFO - joeynmt.training - Epoch   9, Step:    23700, Batch Loss:     1.367027, Batch Acc: 0.575185, Tokens per Sec:     3737, Lr: 0.000210
2024-05-23 13:27:28,963 - INFO - joeynmt.training - Epoch   9: total training loss 3407.47
2024-05-23 13:27:28,964 - INFO - joeynmt.training - EPOCH 10
2024-05-23 13:27:36,058 - INFO - joeynmt.training - Epoch  10, Step:    23800, Batch Loss:     1.203259, Batch Acc: 0.601385, Tokens per Sec:     3643, Lr: 0.000210
2024-05-23 13:27:53,970 - INFO - joeynmt.training - Epoch  10, Step:    23900, Batch Loss:     1.316600, Batch Acc: 0.602192, Tokens per Sec:     3581, Lr: 0.000210
2024-05-23 13:28:12,321 - INFO - joeynmt.training - Epoch  10, Step:    24000, Batch Loss:     1.116491, Batch Acc: 0.605886, Tokens per Sec:     3641, Lr: 0.000210
2024-05-23 13:28:12,322 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:28:12,322 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:28:40,652 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.23, acc:   0.50, generation: 28.3247[sec], evaluation: 0.0000[sec]
2024-05-23 13:28:40,654 - INFO - joeynmt.training - Example #0
2024-05-23 13:28:40,654 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 13:28:40,654 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 13:28:40,654 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', '<unk>', 'also', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'das', '<unk>', 'Eis', '<unk>', '', '<unk>', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 13:28:40,654 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 13:28:40,654 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 13:28:40,654 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> <unk> also <unk> dass das <unk> Eis <unk>  das <unk> Eis <unk>  <unk> der letzten drei Millionen Jahre <unk>  hat die Größe der <unk> <unk>  <unk> <unk>
2024-05-23 13:28:40,654 - INFO - joeynmt.training - Example #1
2024-05-23 13:28:40,654 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 13:28:40,654 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 13:28:40,654 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 13:28:40,654 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 13:28:40,654 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 13:28:40,654 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 13:28:40,654 - INFO - joeynmt.training - Example #2
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'im', '<unk>', '', 'das', '<unk>', 'Herz', 'des', '<unk>', '</s>']
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist im <unk>  das <unk> Herz des <unk>
2024-05-23 13:28:40,655 - INFO - joeynmt.training - Example #3
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 13:28:40,655 - INFO - joeynmt.training - Example #4
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 13:28:40,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'eine', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 13:28:40,655 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  eine <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 13:28:57,891 - INFO - joeynmt.training - Epoch  10, Step:    24100, Batch Loss:     1.142857, Batch Acc: 0.602727, Tokens per Sec:     3726, Lr: 0.000210
2024-05-23 13:29:15,657 - INFO - joeynmt.training - Epoch  10, Step:    24200, Batch Loss:     1.205910, Batch Acc: 0.604399, Tokens per Sec:     3749, Lr: 0.000210
2024-05-23 13:29:33,054 - INFO - joeynmt.training - Epoch  10, Step:    24300, Batch Loss:     1.233373, Batch Acc: 0.602150, Tokens per Sec:     3824, Lr: 0.000210
2024-05-23 13:29:50,881 - INFO - joeynmt.training - Epoch  10, Step:    24400, Batch Loss:     1.369931, Batch Acc: 0.599190, Tokens per Sec:     3641, Lr: 0.000210
2024-05-23 13:30:08,429 - INFO - joeynmt.training - Epoch  10, Step:    24500, Batch Loss:     1.209384, Batch Acc: 0.598138, Tokens per Sec:     3648, Lr: 0.000210
2024-05-23 13:30:08,430 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:30:08,431 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:30:39,243 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.40, acc:   0.49, generation: 30.8074[sec], evaluation: 0.0000[sec]
2024-05-23 13:30:39,244 - INFO - joeynmt.training - Example #0
2024-05-23 13:30:39,244 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 13:30:39,244 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 13:30:39,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', 'dass', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', '<unk>', 'ist', 'die', 'Größe', 'der', '<unk>', '<unk>', '', '<unk>', '<unk>', '</s>']
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so dass  <unk> dass das <unk> Eis <unk>  das für die meisten letzten drei Millionen Jahre <unk>  <unk> ist die Größe der <unk> <unk>  <unk> <unk>
2024-05-23 13:30:39,245 - INFO - joeynmt.training - Example #1
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', '<unk>', 'die', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Hypothesis: Aber diese <unk> die <unk> dieses <unk> Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 13:30:39,245 - INFO - joeynmt.training - Example #2
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'das', '<unk>', 'Herz', 'des', 'globalen', '<unk>', '</s>']
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  das <unk> Herz des globalen <unk>
2024-05-23 13:30:39,245 - INFO - joeynmt.training - Example #3
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 13:30:39,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 13:30:39,245 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 13:30:39,246 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 13:30:39,246 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 13:30:39,246 - INFO - joeynmt.training - Example #4
2024-05-23 13:30:39,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 13:30:39,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 13:30:39,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', 'zeigen', '', 'ein', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 13:30:39,246 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 13:30:39,246 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 13:30:39,246 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen zeigen  ein <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 13:30:56,213 - INFO - joeynmt.training - Epoch  10, Step:    24600, Batch Loss:     1.242318, Batch Acc: 0.597220, Tokens per Sec:     3807, Lr: 0.000210
2024-05-23 13:31:13,566 - INFO - joeynmt.training - Epoch  10, Step:    24700, Batch Loss:     1.190590, Batch Acc: 0.593171, Tokens per Sec:     3700, Lr: 0.000210
2024-05-23 13:31:30,467 - INFO - joeynmt.training - Epoch  10, Step:    24800, Batch Loss:     1.177501, Batch Acc: 0.591642, Tokens per Sec:     3841, Lr: 0.000210
2024-05-23 13:31:48,289 - INFO - joeynmt.training - Epoch  10, Step:    24900, Batch Loss:     1.289840, Batch Acc: 0.594638, Tokens per Sec:     3655, Lr: 0.000210
2024-05-23 13:32:06,728 - INFO - joeynmt.training - Epoch  10, Step:    25000, Batch Loss:     1.200751, Batch Acc: 0.592216, Tokens per Sec:     3572, Lr: 0.000210
2024-05-23 13:32:06,730 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:32:06,730 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:32:31,601 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.21, acc:   0.49, generation: 24.8640[sec], evaluation: 0.0000[sec]
2024-05-23 13:32:31,603 - INFO - joeynmt.training - Example #0
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', 'so', 'dass', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'der', 'Größe', 'der', '<unk>', '<unk>', '<unk>', 'hat.', '', '<unk>', 'hat', 'sich', '<unk>', '40', 'Prozent.', '</s>']
2024-05-23 13:32:31,603 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 13:32:31,603 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 13:32:31,603 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> so dass  <unk> dass das <unk> Eis <unk>  das für die meisten letzten drei Millionen Jahre <unk>  der Größe der <unk> <unk> <unk> hat.  <unk> hat sich <unk> 40 Prozent.
2024-05-23 13:32:31,603 - INFO - joeynmt.training - Example #1
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', '<unk>', 'das', '<unk>', 'dieses', '<unk>', 'Problem', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 13:32:31,603 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 13:32:31,603 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 13:32:31,603 - INFO - joeynmt.training - 	Hypothesis: Aber das <unk> das <unk> dieses <unk> Problem  weil es nicht die <unk> des <unk> <unk>
2024-05-23 13:32:31,603 - INFO - joeynmt.training - Example #2
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 13:32:31,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '', 'das', '<unk>', 'Herz', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk>  das <unk> Herz des <unk> <unk>
2024-05-23 13:32:31,604 - INFO - joeynmt.training - Example #3
2024-05-23 13:32:31,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 13:32:31,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 13:32:31,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 13:32:31,604 - INFO - joeynmt.training - Example #4
2024-05-23 13:32:31,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 13:32:31,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 13:32:31,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', 'davon', 'wird', 'in', 'den', 'letzten', '25', 'Jahren', '<unk>', '</s>']
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 13:32:31,604 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> davon wird in den letzten 25 Jahren <unk>
2024-05-23 13:32:48,743 - INFO - joeynmt.training - Epoch  10, Step:    25100, Batch Loss:     1.423579, Batch Acc: 0.595416, Tokens per Sec:     3812, Lr: 0.000210
2024-05-23 13:33:06,362 - INFO - joeynmt.training - Epoch  10, Step:    25200, Batch Loss:     1.176261, Batch Acc: 0.594493, Tokens per Sec:     3778, Lr: 0.000210
2024-05-23 13:33:24,547 - INFO - joeynmt.training - Epoch  10, Step:    25300, Batch Loss:     1.285434, Batch Acc: 0.596362, Tokens per Sec:     3534, Lr: 0.000210
2024-05-23 13:33:42,218 - INFO - joeynmt.training - Epoch  10, Step:    25400, Batch Loss:     1.390783, Batch Acc: 0.588526, Tokens per Sec:     3665, Lr: 0.000210
2024-05-23 13:33:59,564 - INFO - joeynmt.training - Epoch  10, Step:    25500, Batch Loss:     1.250886, Batch Acc: 0.588576, Tokens per Sec:     3727, Lr: 0.000210
2024-05-23 13:33:59,565 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:33:59,565 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:34:33,946 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.22, acc:   0.49, generation: 34.3765[sec], evaluation: 0.0000[sec]
2024-05-23 13:34:33,948 - INFO - joeynmt.training - Example #0
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'beiden', '<unk>', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', '', 'das', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'hat', 'die', 'Größe', 'der', '<unk>', '<unk>', '', '<unk>', 'hat', '<unk>', '40', 'Prozent.', '</s>']
2024-05-23 13:34:33,948 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 13:34:33,948 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 13:34:33,948 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese beiden <unk> <unk>  <unk> dass das <unk> Eis <unk> <unk>  das für die meisten letzten drei Millionen Jahre <unk>  hat die Größe der <unk> <unk>  <unk> hat <unk> 40 Prozent.
2024-05-23 13:34:33,948 - INFO - joeynmt.training - Example #1
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'Problem', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', '<unk>', '</s>']
2024-05-23 13:34:33,948 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 13:34:33,948 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 13:34:33,948 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> Problem <unk>  weil es nicht die <unk> <unk>
2024-05-23 13:34:33,948 - INFO - joeynmt.training - Example #2
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 13:34:33,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', '<unk>', '<unk>', '', 'der', '<unk>', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in <unk> <unk>  der <unk> <unk> des <unk>
2024-05-23 13:34:33,949 - INFO - joeynmt.training - Example #3
2024-05-23 13:34:33,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 13:34:33,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 13:34:33,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', '<unk>', '</s>']
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> <unk>
2024-05-23 13:34:33,949 - INFO - joeynmt.training - Example #4
2024-05-23 13:34:33,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 13:34:33,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 13:34:33,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 13:34:33,949 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> <unk> von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 13:34:50,933 - INFO - joeynmt.training - Epoch  10, Step:    25600, Batch Loss:     1.190875, Batch Acc: 0.593403, Tokens per Sec:     3804, Lr: 0.000210
2024-05-23 13:35:08,045 - INFO - joeynmt.training - Epoch  10, Step:    25700, Batch Loss:     1.195247, Batch Acc: 0.591355, Tokens per Sec:     3833, Lr: 0.000210
2024-05-23 13:35:25,398 - INFO - joeynmt.training - Epoch  10, Step:    25800, Batch Loss:     1.288661, Batch Acc: 0.592976, Tokens per Sec:     3722, Lr: 0.000210
2024-05-23 13:35:43,632 - INFO - joeynmt.training - Epoch  10, Step:    25900, Batch Loss:     1.237161, Batch Acc: 0.589317, Tokens per Sec:     3543, Lr: 0.000210
2024-05-23 13:36:01,082 - INFO - joeynmt.training - Epoch  10, Step:    26000, Batch Loss:     1.202097, Batch Acc: 0.588185, Tokens per Sec:     3717, Lr: 0.000210
2024-05-23 13:36:01,083 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:36:01,083 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:36:29,717 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.16, acc:   0.49, generation: 28.6305[sec], evaluation: 0.0000[sec]
2024-05-23 13:36:29,719 - INFO - joeynmt.training - Example #0
2024-05-23 13:36:29,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['Last', 'year', 'I', 'showed', 'these', 'two', 'slides', 'so', 'that', '', 'demonstrate', 'that', 'the', 'arctic', 'ice', 'cap,', '', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', '', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '48', 'states,', '', 'has', 'shrunk', 'by', '40', 'percent.']
2024-05-23 13:36:29,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Letztes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Folien', 'gezeigt,', 'um', 'zu', 'veranschaulichen,', 'dass', 'die', 'arktische', 'Eiskappe,', 'die', 'für', 'annähernd', 'drei', 'Millionen', 'Jahre', 'die', 'Grösse', 'der', 'unteren', '48', 'Staaten', 'hatte,', 'um', '40', 'Prozent', 'geschrumpft', 'ist.']
2024-05-23 13:36:29,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'Jahr', 'habe', 'ich', 'diese', 'zwei', '<unk>', 'so', '<unk>', '', '<unk>', 'dass', 'das', '<unk>', 'Eis', '<unk>', '<unk>', '<unk>', '<unk>', 'der', '<unk>', 'drei', 'Millionen', 'Jahre', '<unk>', '', 'der', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', 'hat', '<unk>', '40', 'Prozent.', '</s>']
2024-05-23 13:36:29,719 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 13:36:29,719 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 13:36:29,719 - INFO - joeynmt.training - 	Hypothesis: <unk> Jahr habe ich diese zwei <unk> so <unk>  <unk> dass das <unk> Eis <unk> <unk> <unk> <unk> der <unk> drei Millionen Jahre <unk>  der <unk> <unk> <unk> <unk> <unk>  hat <unk> 40 Prozent.
2024-05-23 13:36:29,719 - INFO - joeynmt.training - Example #1
2024-05-23 13:36:29,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'understates', 'the', 'seriousness', 'of', 'this', 'particular', 'problem', '', 'because', 'it', "doesn't", 'show', 'the', 'thickness', 'of', 'the', 'ice.']
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drückt', 'nicht', 'stark', 'genug', 'die', 'Ernsthaftigkeit', 'dieses', 'speziellen', 'Problems', 'aus,', 'da', 'es', 'nicht', 'die', 'Dicke', 'des', 'Eises', 'zeigt.']
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', '<unk>', 'das', '<unk>', 'dieses', '<unk>', '', 'weil', 'es', 'nicht', 'die', '<unk>', 'des', '<unk>', '<unk>', '</s>']
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Hypothesis: Aber dieses <unk> das <unk> dieses <unk>  weil es nicht die <unk> des <unk> <unk>
2024-05-23 13:36:29,720 - INFO - joeynmt.training - Example #2
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'arctic', 'ice', 'cap', 'is,', 'in', 'a', 'sense,', '', 'the', 'beating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewissem', 'Sinne', 'ist', 'die', 'arktische', 'Eiskappe', 'das', 'schlagende', 'Herz', 'unseres', 'globalen', 'Klimasystems.']
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', '<unk>', 'Eis', 'ist', 'in', 'einem', '<unk>', '', 'der', '<unk>', '<unk>', 'des', '<unk>', '</s>']
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Hypothesis: Das <unk> Eis ist in einem <unk>  der <unk> <unk> des <unk>
2024-05-23 13:36:29,720 - INFO - joeynmt.training - Example #3
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'expands', 'in', 'winter', 'and', 'contracts', 'in', 'summer.']
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'wächst', 'im', 'Winter', 'und', 'schrumpft', 'im', 'Sommer.']
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', '<unk>', 'in', '<unk>', 'und', '<unk>', 'in', '<unk>', '</s>']
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 13:36:29,720 - INFO - joeynmt.training - 	Hypothesis: Es <unk> in <unk> und <unk> in <unk>
2024-05-23 13:36:29,720 - INFO - joeynmt.training - Example #4
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', '', 'a', 'rapid', 'fast-forward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 13:36:29,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Folie,', 'die', 'ich', 'Ihnen', 'zeige,', 'ist', 'eine', 'Zeitrafferaufnahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 13:36:29,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', '<unk>', 'die', 'ich', 'Ihnen', '<unk>', '', 'ein', '<unk>', '<unk>', 'von', 'dem', '<unk>', '<unk>', '<unk>', '<unk>', '</s>']
2024-05-23 13:36:29,721 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 13:36:29,721 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 13:36:29,721 - INFO - joeynmt.training - 	Hypothesis: Die nächste <unk> die ich Ihnen <unk>  ein <unk> <unk> von dem <unk> <unk> <unk> <unk>
2024-05-23 13:36:47,282 - INFO - joeynmt.training - Epoch  10, Step:    26100, Batch Loss:     1.233078, Batch Acc: 0.594712, Tokens per Sec:     3680, Lr: 0.000147
2024-05-23 13:37:04,672 - INFO - joeynmt.training - Epoch  10, Step:    26200, Batch Loss:     1.249256, Batch Acc: 0.592134, Tokens per Sec:     3740, Lr: 0.000147
2024-05-23 13:37:22,296 - INFO - joeynmt.training - Epoch  10, Step:    26300, Batch Loss:     1.195392, Batch Acc: 0.591291, Tokens per Sec:     3755, Lr: 0.000147
2024-05-23 13:37:39,871 - INFO - joeynmt.training - Epoch  10, Step:    26400, Batch Loss:     1.086623, Batch Acc: 0.590239, Tokens per Sec:     3662, Lr: 0.000147
2024-05-23 13:37:39,872 - INFO - joeynmt.training - Epoch  10: total training loss 3314.39
2024-05-23 13:37:39,872 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-23 13:37:39,872 - INFO - joeynmt.training - Best validation result (greedy) at step    21500:   4.97 ppl.
2024-05-23 13:37:39,883 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-23 13:37:39,926 - INFO - joeynmt.model - Enc-dec model built.
2024-05-23 13:37:40,164 - INFO - joeynmt.helpers - Load model from /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_wordlevel/21500.ckpt.
2024-05-23 13:37:40,167 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=None)
2024-05-23 13:37:40,169 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-23 13:37:40,169 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:37:40,170 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:38:04,688 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 24.5139[sec], evaluation: 0.0000[sec]
2024-05-23 13:38:04,690 - INFO - joeynmt.prediction - Translations saved to: /Users/janinehindermann/Documents/workarea/mt-exercise-5/scripts/../models/model_wordlevel/00021500.hyps.dev.
2024-05-23 13:38:04,690 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-23 13:38:04,690 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 13:38:04,690 - INFO - joeynmt.prediction - Predicting 1568 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 13:38:37,861 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 33.1638[sec], evaluation: 0.0000[sec]
2024-05-23 13:38:37,864 - INFO - joeynmt.prediction - Translations saved to: /Users/janinehindermann/Documents/workarea/mt-exercise-5/scripts/../models/model_wordlevel/00021500.hyps.test.
