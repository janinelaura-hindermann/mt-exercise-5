2024-05-23 14:19:55,357 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                           cfg.name : bpe_2000
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                     cfg.data.train : data_sampled/sampled_train.en-de
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev.en-de
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                      cfg.data.test : data/test.en-de
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data_bpe/joint_vocab_clean.txt
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 2000
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data_bpe/bpe_code.bpe
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : de
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data_bpe/joint_vocab_clean.txt
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 2000
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data_bpe/bpe_code.bpe
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-23 14:19:55,357 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -             cfg.training.model_dir : ../models/model_bpe_2000
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-23 14:19:55,358 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-23 14:19:55,360 - INFO - joeynmt.data - Building tokenizer...
2024-05-23 14:19:55,363 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-23 14:19:55,363 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-23 14:19:55,363 - INFO - joeynmt.data - Loading train set...
2024-05-23 14:19:55,491 - INFO - joeynmt.data - Building vocabulary...
2024-05-23 14:19:55,518 - INFO - joeynmt.data - Loading dev set...
2024-05-23 14:19:55,521 - INFO - joeynmt.data - Loading test set...
2024-05-23 14:19:55,523 - INFO - joeynmt.data - Data loaded.
2024-05-23 14:19:55,523 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 14:19:55,523 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=888, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 14:19:55,523 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1568, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 14:19:55,524 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ or@@ e: A@@ ver@@ ting the c@@ lim@@ ate c@@ ris@@ is
	[TRG] A@@ l G@@ or@@ e: Die Ab@@ wen@@ d@@ ung der K@@ li@@ ma@@ k@@ at@@ ast@@ ro@@ p@@ he
2024-05-23 14:19:55,524 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) !@@ (6) " (7) "@@ (8) # (9) #@@
2024-05-23 14:19:55,524 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) !@@ (6) " (7) "@@ (8) # (9) #@@
2024-05-23 14:19:55,524 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2000
2024-05-23 14:19:55,524 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2000
2024-05-23 14:19:55,524 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-23 14:19:55,586 - INFO - joeynmt.model - Enc-dec model built.
2024-05-23 14:19:55,588 - INFO - joeynmt.model - Total params: 3411200
2024-05-23 14:19:55,588 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2024-05-23 14:19:55,588 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2000),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2000),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-23 14:19:55,588 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-23 14:19:55,588 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-23 14:19:55,589 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-23 14:19:55,589 - INFO - joeynmt.training - EPOCH 1
2024-05-23 14:20:11,010 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.066433, Batch Acc: 0.031849, Tokens per Sec:     5066, Lr: 0.000300
2024-05-23 14:20:28,488 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.034307, Batch Acc: 0.044644, Tokens per Sec:     4477, Lr: 0.000300
2024-05-23 14:20:45,206 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.812288, Batch Acc: 0.056473, Tokens per Sec:     4656, Lr: 0.000300
2024-05-23 14:21:02,118 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.743715, Batch Acc: 0.066048, Tokens per Sec:     4610, Lr: 0.000300
2024-05-23 14:21:18,248 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.681587, Batch Acc: 0.073470, Tokens per Sec:     4696, Lr: 0.000300
2024-05-23 14:21:18,249 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:21:18,250 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:23:02,632 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.68, ppl:  39.72, acc:   0.07, generation: 104.3611[sec], evaluation: 0.0000[sec]
2024-05-23 14:23:02,636 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:23:02,888 - INFO - joeynmt.training - Example #0
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich']
2024-05-23 14:23:02,889 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:23:02,889 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:23:02,889 - INFO - joeynmt.training - 	Hypothesis: Ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich
2024-05-23 14:23:02,889 - INFO - joeynmt.training - Example #1
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'ich', 'ich', 'ich', 'ich', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die']
2024-05-23 14:23:02,889 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:23:02,889 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:23:02,889 - INFO - joeynmt.training - 	Hypothesis: Aber ich ich ich ich die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die die
2024-05-23 14:23:02,889 - INFO - joeynmt.training - Example #2
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:23:02,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ist', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der']
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Hypothesis: Das ist die die die die die die die die der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der
2024-05-23 14:23:02,890 - INFO - joeynmt.training - Example #3
2024-05-23 14:23:02,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:23:02,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:23:02,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'das', 'das', 'das', 'das', 'das', 'das', 'das', 'das', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und']
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Hypothesis: Es ist das das das das das das das das und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und
2024-05-23 14:23:02,890 - INFO - joeynmt.training - Example #4
2024-05-23 14:23:02,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:23:02,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:23:02,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich']
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:23:02,890 - INFO - joeynmt.training - 	Hypothesis: Ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich
2024-05-23 14:23:19,230 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.583009, Batch Acc: 0.080594, Tokens per Sec:     4687, Lr: 0.000300
2024-05-23 14:23:36,128 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.486331, Batch Acc: 0.087294, Tokens per Sec:     4570, Lr: 0.000300
2024-05-23 14:23:52,314 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.502900, Batch Acc: 0.096167, Tokens per Sec:     4707, Lr: 0.000300
2024-05-23 14:24:09,003 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.428226, Batch Acc: 0.102551, Tokens per Sec:     4588, Lr: 0.000300
2024-05-23 14:24:25,528 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.304496, Batch Acc: 0.112530, Tokens per Sec:     4588, Lr: 0.000300
2024-05-23 14:24:25,529 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:24:25,529 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:26:11,653 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.35, ppl:  28.56, acc:   0.12, generation: 106.1064[sec], evaluation: 0.0000[sec]
2024-05-23 14:26:11,654 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:26:11,877 - INFO - joeynmt.training - Example #0
2024-05-23 14:26:11,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:26:11,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:26:11,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'S@@', 'o@@', 'ch', 'der', 'S@@', 'ei@@', ',', 'dass', 'ich', 'ich', 'die', 'S@@', 'ei@@', ',', 'dass', 'ich', 'die', 'S@@', 'ei@@', 'ch', 'der', 'S@@', 'ei@@', ',', 'dass', 'ich', 'der', 'S@@', 'ei@@', ',', 'dass', 'ich', 'die', 'B@@', 'ei@@', 'ch', 'der', 'S@@', 'ei@@', 'ch', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu']
2024-05-23 14:26:11,877 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:26:11,877 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:26:11,877 - INFO - joeynmt.training - 	Hypothesis: SSoch der Sei, dass ich ich die Sei, dass ich die Seich der Sei, dass ich der Sei, dass ich die Beich der Seich zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu
2024-05-23 14:26:11,878 - INFO - joeynmt.training - Example #1
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'die', 'S@@', 'ei@@', 'ch', 'die', 'S@@', 'ei@@', 'ch', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'S@@', 'ei@@', 'k@@', 'en.', '</s>']
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Hypothesis: Aber die Seich die Seich der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der Seiken.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - Example #2
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'M@@', 'ann', 'die', 'S@@', 'ei@@', ',', 'die', 'S@@', 'ei@@', ',', 'die', 'S@@', 'ei@@', 'en', 'der', 'der', 'der', 'S@@', 'ei@@', 'en', 'der', 'der', 'S@@', 'ei@@', 'en', 'der', 'S@@', 'ei@@', 'en', 'der', 'S@@', 'ei@@', 'en', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'l@@', 'ü@@', 'st@@', 'en.', '</s>']
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Hypothesis: Die Mann die Sei, die Sei, die Seien der der der Seien der der Seien der Seien der Seien zu zu zu zu zu zu zu zu zu zu lüsten.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - Example #3
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:26:11,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'ein', 'S@@', 'ei@@', 'en', 'und', 'S@@', 'ei@@', 'en', 'und', 'S@@', 'ei@@', 'en', 'und', 'S@@', 'ei@@', 'en', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'S@@', 'ei@@', '.', '</s>']
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - 	Hypothesis: Es ist ein Seien und Seien und Seien und Seien und und und und und und und Sei.
2024-05-23 14:26:11,878 - INFO - joeynmt.training - Example #4
2024-05-23 14:26:11,879 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:26:11,879 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:26:11,879 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'M@@', 'ann', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich', 'ich']
2024-05-23 14:26:11,879 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:26:11,879 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:26:11,879 - INFO - joeynmt.training - 	Hypothesis: Die Mann ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich ich
2024-05-23 14:26:28,245 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.238074, Batch Acc: 0.129021, Tokens per Sec:     4596, Lr: 0.000300
2024-05-23 14:26:45,439 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.268338, Batch Acc: 0.139838, Tokens per Sec:     4547, Lr: 0.000300
2024-05-23 14:27:02,029 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.190178, Batch Acc: 0.154548, Tokens per Sec:     4744, Lr: 0.000300
2024-05-23 14:27:18,442 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.030136, Batch Acc: 0.159703, Tokens per Sec:     4693, Lr: 0.000300
2024-05-23 14:27:34,534 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.078868, Batch Acc: 0.168368, Tokens per Sec:     4803, Lr: 0.000300
2024-05-23 14:27:34,536 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:27:34,536 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:29:22,077 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.01, ppl:  20.38, acc:   0.16, generation: 107.5289[sec], evaluation: 0.0000[sec]
2024-05-23 14:29:22,080 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:29:22,241 - INFO - joeynmt.training - Example #0
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'icht', 'in', 'den', 'K@@', 're@@', 're@@', 're@@', 'chen', 'und', 'die', 'K@@', 're@@', 're@@', 're@@', 'chen', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu']
2024-05-23 14:29:22,242 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:29:22,242 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:29:22,242 - INFO - joeynmt.training - 	Hypothesis: Licht in den Krererechen und die Krererechen zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu
2024-05-23 14:29:22,242 - INFO - joeynmt.training - Example #1
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'T@@', 'ag@@', 'ag@@', 'e,', 'die', 'die', 'die', 'K@@', 're@@', 're@@', 'k@@', 'k@@', 'k@@', 'k@@', 'un@@', 'st', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht']
2024-05-23 14:29:22,242 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:29:22,242 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:29:22,242 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Tagage, die die die Krerekkkkunst nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht
2024-05-23 14:29:22,242 - INFO - joeynmt.training - Example #2
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:29:22,242 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'M@@', 'ann', 'in', 'der', 'S@@', 'ei@@', 'te', 'in', 'der', 'K@@', 'un@@', 'ft', 'in', 'der', 'K@@', 'un@@', 'ft', 'in', 'der', 'K@@', 're@@', 're@@', 'k@@', 'k@@', 'k@@', 'k@@', 'un@@', 'gef@@', 'äh@@', 'r', 'von', 'der', 'K@@', 'un@@', 'ft', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'er@@', 'en.', '</s>']
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Hypothesis: Die Mann in der Seite in der Kunft in der Kunft in der Krerekkkkungefähr von der Kunft zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu eren.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - Example #3
2024-05-23 14:29:22,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:29:22,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:29:22,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'eine', 'S@@', 'ache', 'in', 'der', 'S@@', 'ei@@', 'te', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'sie', 'sie', 'sind', 'in', 'der', 'L@@', 'and@@', 'en.', '</s>']
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Hypothesis: Es ist eine Sache in der Seite und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und sie sie sind in der Landen.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - Example #4
2024-05-23 14:29:22,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:29:22,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:29:22,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'M@@', 'ann', 'ann', 'ich', 'einen', 'K@@', 'ar@@', 'ar@@', ',', 'dass', 'man', 'eine', 'K@@', 'ar@@', 'ten', 'in', 'den', 'K@@', 're@@', 're@@', 'm', 'D@@', 'at@@', 'z', 'in', 'den', 'K@@', 'ar@@', 'ar@@', 'ten', 'in', 'den', 'K@@', 'ar@@', 'ar@@', 't@@', 't@@', 't@@', 't@@', 't@@', 'al@@', 'ten', 'in', 'den', 'K@@', 'o@@', 'ch@@', 'ch@@', 'ch@@', 'ch@@', 'ch@@', 's', 'in', 'den', 'K@@', 'ar@@', '.', '</s>']
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:29:22,243 - INFO - joeynmt.training - 	Hypothesis: Die Mann ann ich einen Karar, dass man eine Karten in den Krerem Datz in den Kararten in den Karartttttalten in den Kochchchchchs in den Kar.
2024-05-23 14:29:38,960 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.926992, Batch Acc: 0.175945, Tokens per Sec:     4585, Lr: 0.000300
2024-05-23 14:29:55,489 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.889471, Batch Acc: 0.185188, Tokens per Sec:     4714, Lr: 0.000300
2024-05-23 14:30:12,084 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.802256, Batch Acc: 0.188961, Tokens per Sec:     4601, Lr: 0.000300
2024-05-23 14:30:28,769 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.901929, Batch Acc: 0.199395, Tokens per Sec:     4578, Lr: 0.000300
2024-05-23 14:30:44,571 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.836817, Batch Acc: 0.205279, Tokens per Sec:     4774, Lr: 0.000300
2024-05-23 14:30:44,572 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:30:44,572 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:32:22,891 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.83, ppl:  16.91, acc:   0.20, generation: 98.3089[sec], evaluation: 0.0000[sec]
2024-05-23 14:32:22,893 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:32:23,106 - INFO - joeynmt.training - Example #0
2024-05-23 14:32:23,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:32:23,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:32:23,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'L@@', 'and', 'in', 'der', 'S@@', 'ach@@', 'en', 'in', 'der', 'S@@', 'ach@@', 'en', 'in', 'der', 'L@@', 'ö@@', 'l@@', 'ung', 'von', 'dem', 'T@@', 'eil@@', 'en', 'der', 'L@@', 'ö@@', 'l@@', 'ung', 'von', 'dem', 'T@@', 'eil@@', 'en', 'von', 'dem', 'T@@', 'eil@@', 'en', 'von', 'der', 'S@@', 'on@@', 'n@@', 'en,', 'die', 'die', 'T@@', 'eil@@', 'en', 'von', 'der', 'L@@', 'ö@@', 's@@', 'ung', 'von', 'der', 'L@@', 'ö@@', 'hn@@', 'lichen', 'S@@', 'on@@', 'n@@', 'en,', 'die', 'ich', 'die', 'T@@', 'eil@@', 'en', 'von', 'der', 'L@@', 'ö@@', 's@@', 'ung', 'von', 'der', 'S@@', 'on@@', 'n@@', 'en@@', 'en@@', 'en@@', 'en', 'zu', 'ver@@', 'sch@@', 'ü@@', 'tz@@', 'en,', 'die', 'ich', 'die', '1@@', '0@@', '.@@', '000', 'Jahren', 'Jahren', 'hat@@', 'ten', 'von']
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Hypothesis: LLand in der Sachen in der Sachen in der Lölung von dem Teilen der Lölung von dem Teilen von dem Teilen von der Sonnen, die die Teilen von der Lösung von der Löhnlichen Sonnen, die ich die Teilen von der Lösung von der Sonnenenenen zu verschützen, die ich die 10.000 Jahren Jahren hatten von
2024-05-23 14:32:23,107 - INFO - joeynmt.training - Example #1
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'S@@', 'ach@@', 'e,', 'die', 'die', 'die', 'S@@', 'ach@@', 'en', 'der', 'L@@', 'ei@@', 't,', 'dass', 'es', 'nicht', 'die', 'T@@', 'o@@', 'ft', 'der', 'der', 'L@@', 'ei@@', 'ten', 'der', 'der', 'L@@', 'ei@@', 'ten', 'der', 'der', 'L@@', 'ei@@', 't.', '</s>']
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Sache, die die die Sachen der Leit, dass es nicht die Toft der der Leiten der der Leiten der der Leit.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - Example #2
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ind', 'in', 'der', 'K@@', 'rank@@', 'en@@', 'h@@', 'aup@@', 't', 'in', 'der', 'K@@', 'rank@@', 'en@@', 'h@@', 'eit@@', 's@@', 'einer', 'der', 'K@@', 'ör@@', 'per', 'der', 'K@@', 'rank@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', '."', '</s>']
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Hypothesis: Die Kind in der Krankenhaupt in der Krankenheitseiner der Körper der Krankenenenenenenenen."
2024-05-23 14:32:23,107 - INFO - joeynmt.training - Example #3
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:32:23,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'der', 'L@@', 'ei@@', 'ten', 'und', 'in', 'der', 'L@@', 'u@@', 'f@@', 'f@@', 't.', '</s>']
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:32:23,107 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:32:23,108 - INFO - joeynmt.training - 	Hypothesis: Es gibt in der Leiten und in der Lufft.
2024-05-23 14:32:23,108 - INFO - joeynmt.training - Example #4
2024-05-23 14:32:23,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:32:23,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:32:23,108 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'M@@', 'an@@', 'an@@', 'an@@', 'an@@', 'den', 'ich', 'ein', 'paar', 'D@@', 'oll@@', 'ar', 'zu', 'einem', 'F@@', 'äh@@', 'ig@@', 'kei@@', 'ten', 'von', '1@@', '0@@', '.@@', '000', 'D@@', 'oll@@', 'ar', 'in', 'den', '1@@', '1@@', '0@@', '.@@', '000', '1@@', '1@@', '0@@', '.', '</s>']
2024-05-23 14:32:23,108 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:32:23,108 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:32:23,108 - INFO - joeynmt.training - 	Hypothesis: Die Mananananden ich ein paar Dollar zu einem Fähigkeiten von 10.000 Dollar in den 110.000 110.
2024-05-23 14:32:40,203 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.842587, Batch Acc: 0.209503, Tokens per Sec:     4366, Lr: 0.000300
2024-05-23 14:32:56,924 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.780787, Batch Acc: 0.216653, Tokens per Sec:     4724, Lr: 0.000300
2024-05-23 14:33:13,582 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.878763, Batch Acc: 0.222787, Tokens per Sec:     4640, Lr: 0.000300
2024-05-23 14:33:30,434 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.555474, Batch Acc: 0.230218, Tokens per Sec:     4502, Lr: 0.000300
2024-05-23 14:33:46,891 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.613667, Batch Acc: 0.240684, Tokens per Sec:     4654, Lr: 0.000300
2024-05-23 14:33:46,892 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:33:46,892 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:35:22,716 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.69, ppl:  14.69, acc:   0.23, generation: 95.8135[sec], evaluation: 0.0000[sec]
2024-05-23 14:35:22,718 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:35:22,934 - INFO - joeynmt.training - Example #0
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'L@@', 'and', 'war', 'ich', 'zwei', 'Jahre', 'sp@@', 'rech@@', 't', 'ich', 'zwei', 'Jahren', 'habe', 'zwei', 'Jahren', 'die', 'letz@@', 'ten', 'S@@', 'pr@@', 'o', 'von', '1@@', '8', 'Jahren', 'die', 'letz@@', 'ten', 'Jahren', 'die', 'letz@@', 'ten', 'Jahren', 'Jahren', 'habe', 'habe', 'letz@@', 'ten', 'Jahren', 'Jahren', 'habe', 'habe', 'habe', 'ich', '1@@', '0@@', '0@@', '.@@', '000', 'Jahren', 'von', '1@@', '1@@', '1@@', '1@@', '0@@', '0@@', '.@@', '000', 'Jahre', 'al@@', 'so,', 'die', 'letz@@', 'ten', 'Jahr', '19@@', '9@@', '9@@', '0', 'Jahren', 'haben', 'haben', 'haben', 'haben', 'haben', 'haben', 'haben', 'haben', 'haben', 'diese', 'T@@', 'eil@@', 'en', 'von', 'zwei', 'Jahren', 'war', 'die', 'letz@@', 'ten', 'Jahren', 'war', 'die', 'letz@@', 'ten', 'Jahren', 'haben', 'haben', 'haben', 'diese', 'letz@@', 'ten', 'Jahren']
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Hypothesis: LLand war ich zwei Jahre sprecht ich zwei Jahren habe zwei Jahren die letzten Spro von 18 Jahren die letzten Jahren die letzten Jahren Jahren habe habe letzten Jahren Jahren habe habe habe ich 100.000 Jahren von 111100.000 Jahre also, die letzten Jahr 19990 Jahren haben haben haben haben haben haben haben haben haben diese Teilen von zwei Jahren war die letzten Jahren war die letzten Jahren haben haben haben diese letzten Jahren
2024-05-23 14:35:22,935 - INFO - joeynmt.training - Example #1
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'der', 'T@@', 'o@@', 'ch', 'der', 'T@@', 'o@@', 'ch', 'der', 'der', 'K@@', 'ör@@', 'per', 'zu', 'ver@@', 'wen@@', 'den,', 'weil', 'es', 'nicht', 'die', 'T@@', 'o@@', 'ch', 'des', 'T@@', 'o@@', 'ch', 'des', 'K@@', 'un@@', 'ft', 'des', 'K@@', 'un@@', 'ft', 'des', 'K@@', 'un@@', 'ft', 'des', 'K@@', 'ör@@', 'per@@', 's@@', 's@@', 'ch', 'des', 'K@@', 'ör@@', 'per@@', 's@@', 's@@', 'ch', 'zu', 'machen.', '</s>']
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Hypothesis: Aber das ist der Toch der Toch der der Körper zu verwenden, weil es nicht die Toch des Toch des Kunft des Kunft des Kunft des Körperssch des Körperssch zu machen.
2024-05-23 14:35:22,935 - INFO - joeynmt.training - Example #2
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:35:22,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ar@@', 'te', 'ist', 'eine', 'Art', 'von', 'einem', 'F@@', 'am@@', 'am@@', 'am@@', 'am@@', 'am@@', 'p@@', 'e,', 'die', 'die', 'K@@', 'un@@', 'gef@@', 'äh@@', 'r', 'der', 'K@@', 'un@@', 'un@@', 'gef@@', 'äh@@', 'n@@', 'lich@@', 'er', 'K@@', 'un@@', 'un@@', 'gef@@', 'äh@@', 'r', 'zu', 'ver@@', 'änder@@', 'n.', '</s>']
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:35:22,935 - INFO - joeynmt.training - 	Hypothesis: Die Karte ist eine Art von einem Famamamamampe, die die Kungefähr der Kunungefähnlicher Kunungefähr zu verändern.
2024-05-23 14:35:22,936 - INFO - joeynmt.training - Example #3
2024-05-23 14:35:22,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:35:22,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:35:22,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'der', 'S@@', 'ach@@', 'en', 'und', 'in', 'und', 'in', 'der', 'K@@', 'un@@', 'ft', 'in', 'der', 'S@@', 'ach@@', 'e.', '</s>']
2024-05-23 14:35:22,936 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:35:22,936 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:35:22,936 - INFO - joeynmt.training - 	Hypothesis: Es gibt in der Sachen und in und in der Kunft in der Sache.
2024-05-23 14:35:22,936 - INFO - joeynmt.training - Example #4
2024-05-23 14:35:22,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:35:22,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:35:22,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'sten', 'näch@@', 'ste', 'ste', 'T@@', 'ra@@', 'ß@@', ',', 'dass', 'ich', 'ein', 'F@@', 'ra@@', 'ß@@', 'er', 'ge@@', 'l', 'in', 'den', 'letz@@', 'ten', 'Jahr', 'passi@@', 'er@@', 't,', 'was', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 14:35:22,936 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:35:22,936 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:35:22,936 - INFO - joeynmt.training - 	Hypothesis: Das nächsten nächste ste Traß, dass ich ein Fraßer gel in den letzten Jahr passiert, was passiert.
2024-05-23 14:35:38,645 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.477320, Batch Acc: 0.245207, Tokens per Sec:     4866, Lr: 0.000300
2024-05-23 14:35:56,246 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.716754, Batch Acc: 0.250492, Tokens per Sec:     4416, Lr: 0.000300
2024-05-23 14:36:14,143 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.690710, Batch Acc: 0.249196, Tokens per Sec:     4170, Lr: 0.000300
2024-05-23 14:36:32,526 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.522695, Batch Acc: 0.259866, Tokens per Sec:     4166, Lr: 0.000300
2024-05-23 14:36:49,710 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.539113, Batch Acc: 0.264295, Tokens per Sec:     4459, Lr: 0.000300
2024-05-23 14:36:49,710 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:36:49,710 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:38:09,221 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.57, ppl:  13.07, acc:   0.26, generation: 79.5008[sec], evaluation: 0.0000[sec]
2024-05-23 14:38:09,223 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:38:09,432 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/500.ckpt
2024-05-23 14:38:09,512 - INFO - joeynmt.training - Example #0
2024-05-23 14:38:09,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:38:09,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:38:09,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'and', 'Jahr', 'Jahr', 'sp@@', 're@@', 'chen', 'ich', 'zwei', 'Jahre', 'sp@@', 're@@', 'chen', 'und', 'die', 'T@@', 'r@@', 'ü@@', 'r', 'der', 'letz@@', 'ten', 'S@@', 'tr@@', 'at@@', 'z@@', 'en,', 'die', 'die', 'die', 'letz@@', 'ten', 'drei', 'Jahre', 'von', 'drei', 'Jahren', 'der', 'letz@@', 'ten', 'Jahren', 'der', 'letz@@', 'ten', 'drei', 'Jahre', 'von', 'drei', 'Jahre', 'von', 'drei', 'Jahre', 'von', 'drei', 'Jahre', 'sp@@', 're@@', 'chen', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'letz@@', 'ten', 'Jahr', '200@@', '5', '5', 'Proz@@', 'ent', 'der', 'letz@@', 'ten', 'Jahr', '1@@', '5', 'Jahren', 'hat', 'drei', 'Jahre', 'von', 'drei', 'Jahren', 'ver@@', 'l@@', 'ä@@', 's@@', 's@@', 's@@', 's@@', 'eit', '1@@', '5', 'Jahre', 'sp@@', 'r@@', 'ü@@', 'tz@@', 'en.', '</s>']
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Hypothesis: Land Jahr Jahr sprechen ich zwei Jahre sprechen und die Trür der letzten Stratzen, die die die letzten drei Jahre von drei Jahren der letzten Jahren der letzten drei Jahre von drei Jahre von drei Jahre von drei Jahre sprechen von 40 Prozent der letzten Jahr 2005 5 Prozent der letzten Jahr 15 Jahren hat drei Jahre von drei Jahren verlässsseit 15 Jahre sprützen.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - Example #1
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'das', 'W@@', 'ass@@', 'er', 'der', 'K@@', 'op@@', 'f@@', 'te', 'dieses', 'Pro@@', 'blem', 'ist', 'das', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'nicht', 'die', 'K@@', 'ön@@', 'e', 'nicht', 'die', 'T@@', 'eil@@', 'e.', '</s>']
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Hypothesis: Aber das ist das Wasser der Kopfte dieses Problem ist das Problem ist, weil es nicht die Köne nicht die Teile.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - Example #2
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'F@@', 'ar@@', 'te', 'ist', 'ein', 'K@@', 'li@@', 'li@@', 'z@@', 'ier@@', 'ung', 'in', 'einem', 'K@@', 'li@@', 'li@@', 'z@@', 'ier@@', 'ung', 'der', 'K@@', 'li@@', 'li@@', 'ma@@', 'ma@@', 'ß@@', '.', '</s>']
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Hypothesis: Die Farte ist ein Klilizierung in einem Klilizierung der Klilimamaß.
2024-05-23 14:38:09,513 - INFO - joeynmt.training - Example #3
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:38:09,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'S@@', 'tr@@', 'ei@@', 'te', 'und', 'K@@', 'omm@@', 'un@@', 'st', 'in', 'S@@', 'pr@@', 'ache', 'zu', 'sein.', '</s>']
2024-05-23 14:38:09,513 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:38:09,514 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:38:09,514 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Streite und Kommunst in Sprache zu sein.
2024-05-23 14:38:09,514 - INFO - joeynmt.training - Example #4
2024-05-23 14:38:09,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:38:09,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:38:09,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'sten', 'näch@@', 'ste', 'T@@', 'r@@', 'au@@', 'er', 'zu', 'einem', 'letz@@', 'ten', 'F@@', 'ra@@', 'f@@', 'te', 'der', 'letz@@', 'ten', 'letz@@', 'ten', 'Jahr', '1@@', '5', 'Jahre', 'sp@@', 'ä@@', 't', 'wur@@', 'den', 'letz@@', 'ten', 'Jahr', '1@@', '5', 'Jahre', 'sp@@', 'ä@@', 'ter', 'der', 'letz@@', 'ten', 'Jahr', '1@@', '5', 'Jahre', 'sp@@', 'ä@@', 't.', '</s>']
2024-05-23 14:38:09,514 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:38:09,514 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:38:09,514 - INFO - joeynmt.training - 	Hypothesis: Die nächsten nächste Trauer zu einem letzten Frafte der letzten letzten Jahr 15 Jahre spät wurden letzten Jahr 15 Jahre später der letzten Jahr 15 Jahre spät.
2024-05-23 14:38:26,226 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.499991, Batch Acc: 0.272294, Tokens per Sec:     4529, Lr: 0.000300
2024-05-23 14:38:42,609 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.359666, Batch Acc: 0.283602, Tokens per Sec:     4872, Lr: 0.000300
2024-05-23 14:38:59,805 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.522318, Batch Acc: 0.282597, Tokens per Sec:     4521, Lr: 0.000300
2024-05-23 14:39:16,494 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.330950, Batch Acc: 0.287930, Tokens per Sec:     4536, Lr: 0.000300
2024-05-23 14:39:32,719 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.540023, Batch Acc: 0.295711, Tokens per Sec:     4696, Lr: 0.000300
2024-05-23 14:39:32,720 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:39:32,720 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:40:50,830 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.46, ppl:  11.75, acc:   0.29, generation: 78.1007[sec], evaluation: 0.0000[sec]
2024-05-23 14:40:50,833 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:40:51,092 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/1000.ckpt
2024-05-23 14:40:51,124 - INFO - joeynmt.training - Example #0
2024-05-23 14:40:51,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:40:51,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:40:51,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ich@@', 'te', 'ich', 'diese', 'zwei', 'Jahre', 'al@@', 'ten', 'zwei', 'Jahre', 'al@@', 't,', 'dass', 'die', 'T@@', 'r@@', 'au@@', 'er', 'der', 'T@@', 'r@@', 'au@@', 'er', 'der', 'letz@@', 'ten', 'S@@', 'ei@@', 'te', 'der', 'letz@@', 'ten', 'Jahre', 'al@@', 'ten', 'Jahren', 'der', 'letz@@', 'ten', 'Jahr', 'von', 'der', 'letz@@', 'ten', '4@@', '8', 'von', '4@@', '8', 'von', '4@@', '4@@', '8', 'Proz@@', 'ent', 'der', 'T@@', 'o@@', 's', 'des', '4@@', '0', 'Proz@@', 'ent', 'der', 'T@@', 'o@@', 's', 'des', '4@@', '0', 'Proz@@', 'ent', 'der', 'T@@', 'ag@@', 'es', 'des', 'Jahr', '4@@', '0', 'Proz@@', 'ent', 'der', 'T@@', 'o@@', 's', 'des', 'Jahr', '4@@', '0', 'Proz@@', 'ent', 'zwei', 'Jahre', 'al@@', 'ten', 'Mon@@', 'ate', 'des', 'Jahr', 'sp@@', 'ro@@', 'p@@']
2024-05-23 14:40:51,124 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:40:51,124 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:40:51,124 - INFO - joeynmt.training - 	Hypothesis: Lichte ich diese zwei Jahre alten zwei Jahre alt, dass die Trauer der Trauer der letzten Seite der letzten Jahre alten Jahren der letzten Jahr von der letzten 48 von 48 von 448 Prozent der Tos des 40 Prozent der Tos des 40 Prozent der Tages des Jahr 40 Prozent der Tos des Jahr 40 Prozent zwei Jahre alten Monate des Jahr sprop
2024-05-23 14:40:51,124 - INFO - joeynmt.training - Example #1
2024-05-23 14:40:51,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:40:51,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:40:51,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'K@@', 'ön@@', 'ig@@', 'i@@', 'ere', 'dieser', 'K@@', 'ör@@', 'per@@', 's@@', 'at@@', 'z', 'der', 'Er@@', 'fahr@@', 'ung', 'der', 'Er@@', 'n@@', 'utz@@', 't', 'nicht', 'die', 'Er@@', 'n@@', 'utz@@', 'ung', 'des', 'K@@', 'o@@', 's', 'des', 'K@@', 'o@@', 's', 'des', 'K@@', 'un@@', 's.', '</s>']
2024-05-23 14:40:51,124 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:40:51,124 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:40:51,124 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Königiere dieser Körpersatz der Erfahrung der Ernutzt nicht die Ernutzung des Kos des Kos des Kuns.
2024-05-23 14:40:51,124 - INFO - joeynmt.training - Example #2
2024-05-23 14:40:51,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ar@@', 'k@@', 'k@@', 'am', 'am', 'K@@', 'li@@', 'z@@', 'ier@@', 'te', 'der', 'Er@@', 'de', 'des', 'glob@@', 'alen', 'glob@@', 'alen', 'glob@@', 'alen', 'glob@@', 'alen', 'glob@@', 'alen', 'glob@@', 'alen', 'N@@', 'N@@', 'etz@@', 'wer@@', 'k@@', 'en.', '</s>']
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Hypothesis: Die Karkkam am Klizierte der Erde des globalen globalen globalen globalen globalen globalen NNetzwerken.
2024-05-23 14:40:51,125 - INFO - joeynmt.training - Example #3
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'sch@@', 'i@@', 'ed', 'und', 'in', 'und', 'in', 'den', 'K@@', 're@@', 'k@@', 't@@', 'ten', 'in', '</s>']
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Hypothesis: Es erschied und in und in den Krektten in
2024-05-23 14:40:51,125 - INFO - joeynmt.training - Example #4
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:40:51,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'sten', 'näch@@', 'sten', 'B@@', 'ild', 'ich', 'Ihnen', 'ein', 'F@@', 'ra@@', 'u', 'zu', 'einem', 'F@@', 'ra@@', 'u', 'in', 'der', 'letz@@', 'ten', 'Jahr', 'passi@@', 'er@@', 'te', 'das', 'letz@@', 'ten', 'Jahr', '2@@', '5', 'Jahre', 'al@@', 'ten', 'Jahr@@', 'en.', '</s>']
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:40:51,125 - INFO - joeynmt.training - 	Hypothesis: Die nächsten nächsten Bild ich Ihnen ein Frau zu einem Frau in der letzten Jahr passierte das letzten Jahr 25 Jahre alten Jahren.
2024-05-23 14:41:07,331 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.321491, Batch Acc: 0.304117, Tokens per Sec:     4648, Lr: 0.000300
2024-05-23 14:41:24,148 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.340534, Batch Acc: 0.309569, Tokens per Sec:     4595, Lr: 0.000300
2024-05-23 14:41:41,320 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.461084, Batch Acc: 0.311727, Tokens per Sec:     4652, Lr: 0.000300
2024-05-23 14:41:57,652 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.232846, Batch Acc: 0.317998, Tokens per Sec:     4612, Lr: 0.000300
2024-05-23 14:42:14,482 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.363017, Batch Acc: 0.317016, Tokens per Sec:     4587, Lr: 0.000300
2024-05-23 14:42:14,483 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:42:14,483 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:43:34,301 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.37, ppl:  10.70, acc:   0.32, generation: 79.8088[sec], evaluation: 0.0000[sec]
2024-05-23 14:43:34,303 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:43:34,548 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/1500.ckpt
2024-05-23 14:43:34,602 - INFO - joeynmt.training - Example #0
2024-05-23 14:43:34,602 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:43:34,602 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:43:34,602 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'as@@', 'sen', 'Sie', 'diese', 'zwei', 'Jahre', 'sp@@', 're@@', 'chen', 'zwei', 'Mon@@', 'aten', 'zu', 'er@@', 'w@@', 'ach@@', 'sen', 'dass', 'die', 'letz@@', 'ten', 'Mon@@', 'at@@', 'en,', 'die', 'die', 'letz@@', 'ten', 'drei', 'Jahre', 'sp@@', 'rech@@', 'en,', 'die', 'letz@@', 'ten', 'drei', 'Jahre', 'sp@@', 'rech@@', 'en,', 'die', 'letz@@', 'ten', '4@@', '8', 'Proz@@', 'ent', 'von', '4@@', '8', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 14:43:34,602 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:43:34,602 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:43:34,602 - INFO - joeynmt.training - 	Hypothesis: Lassen Sie diese zwei Jahre sprechen zwei Monaten zu erwachsen dass die letzten Monaten, die die letzten drei Jahre sprechen, die letzten drei Jahre sprechen, die letzten 48 Prozent von 48 Prozent von 40 Prozent von 40 Prozent von 40 Prozent von 40 Prozent von 40 Prozent von 40 Prozent von 40 Prozent von 40 Prozent von 40 Prozent.
2024-05-23 14:43:34,602 - INFO - joeynmt.training - Example #1
2024-05-23 14:43:34,602 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:43:34,602 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:43:34,602 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'B@@', 'es@@', 'ch@@', 'i@@', 'ere', 'der', 'B@@', 'et@@', 'ri@@', 'es@@', 'ige', 'dieser', 'S@@', 'ei@@', 'te', 'der', 'Er@@', 'de', 'nicht', 'die', 'S@@', 'ach@@', 'e,', 'weil', 'es', 'nicht', 'die', 'S@@', 'ach@@', 'e.', '</s>']
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Hypothesis: Aber diese Beschiere der Betriesige dieser Seite der Erde nicht die Sache, weil es nicht die Sache.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - Example #2
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ar@@', 'k@@', 'k@@', 'ri@@', 'eg@@', 't', 'sich', 'in', 'einer', 'G@@', 'ru@@', 'pp@@', 'e', 'der', 'der', 'glob@@', 'ale', 'Wel@@', 't.', '</s>']
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Hypothesis: Die Karkkriegt sich in einer Gruppe der der globale Welt.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - Example #3
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'S@@', 'pr@@', 'ach@@', 'en', 'und', 'in', 'der', 'L@@', 'ehr@@', 'er', 'in', 'der', 'S@@', 'ach@@', 'e.', '</s>']
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Sprachen und in der Lehrer in der Sache.
2024-05-23 14:43:34,603 - INFO - joeynmt.training - Example #4
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:43:34,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'S@@', 'li@@', 'ck', 'ist,', 'dass', 'ich', 'Ihnen', 'ein', 'B@@', 'r@@', 'r@@', 'ü@@', 'st@@', 'st@@', 'st@@', 'a@@', 'at@@', '-@@', 'S@@', 'ei@@', 'te', 'zu', 'sehen,', 'was', 'das', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'sp@@', 'ä@@', 'ter', 'wur@@', 'de.', '</s>']
2024-05-23 14:43:34,604 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:43:34,604 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:43:34,604 - INFO - joeynmt.training - 	Hypothesis: Die nächste Slick ist, dass ich Ihnen ein Brrüstststaat-Seite zu sehen, was das letzten 25 Jahre später wurde.
2024-05-23 14:43:53,392 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.245910, Batch Acc: 0.324918, Tokens per Sec:     4059, Lr: 0.000300
2024-05-23 14:44:09,750 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.103045, Batch Acc: 0.331296, Tokens per Sec:     4710, Lr: 0.000300
2024-05-23 14:44:27,008 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.289095, Batch Acc: 0.337816, Tokens per Sec:     4499, Lr: 0.000300
2024-05-23 14:44:42,955 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.141785, Batch Acc: 0.341349, Tokens per Sec:     4796, Lr: 0.000300
2024-05-23 14:44:59,763 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.347210, Batch Acc: 0.349329, Tokens per Sec:     4594, Lr: 0.000300
2024-05-23 14:44:59,763 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:44:59,763 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:46:16,704 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.28, ppl:   9.76, acc:   0.34, generation: 76.9314[sec], evaluation: 0.0000[sec]
2024-05-23 14:46:16,706 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:46:16,942 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/2000.ckpt
2024-05-23 14:46:16,996 - INFO - joeynmt.training - Example #0
2024-05-23 14:46:16,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:46:16,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:46:16,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'Jahren', 'habe', 'ich', 'diese', 'zwei', 'zwei', 'zwei', 'zwei', 'Jahren', 'ver@@', 'wen@@', 'd@@', 'et,', 'dass', 'die', 'die', 'letz@@', 'ten', '4@@', '0', 'Jahren', 'für', 'drei', 'Jahren', 'für', 'drei', 'Jahren', 'die', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'von', '4@@', '8', 'Jahren', 'haben', 'die', 'letz@@', 'ten', '4@@', '8', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 14:46:16,996 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:46:16,996 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:46:16,996 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Jahren habe ich diese zwei zwei zwei zwei Jahren verwendet, dass die die letzten 40 Jahren für drei Jahren für drei Jahren die letzten drei Millionen Jahren von 48 Jahren haben die letzten 48 Prozent von 40 Prozent.
2024-05-23 14:46:16,996 - INFO - joeynmt.training - Example #1
2024-05-23 14:46:16,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:46:16,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:46:16,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'sind', 'die', 'W@@', 'er@@', 'sten', 'der', 'bes@@', 'onder@@', 'e', 'von', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'des', 'des', 'T@@', 'i@@', 'ere', 'der', 'W@@', 'eg@@', 's@@', 'at@@', 't', 'der', 'R@@', 'ei@@', 'se', 'des', 'W@@', 'eg@@', 's.', '</s>']
2024-05-23 14:46:16,996 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:46:16,996 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:46:16,996 - INFO - joeynmt.training - 	Hypothesis: Aber das sind die Wersten der besondere von dieser besonders Problem des des Tiere der Wegsatt der Reise des Wegs.
2024-05-23 14:46:16,996 - INFO - joeynmt.training - Example #2
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ar@@', 'k@@', 'ti@@', 'k@@', 'er', 'ist', 'in', 'einem', 'K@@', 'li@@', 'ma@@', 'ma@@', 'g', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'ma@@', 'at@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'ma@@', 'at@@', '.', '</s>']
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Hypothesis: Die Karktiker ist in einem Klimamag der globalen Klimamaatz des globalen Klimamaat.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - Example #3
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'die', 'F@@', 'ol@@', 'le', 'und', 'Kon@@', 'sum@@', 'en@@', 'ten', 'zu', 'einem', 'Z@@', 'eit@@', 'en.', '</s>']
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Hypothesis: Es ist die Folle und Konsumenten zu einem Zeiten.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - Example #4
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:46:16,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'sten', 'sten', 'S@@', 'ei@@', 'te', 'ich', 'Ihnen', 'ein', 'St@@', 'ra@@', 'f@@', 'te', 'zu', 'einem', 'F@@', 'ra@@', 'u', 'w@@', 'o@@', 'ben', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:46:16,997 - INFO - joeynmt.training - 	Hypothesis: Die nächsten sten Seite ich Ihnen ein Strafte zu einem Frau woben passiert ist.
2024-05-23 14:46:18,826 - INFO - joeynmt.training - Epoch   1: total training loss 13038.64
2024-05-23 14:46:18,826 - INFO - joeynmt.training - EPOCH 2
2024-05-23 14:46:32,783 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.159405, Batch Acc: 0.356735, Tokens per Sec:     4777, Lr: 0.000300
2024-05-23 14:46:48,191 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     2.116346, Batch Acc: 0.358192, Tokens per Sec:     5112, Lr: 0.000300
2024-05-23 14:47:03,836 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.141893, Batch Acc: 0.360890, Tokens per Sec:     4869, Lr: 0.000300
2024-05-23 14:47:18,760 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.219309, Batch Acc: 0.369864, Tokens per Sec:     5190, Lr: 0.000300
2024-05-23 14:47:34,601 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     2.106011, Batch Acc: 0.370018, Tokens per Sec:     4878, Lr: 0.000300
2024-05-23 14:47:34,602 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:47:34,603 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:48:55,955 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.22, ppl:   9.19, acc:   0.36, generation: 81.3413[sec], evaluation: 0.0000[sec]
2024-05-23 14:48:55,957 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:48:56,178 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/2500.ckpt
2024-05-23 14:48:56,238 - INFO - joeynmt.training - Example #0
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'Jahre', 'al@@', 't', 'diese', 'zwei', 'S@@', 'li@@', 'ck', 'auf', 'die', 'K@@', 'ar@@', 'z@@', ',', 'dass', 'die', 'K@@', 'ar@@', 'z@@', 't@@', 'ische', 'K@@', 'ap@@', 'it@@', 'z@@', ',', 'die', 'für', 'die', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'K@@', 'ar@@', 'z@@', ',', 'die', 'K@@', 'ar@@', 'z@@', ',', 'die', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Jahre alt diese zwei Slick auf die Karz, dass die Karztische Kapitz, die für die drei Millionen Jahren hat die drei Millionen Jahren hat die Karz, die Karz, die von 40 Prozent.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - Example #1
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'An@@', 'der@@', 'e', 'der', 'T@@', 'ri@@', 'eb@@', 's@@', 's@@', 'ei@@', 'te', 'dieses', 'T@@', 'eil@@', 's', 'Pro@@', 'blem', 'des', 'T@@', 'ag@@', 'es', 'nicht', 'die', 'An@@', 's@@', 'ei@@', 'tig', 'der', 'Gr@@', 'öß@@', 'e', 'des', 'Gr@@', 'unde', 'der', 'Gr@@', 'unde', 'des', 'ist.', '</s>']
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Andere der Triebsseite dieses Teils Problem des Tages nicht die Anseitig der Größe des Grunde der Grunde des ist.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - Example #2
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:48:56,239 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ar@@', 'k@@', 'ti@@', 'k@@', 'er', 'ist', 'in', 'einem', 'K@@', 'ra@@', 'ft', 'der', 'Wel@@', 't,', 'die', 'Her@@', 'z@@', 'z@@', 'en@@', 'st@@', 'off@@', 'e', 'Sy@@', 'st@@', 'em@@', 'at@@', '.', '</s>']
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:48:56,239 - INFO - joeynmt.training - 	Hypothesis: Die Karktiker ist in einem Kraft der Welt, die Herzzenstoffe Systemat.
2024-05-23 14:48:56,240 - INFO - joeynmt.training - Example #3
2024-05-23 14:48:56,240 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:48:56,240 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:48:56,240 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'in', 'W@@', 'in@@', 'k@@', 'tr@@', 'ä@@', 's@@', 'st', 'in', 'der', 'F@@', 'in@@', 'anz@@', '.', '</s>']
2024-05-23 14:48:56,240 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:48:56,240 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:48:56,240 - INFO - joeynmt.training - 	Hypothesis: Es erwartete in Winkträsst in der Finanz.
2024-05-23 14:48:56,240 - INFO - joeynmt.training - Example #4
2024-05-23 14:48:56,240 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:48:56,240 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:48:56,240 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'S@@', 'li@@', 'ck', 'ich', 'Ihnen', 'eine', 'St@@', 'ra@@', 'f@@', 'te', 'ich', 'Ihnen', 'eine', 'St@@', 'ra@@', 'f@@', 'te', 'von', '2@@', '5', 'Jahren', 'das', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'das', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'das', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'al@@', 't.', '</s>']
2024-05-23 14:48:56,240 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:48:56,240 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:48:56,240 - INFO - joeynmt.training - 	Hypothesis: Die nächste Slick ich Ihnen eine Strafte ich Ihnen eine Strafte von 25 Jahren das letzten 25 Jahren das letzten 25 Jahren das letzten 25 Jahre alt.
2024-05-23 14:49:13,726 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.168033, Batch Acc: 0.373561, Tokens per Sec:     4507, Lr: 0.000300
2024-05-23 14:49:31,341 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.108284, Batch Acc: 0.380676, Tokens per Sec:     4438, Lr: 0.000300
2024-05-23 14:49:48,742 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.202296, Batch Acc: 0.378945, Tokens per Sec:     4569, Lr: 0.000300
2024-05-23 14:50:07,064 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.045219, Batch Acc: 0.384831, Tokens per Sec:     4204, Lr: 0.000300
2024-05-23 14:50:24,526 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     2.212646, Batch Acc: 0.387874, Tokens per Sec:     4233, Lr: 0.000300
2024-05-23 14:50:24,527 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:50:24,527 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:51:43,310 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.16, ppl:   8.66, acc:   0.38, generation: 78.7727[sec], evaluation: 0.0000[sec]
2024-05-23 14:51:43,313 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:51:43,518 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/3000.ckpt
2024-05-23 14:51:43,559 - INFO - joeynmt.training - Example #0
2024-05-23 14:51:43,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:51:43,560 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:51:43,560 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'Jahren', 'habe', 'diese', 'zwei', 'S@@', 'li@@', 'ck', 'auf', 'die', 'K@@', 'ar@@', 'k@@', 'ti@@', 'ef@@', 'e', 'der', 'K@@', 'ar@@', 'k@@', 'ti@@', 'k', 'der', 'letz@@', 'te', 'drei', 'Millionen', 'Jahren', 'der', 'letz@@', 'te', 'drei', 'Millionen', 'Jahren', 'der', 'L@@', 'eu@@', 'ten', 'der', 'L@@', 'age', 'der', 'von', '4@@', '8', 'Proz@@', 'ent', 'der', 'L@@', 'eu@@', 'ten', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 14:51:43,560 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:51:43,560 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:51:43,560 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Jahren habe diese zwei Slick auf die Karktiefe der Karktik der letzte drei Millionen Jahren der letzte drei Millionen Jahren der Leuten der Lage der von 48 Prozent der Leuten von 40 Prozent.
2024-05-23 14:51:43,560 - INFO - joeynmt.training - Example #1
2024-05-23 14:51:43,560 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:51:43,560 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:51:43,560 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'W@@', 'er@@', 'sten', 'der', 'T@@', 'ats@@', 'ach@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'rit@@', 'te', 'Pro@@', 'blem', 'des', 'D@@', 'am@@', 'es', 'ist', 'nicht', 'die', 'D@@', 'rit@@', 'te@@', 'l', 'der', 'Gr@@', 'und@@', 'e.', '</s>']
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Wersten der Tatsache, weil es nicht die Dritte Problem des Dames ist nicht die Drittel der Grunde.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - Example #2
2024-05-23 14:51:43,561 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:51:43,561 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:51:43,561 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ar@@', 'ti@@', 'k@@', 'er', 'ist', 'eine', 'S@@', 'ei@@', 'te', 'der', 'glob@@', 'ale', 'K@@', 'li@@', 'z@@', 'ier@@', 'te', 'der', 'glob@@', 'ale', 'K@@', 'li@@', 'ma@@', 'ma@@', 'at@@', 'isch', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Hypothesis: Die Kartiker ist eine Seite der globale Klizierte der globale Klimamaatisch System.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - Example #3
2024-05-23 14:51:43,561 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:51:43,561 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:51:43,561 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ung', 'und', 'Kon@@', 'ta@@', 'k@@', 'ten', 'und', 'Kon@@', 'sum@@', 'en@@', 'ten', 'in', 'der', 'Ver@@', 'ein@@', 'ig@@', 'ung.', '</s>']
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - 	Hypothesis: Es erwartung und Kontakten und Konsumenten in der Vereinigung.
2024-05-23 14:51:43,561 - INFO - joeynmt.training - Example #4
2024-05-23 14:51:43,562 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:51:43,562 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:51:43,562 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'li@@', 'sch@@', 'e', 'ich', 'Ihnen', 'ein', 'St@@', 'ra@@', 'f@@', 'te', 'ich', 'Ihnen', 'ein', 'St@@', 'ra@@', 'f@@', 'te', 'der', 'letz@@', 'te', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 14:51:43,562 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:51:43,562 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:51:43,562 - INFO - joeynmt.training - 	Hypothesis: Die nächste Flische ich Ihnen ein Strafte ich Ihnen ein Strafte der letzte 25 Jahren passiert.
2024-05-23 14:52:01,086 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.186224, Batch Acc: 0.389894, Tokens per Sec:     4227, Lr: 0.000300
2024-05-23 14:52:17,837 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     2.038447, Batch Acc: 0.392037, Tokens per Sec:     4702, Lr: 0.000300
2024-05-23 14:52:34,238 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.999608, Batch Acc: 0.393146, Tokens per Sec:     4655, Lr: 0.000300
2024-05-23 14:52:51,812 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     2.100121, Batch Acc: 0.393393, Tokens per Sec:     4462, Lr: 0.000300
2024-05-23 14:53:07,718 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     1.960646, Batch Acc: 0.395860, Tokens per Sec:     4860, Lr: 0.000300
2024-05-23 14:53:07,719 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:53:07,719 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:54:23,680 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.10, ppl:   8.14, acc:   0.39, generation: 75.9506[sec], evaluation: 0.0000[sec]
2024-05-23 14:54:23,682 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:54:23,933 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/3500.ckpt
2024-05-23 14:54:24,008 - INFO - joeynmt.training - Example #0
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'z@@', 'ier@@', 'en,', 'so', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'K@@', 'op@@', 'f', 'von', 'den', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'S@@', 'on@@', 'n@@', 'en@@', 'nen', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Hypothesis: Letzten Jahr habe ich diese zwei Slizieren, so dass die Künstler der Kopf von den letzten drei Millionen Jahre hat die meisten drei Millionen Jahre hat die meisten drei Millionen Jahre hat die Sonnennen von 40 Prozent.
2024-05-23 14:54:24,009 - INFO - joeynmt.training - Example #1
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'S@@', 'er@@', 'ie', 'der', 'T@@', 'ei@@', 'le', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'des', 'bes@@', 'onder@@', 's', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'rit@@', 'te', 'der', 'S@@', 'tim@@', 'm@@', 'e.', '</s>']
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Serie der Teile dieser besonders Problem des besonders ist, weil es nicht die Dritte der Stimme.
2024-05-23 14:54:24,009 - INFO - joeynmt.training - Example #2
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:54:24,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ar@@', 'ti@@', 'k@@', 'er', 'ist', 'ein', 'S@@', 'pe@@', 'k@@', 'k@@', 'k@@', 'am', 'bes@@', 't@@', 'eht', 'die', 'G@@', 'lob@@', 'al', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'at@@', 'z', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'at@@', 'z@@', '.', '</s>']
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:54:24,009 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:54:24,010 - INFO - joeynmt.training - 	Hypothesis: Die Kartiker ist ein Spekkkam besteht die Global der globalen Klimaatz der globalen Klimaatz.
2024-05-23 14:54:24,010 - INFO - joeynmt.training - Example #3
2024-05-23 14:54:24,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:54:24,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:54:24,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'eten', 'in', 'W@@', 'in@@', 'der@@', ',', 'und', 'Kon@@', 'sum@@', 'en@@', 'ten', 'in', 'S@@', 'omm@@', '.', '</s>']
2024-05-23 14:54:24,010 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:54:24,010 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:54:24,010 - INFO - joeynmt.training - 	Hypothesis: Es erwarteten in Winder, und Konsumenten in Somm.
2024-05-23 14:54:24,010 - INFO - joeynmt.training - Example #4
2024-05-23 14:54:24,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:54:24,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:54:24,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'S@@', 'li@@', 'z', 'zei@@', 'gen', 'ich', 'Ihnen', 'ein', 'St@@', 'ück', 'St@@', 'ra@@', 'ß@@', '-@@', 'F@@', 'ast@@', '-@@', 'B@@', 'l@@', 'ü@@', 'h@@', 't', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 14:54:24,010 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:54:24,010 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:54:24,010 - INFO - joeynmt.training - 	Hypothesis: Die nächste Sliz zeigen ich Ihnen ein Stück Straß-Fast-Blüht passiert ist.
2024-05-23 14:54:41,797 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     2.098474, Batch Acc: 0.397750, Tokens per Sec:     4361, Lr: 0.000300
2024-05-23 14:54:58,404 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     2.076547, Batch Acc: 0.399033, Tokens per Sec:     4458, Lr: 0.000300
2024-05-23 14:55:15,738 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     2.024735, Batch Acc: 0.402476, Tokens per Sec:     4437, Lr: 0.000300
2024-05-23 14:55:33,012 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.985236, Batch Acc: 0.409242, Tokens per Sec:     4470, Lr: 0.000300
2024-05-23 14:55:49,974 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.030700, Batch Acc: 0.407192, Tokens per Sec:     4603, Lr: 0.000300
2024-05-23 14:55:49,975 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:55:49,975 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:57:04,293 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.06, ppl:   7.83, acc:   0.40, generation: 74.3082[sec], evaluation: 0.0000[sec]
2024-05-23 14:57:04,295 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:57:04,556 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/4000.ckpt
2024-05-23 14:57:04,615 - INFO - joeynmt.training - Example #0
2024-05-23 14:57:04,615 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:57:04,615 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:57:04,615 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'ma@@', 'g', 'zeig@@', 't', 'diese', 'zwei', 'S@@', 'li@@', 'ma@@', 'at@@', ',', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahren', 'die', 'S@@', 'eh@@', 'n@@', 'en@@', 'e', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent', 'Proz@@', 'ent.', '</s>']
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Hypothesis: Letzten ich diese zwei Slimag zeigt diese zwei Slimaat, dass die Künstler der letzten drei Millionen Jahren die meisten drei Millionen Jahren hat die meisten drei Millionen Jahren die Sehnene von 40 Prozent von 40 Prozent von 40 Prozent von 40 Prozent Prozent.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - Example #1
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'St@@', 'at@@', 'at@@', 'en', 'die', 'die', 'S@@', 'er@@', 'for@@', 'sch@@', 'e', 'dieser', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'blem', 'ist,', 'weil', 'es', 'die', 'D@@', 'rit@@', 'rit@@', 'te', 'der', 'Z@@', 'eit@@', 'e.', '</s>']
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Hypothesis: Aber diese Stataten die die Serforsche dieser besondere Problem blem ist, weil es die Dritritte der Zeite.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - Example #2
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einer', 'S@@', 'ei@@', 'te', 'ist', 'in', 'einer', 'S@@', 'ei@@', 'te', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'at@@', 'z@@', '.', '</s>']
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist in einer Seite ist in einer Seite der globalen Klimaatz.
2024-05-23 14:57:04,616 - INFO - joeynmt.training - Example #3
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:57:04,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'sum@@', 'en@@', 'te', 'in', 'S@@', 'omm@@', '.', '</s>']
2024-05-23 14:57:04,617 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:57:04,617 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:57:04,617 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Konsumente in Somm.
2024-05-23 14:57:04,617 - INFO - joeynmt.training - Example #4
2024-05-23 14:57:04,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:57:04,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:57:04,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'S@@', 'li@@', 'de', 'zeig@@', 't', 'man', 'eine', 'St@@', 'ra@@', 'p@@', 'p@@', 'ra@@', 'd', 'zu', 'sein,', 'was', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 14:57:04,617 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:57:04,617 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:57:04,617 - INFO - joeynmt.training - 	Hypothesis: Die nächste Slide zeigt man eine Strapprad zu sein, was passiert.
2024-05-23 14:57:21,874 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     2.068892, Batch Acc: 0.409855, Tokens per Sec:     4297, Lr: 0.000300
2024-05-23 14:57:37,731 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.071429, Batch Acc: 0.414561, Tokens per Sec:     4744, Lr: 0.000300
2024-05-23 14:57:55,534 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     2.078175, Batch Acc: 0.417688, Tokens per Sec:     4377, Lr: 0.000300
2024-05-23 14:58:13,091 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     2.019743, Batch Acc: 0.409573, Tokens per Sec:     4401, Lr: 0.000300
2024-05-23 14:58:29,726 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     1.862424, Batch Acc: 0.417746, Tokens per Sec:     4681, Lr: 0.000300
2024-05-23 14:58:29,727 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 14:58:29,727 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 14:59:50,964 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.53, acc:   0.41, generation: 81.2275[sec], evaluation: 0.0000[sec]
2024-05-23 14:59:50,966 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 14:59:51,213 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/4500.ckpt
2024-05-23 14:59:51,264 - INFO - joeynmt.training - Example #0
2024-05-23 14:59:51,264 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 14:59:51,264 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 14:59:51,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'ck', 'von', '4@@', '0', 'Proz@@', 'ent', 'so', 'dass', 'die', 'K@@', 'ar@@', 'k@@', 't@@', 'ische', 'K@@', 'ap@@', ',', 'die', 'die', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'die', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'die', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'Gr@@', 'öß@@', 'e', 'von', '4@@', '8', 'St@@', 'at@@', 'z@@', 'en,', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 14:59:51,265 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 14:59:51,265 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 14:59:51,265 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Slick von 40 Prozent so dass die Karktische Kap, die die letzten drei Millionen Jahren die letzten drei Millionen Jahren die letzten drei Millionen Jahre hat die Größe von 48 Statzen, von 40 Prozent.
2024-05-23 14:59:51,265 - INFO - joeynmt.training - Example #1
2024-05-23 14:59:51,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 14:59:51,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 14:59:51,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 'e', 'St@@', 'at@@', 'z@@', 'e,', 'die', 'die', 'An@@', 't@@', 'wor@@', 't', 'dieser', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'rit@@', 'isch@@', 'er', 'ist,', 'weil', 'es', 'nicht', 'der', 'Gr@@', 'und@@', 'e.', '</s>']
2024-05-23 14:59:51,265 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 14:59:51,265 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 14:59:51,265 - INFO - joeynmt.training - 	Hypothesis: Aber das verstehe Statze, die die Antwort dieser besondere Problem ist, weil es nicht die Dritischer ist, weil es nicht der Grunde.
2024-05-23 14:59:51,265 - INFO - joeynmt.training - Example #2
2024-05-23 14:59:51,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 14:59:51,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 14:59:51,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einer', 'S@@', 'in@@', 'ne', 'der', 'glob@@', 'ale', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', ',', 'der', 'Her@@', 'z@@', 'k@@', 'li@@', 'eg@@', 't', 'der', 'glob@@', 'ale', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', '-@@', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist in einer Sinne der globale Klimawandel, der Herzkliegt der globale Klimawande-System.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - Example #3
2024-05-23 14:59:51,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 14:59:51,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 14:59:51,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Hypothesis: Es erwartete in Sommer und Kontragen.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - Example #4
2024-05-23 14:59:51,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 14:59:51,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 14:59:51,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'rit@@', 'te', 'zei@@', 'gen', 'werden', 'man', 'eine', 'ra@@', 'f@@', 'te', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'äh@@', 'n@@', 't.', '</s>']
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 14:59:51,266 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schritte zeigen werden man eine rafte Fast-Fast-Fast-Fast-Fähnt.
2024-05-23 15:00:10,452 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     1.976271, Batch Acc: 0.410824, Tokens per Sec:     3833, Lr: 0.000300
2024-05-23 15:00:29,248 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.870361, Batch Acc: 0.417779, Tokens per Sec:     4155, Lr: 0.000300
2024-05-23 15:00:47,748 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.991321, Batch Acc: 0.422570, Tokens per Sec:     4208, Lr: 0.000300
2024-05-23 15:01:08,815 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.924141, Batch Acc: 0.419171, Tokens per Sec:     3692, Lr: 0.000300
2024-05-23 15:01:28,240 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.957932, Batch Acc: 0.426335, Tokens per Sec:     3939, Lr: 0.000300
2024-05-23 15:01:28,240 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:01:28,240 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:03:02,490 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.99, ppl:   7.29, acc:   0.42, generation: 94.2365[sec], evaluation: 0.0000[sec]
2024-05-23 15:03:02,492 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:03:02,776 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/5000.ckpt
2024-05-23 15:03:02,845 - INFO - joeynmt.training - Example #0
2024-05-23 15:03:02,846 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:03:02,846 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:03:02,846 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'ma@@', 'g', 'dieser', 'zwei', 'S@@', 'li@@', 'ma@@', ',', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahren', 'war', 'die', 'letz@@', 'ten', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'ei@@', 'st@@', 'ung', 'von', '4@@', '0', 'Proz@@', 'ent', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:03:02,847 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:03:02,847 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:03:02,847 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Slimag dieser zwei Slima, dass die Künstler der letzten drei Millionen Jahren die meisten drei Millionen Jahren war die letzten 40 Prozent der Leistung von 40 Prozent von 40 Prozent.
2024-05-23 15:03:02,847 - INFO - joeynmt.training - Example #1
2024-05-23 15:03:02,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'l@@', 'ing@@', 's@@', 's@@', 'at@@', 'z', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'nicht', 'die', 'D@@', 'rit@@', 'te', 'der', 'E@@', 'is@@', 'enschaf@@', 't', 'des', 'E@@', 'is@@', 'm@@', 's', 'der', 'E@@', 'is@@', 'enschaf@@', 't', 'ist.', '</s>']
2024-05-23 15:03:02,850 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:03:02,850 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:03:02,850 - INFO - joeynmt.training - 	Hypothesis: Aber das Verlingssatz dieser besonders Problem dieser besonders Problem zu zeigen, weil es nicht die Dritte der Eisenschaft des Eisms der Eisenschaft ist.
2024-05-23 15:03:02,850 - INFO - joeynmt.training - Example #2
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'der', 'S@@', 'in@@', 'ne', 'der', 'Er@@', 'k@@', 'enn@@', 't@@', 'n@@', 'is', 'der', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'der', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'der', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'von', 'der', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'von', 'der', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'der', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'von', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'z@@', '.', '</s>']
2024-05-23 15:03:02,850 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:03:02,850 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:03:02,850 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist der Sinne der Erkenntnis der globalen Klima der globalen Klima der globalen Klima des globalen Klima des globalen Klima von der globalen Klima von der globalen Klima der globalen Klima von der globalen Klimawanz.
2024-05-23 15:03:02,850 - INFO - joeynmt.training - Example #3
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:03:02,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ag@@', 'ung', 'in', 'S@@', 'omm@@', 'en.', '</s>']
2024-05-23 15:03:02,851 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:03:02,851 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:03:02,851 - INFO - joeynmt.training - 	Hypothesis: Es erwartete sich in Winter und Vertragung in Sommen.
2024-05-23 15:03:02,851 - INFO - joeynmt.training - Example #4
2024-05-23 15:03:02,851 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:03:02,851 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:03:02,851 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'au@@', 't@@', 'et@@', 'e,', 'dass', 'Sie', 'ein', 'ra@@', 'ph@@', 'ph@@', 'en', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'Jahre', 'im', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'al@@', 't', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 15:03:02,851 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:03:02,851 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:03:02,851 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schlautete, dass Sie ein raphphen Fast-Fast-Fast-Jahre im letzten 25 Jahre alt passiert.
2024-05-23 15:03:21,126 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.955457, Batch Acc: 0.425416, Tokens per Sec:     4135, Lr: 0.000300
2024-05-23 15:03:40,168 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.887694, Batch Acc: 0.422632, Tokens per Sec:     4053, Lr: 0.000300
2024-05-23 15:03:58,619 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.886832, Batch Acc: 0.423780, Tokens per Sec:     4109, Lr: 0.000300
2024-05-23 15:04:15,560 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.867072, Batch Acc: 0.422694, Tokens per Sec:     4477, Lr: 0.000300
2024-05-23 15:04:31,097 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     1.738309, Batch Acc: 0.426751, Tokens per Sec:     4884, Lr: 0.000300
2024-05-23 15:04:31,099 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:04:31,099 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:05:45,804 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.05, acc:   0.42, generation: 74.6944[sec], evaluation: 0.0000[sec]
2024-05-23 15:05:45,806 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:05:46,042 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/5500.ckpt
2024-05-23 15:05:46,072 - INFO - joeynmt.training - Example #0
2024-05-23 15:05:46,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:05:46,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:05:46,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'ma@@', 'g', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'L@@', 'ö@@', 's@@', 'ung', 'so', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'war', 'die', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 's@@', 'eit', '4@@', '8', 'St@@', 'amm@@', 'el@@', 'n', 'von', '4@@', '8', 'St@@', 'amm@@', 'l@@', 'ung', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:05:46,072 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:05:46,072 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:05:46,072 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Slimag zeigte ich diese zwei Lösung so dass die Künstler der letzten drei Millionen Jahren war die letzten drei Millionen Jahre seit 48 Stammeln von 48 Stammlung von 40 Prozent.
2024-05-23 15:05:46,072 - INFO - joeynmt.training - Example #1
2024-05-23 15:05:46,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:05:46,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:05:46,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'St@@', 'at@@', 't@@', 'ung', 'dieser', 'bes@@', 'onder@@', 'e', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'dieser', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'nicht', 'die', 'D@@', 'rit@@', 'te', 'des', 'E@@', 'ig@@', 'enschaf@@', 't', 'nicht', 'die', 'D@@', 'is@@', 'k@@', 'en@@', 'e', 'des', 'E@@', 'ig@@', 'enschaf@@', 't', 'ist.', '</s>']
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Hypothesis: Aber diese Stattung dieser besondere besondere Problem dieser besondere Problem nicht die Dritte des Eigenschaft nicht die Diskene des Eigenschaft ist.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - Example #2
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'eine', 'S@@', 'in@@', 'ne', 'ist', 'das', 'Her@@', 'z', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist', 'das', 'Her@@', 'z', 'der', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist', 'das', 'Her@@', 'z', 'des', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist', 'das', 'K@@', 'ar@@', 'k@@', 'st@@', 'eig@@']
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Hypothesis: Das Künstler ist eine Sinne ist das Herz der globalen Klimawandel der globalen Klimawandel des globalen Klimawandel ist das Herz des globalen Klimawandel der globalen Klimawandel ist das Herz der Klimawandel der globalen Klimawandel ist das Herz des des globalen Klimawandel ist das Karksteig
2024-05-23 15:05:46,073 - INFO - joeynmt.training - Example #3
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'ag@@', 't', 'in', 'S@@', 'omm@@', 'en', 'in', 'S@@', 'omm@@', 'en.', '</s>']
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Sommer und Kontragt in Sommen in Sommen.
2024-05-23 15:05:46,073 - INFO - joeynmt.training - Example #4
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:05:46,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'L@@', 'ei@@', 'de', 'zeig@@', 't', 'man', 'eine', 'ra@@', 'pi@@', 'de', 'zeig@@', 't', 'man', 'eine', 'ra@@', 'pi@@', 'de', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'al@@', 't', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 15:05:46,074 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:05:46,074 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:05:46,074 - INFO - joeynmt.training - 	Hypothesis: Das nächste Leide zeigt man eine rapide zeigt man eine rapide von der letzten 25 Jahre alt passiert ist.
2024-05-23 15:06:02,923 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.956124, Batch Acc: 0.430781, Tokens per Sec:     4522, Lr: 0.000300
2024-05-23 15:06:19,744 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     2.069097, Batch Acc: 0.433780, Tokens per Sec:     4519, Lr: 0.000300
2024-05-23 15:06:36,407 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.930092, Batch Acc: 0.434621, Tokens per Sec:     4729, Lr: 0.000300
2024-05-23 15:06:53,292 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.827200, Batch Acc: 0.436396, Tokens per Sec:     4556, Lr: 0.000300
2024-05-23 15:07:09,878 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     1.862427, Batch Acc: 0.437755, Tokens per Sec:     4739, Lr: 0.000300
2024-05-23 15:07:09,879 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:07:09,880 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:08:20,166 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.90, acc:   0.43, generation: 70.2768[sec], evaluation: 0.0000[sec]
2024-05-23 15:08:20,167 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:08:20,383 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/6000.ckpt
2024-05-23 15:08:20,427 - INFO - joeynmt.training - Example #0
2024-05-23 15:08:20,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:08:20,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:08:20,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'tr@@', 'at@@', 'z@@', 'e,', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'so', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'eu@@', 'ten', 'der', 'L@@', 'eu@@', 'ten', 'der', 'L@@', 'eu@@', 'ten', '4@@', '8', 'St@@', 'at@@', 't@@', 'et@@', 'e,', 'die', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'eu@@', 'ten', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'eu@@', 'ten', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'eu@@', 'ten', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'eu@@', 'ten', '4@@', '0', 'Proz@@', 'ent', 'der', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'von']
2024-05-23 15:08:20,427 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:08:20,427 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:08:20,427 - INFO - joeynmt.training - 	Hypothesis: Letzten Jahr habe ich diese zwei Stratze, dass die Künstler so dass die Künstler der letzten drei Millionen Jahre der letzten drei Millionen Jahre der Leuten der Leuten der Leuten 48 Stattete, die von 40 Prozent der Leuten 40 Prozent der Leuten 40 Prozent der Leuten von 40 Prozent der Leuten 40 Prozent der Künstler von
2024-05-23 15:08:20,427 - INFO - joeynmt.training - Example #1
2024-05-23 15:08:20,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:08:20,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'St@@', 'at@@', 't@@', 'des@@', 'sen', 'die', 'Ver@@', 'l@@', 'eit@@', 'ung', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'der', 'T@@', 'eil@@', 'chen', 'nicht', 'die', 'D@@', 'rit@@', 'te@@', 'l', 'der', 'Gr@@', 'und@@', 's@@', 'ä@@', 'tz@@', 'e.', '</s>']
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Hypothesis: Aber diese Stattdessen die Verleitung dieser besonders Problem der Teilchen nicht die Drittel der Grundsätze.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - Example #2
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'der', 'S@@', 'in@@', 'n', 'ist', 'der', 'S@@', 'in@@', 'n', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist der Sinn ist der Sinn der globalen Klimawandel der globalen System.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - Example #3
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'sich', 'in', 'F@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'z', 'in', 'S@@', 'omm@@', '.', '</s>']
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - 	Hypothesis: Es erwartete sich in Finter und Kontragenz in Somm.
2024-05-23 15:08:20,428 - INFO - joeynmt.training - Example #4
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:08:20,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'ul@@', 't@@', 'ä@@', 't@@', 'ze', 'ich', 'Ihnen', 'ein', 'ra@@', 'p@@', 'i@@', 'd', 'zu', 'zeig@@', 'en,', 'was', 'passi@@', 'ert', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 15:08:20,429 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:08:20,429 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:08:20,429 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schultätze ich Ihnen ein rapid zu zeigen, was passiert passiert ist.
2024-05-23 15:08:37,194 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     2.046288, Batch Acc: 0.436747, Tokens per Sec:     4382, Lr: 0.000300
2024-05-23 15:08:54,215 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     1.903319, Batch Acc: 0.439592, Tokens per Sec:     4626, Lr: 0.000300
2024-05-23 15:09:11,150 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     1.894703, Batch Acc: 0.437030, Tokens per Sec:     4512, Lr: 0.000300
2024-05-23 15:09:28,651 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     2.069361, Batch Acc: 0.439803, Tokens per Sec:     4449, Lr: 0.000300
2024-05-23 15:09:47,060 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     1.957261, Batch Acc: 0.440358, Tokens per Sec:     4165, Lr: 0.000300
2024-05-23 15:09:47,061 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:09:47,061 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:10:57,242 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.75, acc:   0.44, generation: 70.1672[sec], evaluation: 0.0000[sec]
2024-05-23 15:10:57,246 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:10:57,609 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/6500.ckpt
2024-05-23 15:10:57,700 - INFO - joeynmt.training - Example #0
2024-05-23 15:10:57,700 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:10:57,700 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:10:57,700 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'z@@', 'en@@', 'e', 'so', 'dass', 'die', 'Ar@@', 'z@@', 't@@', 'st@@', 'ad@@', 'en,', 'dass', 'die', 'Ar@@', 'z@@', 't@@', 'sch@@', 'rit@@', 'te', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'die', 'die', 'L@@', 'ie@@', 'g@@', 'e,', 'die', 'für', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'von', '4@@', '8', 'St@@', 'at@@', 't@@', 'et@@', 'e.', '</s>']
2024-05-23 15:10:57,701 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:10:57,701 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:10:57,701 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Slizene so dass die Arztstaden, dass die Arztschritte der letzten drei Millionen Jahren die die Liege, die für die Größe der Größe von 48 Stattete.
2024-05-23 15:10:57,701 - INFO - joeynmt.training - Example #1
2024-05-23 15:10:57,701 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:10:57,701 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:10:57,701 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'S@@', 'er@@', 'ie', 'der', 'S@@', 'er@@', 'ie', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'zu', 'be@@', 'fin@@', 'den,', 'weil', 'es', 'nicht', 'die', 'D@@', 'rit@@', 'te@@', 'l', 'der', 'Gr@@', 'en@@', 'zen', 'des', 'E@@', 'is@@', 's', 'des', 'S@@', 'ach@@', 'e.', '</s>']
2024-05-23 15:10:57,701 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:10:57,701 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:10:57,701 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Serie der Serie dieser besonders Problem zu befinden, weil es nicht die Drittel der Grenzen des Eiss des Sache.
2024-05-23 15:10:57,701 - INFO - joeynmt.training - Example #2
2024-05-23 15:10:57,701 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:10:57,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:10:57,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einer', 'S@@', 'in@@', 'ne', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', ',', 'der', 'Her@@', 'z@@', 'in@@', 'k@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 15:10:57,702 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:10:57,702 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:10:57,702 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist in einer Sinne der globalen Klimawandel, der Herzinklimawandel.
2024-05-23 15:10:57,702 - INFO - joeynmt.training - Example #3
2024-05-23 15:10:57,702 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:10:57,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:10:57,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'in', 'F@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'zu', 'er@@', 'war@@', 'en.', '</s>']
2024-05-23 15:10:57,702 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:10:57,702 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:10:57,702 - INFO - joeynmt.training - 	Hypothesis: Es erwart in Finter und Kontracten in Sommer Sommer zu erwaren.
2024-05-23 15:10:57,702 - INFO - joeynmt.training - Example #4
2024-05-23 15:10:57,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:10:57,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:10:57,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'li@@', 'z@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'j@@', 'äh@@', 'ri@@', 'ge', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'gesch@@', 'aff@@', 't.', '</s>']
2024-05-23 15:10:57,703 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:10:57,703 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:10:57,703 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schlize, dass ich Ihnen ein Fast-Fast-Fast-Fast-Fast-Fast-jährige der letzten 25 Jahren geschafft.
2024-05-23 15:11:03,749 - INFO - joeynmt.training - Epoch   2: total training loss 9178.22
2024-05-23 15:11:03,750 - INFO - joeynmt.training - EPOCH 3
2024-05-23 15:11:21,187 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.748653, Batch Acc: 0.457233, Tokens per Sec:     3283, Lr: 0.000300
2024-05-23 15:11:41,642 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.829390, Batch Acc: 0.459241, Tokens per Sec:     3728, Lr: 0.000300
2024-05-23 15:12:02,309 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.835448, Batch Acc: 0.457792, Tokens per Sec:     3815, Lr: 0.000300
2024-05-23 15:12:23,383 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.715582, Batch Acc: 0.457473, Tokens per Sec:     3653, Lr: 0.000300
2024-05-23 15:12:44,267 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.707021, Batch Acc: 0.456605, Tokens per Sec:     3691, Lr: 0.000300
2024-05-23 15:12:44,268 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:12:44,268 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:14:19,327 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.59, acc:   0.44, generation: 95.0430[sec], evaluation: 0.0000[sec]
2024-05-23 15:14:19,331 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:14:19,744 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/7000.ckpt
2024-05-23 15:14:19,819 - INFO - joeynmt.training - Example #0
2024-05-23 15:14:19,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:14:19,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:14:19,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'L@@', 'ei@@', 'st@@', 'ung', 'so', 'dass', 'die', 'Ar@@', 'z@@', 't@@', 'ische', 'S@@', 'on@@', 'ne', 'des', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'ver@@', 'k@@', 'n@@', 'ü@@', 'pf@@', 'te', 'drei', 'Millionen', 'Jahren', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'ie@@', 'der', 'der', 'L@@', 'o@@', 's@@', 's@@', 's@@', 'ei@@', 'te', 'der', 'L@@', 'o@@', 's@@', 's@@', 's@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'o@@', 's@@', 's@@', 'hr@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'o@@', 's@@', 'hr@@', ',', 'die', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'zu', 'be@@', 'komm@@', 'en.', '</s>']
2024-05-23 15:14:19,819 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:14:19,819 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:14:19,819 - INFO - joeynmt.training - 	Hypothesis: Letzten Jahr habe ich diese zwei Leistung so dass die Arztische Sonne des letzten drei Millionen Jahren verknüpfte drei Millionen Jahren die Größe der Lieder der Lossseite der Losssunk von 40 Prozent der Losshrunk von 40 Prozent der Loshr, die die Künstler zu bekommen.
2024-05-23 15:14:19,819 - INFO - joeynmt.training - Example #1
2024-05-23 15:14:19,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:14:19,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:14:19,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 'en', 'die', 'Er@@', 'st@@', 'ung', 'dieser', 'bes@@', 'onder@@', 's', 'Problem@@', 'e', 'dieser', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'k@@', 'einen', 'D@@', 'u', 'zeig@@', 'en.', '</s>']
2024-05-23 15:14:19,820 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:14:19,820 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:14:19,820 - INFO - joeynmt.training - 	Hypothesis: Aber das verstehen die Erstung dieser besonders Probleme dieser besonders Problem, weil es nicht die Dickkeinen Du zeigen.
2024-05-23 15:14:19,820 - INFO - joeynmt.training - Example #2
2024-05-23 15:14:19,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:14:19,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:14:19,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'ein', 'S@@', 'inn@@', ',', 'der', 'der', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist', 'in', 'einer', 'S@@', 'in@@', 'n', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist', 'ist', 'das', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@']
2024-05-23 15:14:19,820 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:14:19,820 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:14:19,820 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist ein Sinn, der der der globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des Klimawandel der globalen Klimawandel des Klimawandel des Klimawandel des Klimawandel ist in einer Sinn der globalen Klimawandel ist ist das Klimawandel des Klima
2024-05-23 15:14:19,820 - INFO - joeynmt.training - Example #3
2024-05-23 15:14:19,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:14:19,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:14:19,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'z', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 15:14:19,821 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:14:19,821 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:14:19,821 - INFO - joeynmt.training - 	Hypothesis: Es erwartete sich in Winter und Kontragenz in Sommer und Kontragen.
2024-05-23 15:14:19,821 - INFO - joeynmt.training - Example #4
2024-05-23 15:14:19,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:14:19,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:14:19,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'au@@', 't@@', 'et@@', 'e,', 'dass', 'ich', 'Ihnen', 'eine', 'R@@', 'ei@@', 'he', 'von', 'dem', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 15:14:19,821 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:14:19,821 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:14:19,821 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schlautete, dass ich Ihnen eine Reihe von dem letzten 25 Jahren passiert ist.
2024-05-23 15:14:44,964 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.763692, Batch Acc: 0.459762, Tokens per Sec:     2978, Lr: 0.000300
2024-05-23 15:15:09,652 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.791119, Batch Acc: 0.460975, Tokens per Sec:     3038, Lr: 0.000300
2024-05-23 15:15:31,357 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.891338, Batch Acc: 0.455166, Tokens per Sec:     3482, Lr: 0.000300
2024-05-23 15:15:52,594 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.860885, Batch Acc: 0.460313, Tokens per Sec:     3610, Lr: 0.000300
2024-05-23 15:16:14,192 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.815357, Batch Acc: 0.466655, Tokens per Sec:     3628, Lr: 0.000300
2024-05-23 15:16:14,194 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:16:14,194 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:17:40,918 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.50, acc:   0.45, generation: 86.7101[sec], evaluation: 0.0000[sec]
2024-05-23 15:17:40,920 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:17:41,236 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/7500.ckpt
2024-05-23 15:17:41,345 - INFO - joeynmt.training - Example #0
2024-05-23 15:17:41,346 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:17:41,346 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:17:41,346 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'j@@', 'äh@@', 'ri@@', 'gen', 'ich', 'diese', 'zwei', 'S@@', 'ol@@', 'li@@', 'de', 'so', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'S@@', 'on@@', 'ne', 'des', 'letz@@', 'ten', 'T@@', 'ro@@', 'ch@@', 'en@@', 'st@@', 'a@@', 'at@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', '4@@', '8', 'St@@', 'at@@', 'en,', 'die', 'die', 'L@@', 'o@@', 'ch@@', 'en@@', 'e', 'der', 'L@@', 'o@@', 'ch@@', 'en@@', 'e', 'der', 'L@@', 'o@@', 'ch@@', 'i@@', 'en', 'ges@@', 'tell@@', 't.', '</s>']
2024-05-23 15:17:41,346 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:17:41,346 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:17:41,346 - INFO - joeynmt.training - 	Hypothesis: Letzten jährigen ich diese zwei Sollide so dass die arktische Sonne des letzten Trochenstaat, die für die meisten 48 Staten, die die Lochene der Lochene der Lochien gestellt.
2024-05-23 15:17:41,346 - INFO - joeynmt.training - Example #1
2024-05-23 15:17:41,346 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:17:41,346 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:17:41,346 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'versteh@@', 'en', 'die', 'S@@', 'er@@', 'ie', 'dieser', 'T@@', 'eil@@', 'chen', 'dieser', 'T@@', 'eil@@', 'ch@@', 'en@@', 'b@@', 'il@@', 'den', 'dieses', 'T@@', 'eil@@', 'ch@@', 'en@@', 'f@@', 'es@@', 't,', 'weil', 'es', 'nicht', 'der', 'E@@', 'is@@', 'se', 'zeig@@', 't.', '</s>']
2024-05-23 15:17:41,347 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:17:41,347 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:17:41,347 - INFO - joeynmt.training - 	Hypothesis: Aber diese verstehen die Serie dieser Teilchen dieser Teilchenbilden dieses Teilchenfest, weil es nicht der Eisse zeigt.
2024-05-23 15:17:41,347 - INFO - joeynmt.training - Example #2
2024-05-23 15:17:41,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:17:41,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:17:41,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'der', 'S@@', 'inn@@', 'e,', 'in', 'einem', 'S@@', 'inn@@', ',', 'der', 'der', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 15:17:41,347 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:17:41,347 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:17:41,347 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist der Sinne, in einem Sinn, der der der globalen Klimawandel.
2024-05-23 15:17:41,347 - INFO - joeynmt.training - Example #3
2024-05-23 15:17:41,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:17:41,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:17:41,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'ten', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en,', 'die', 'S@@', 'omm@@', 'er', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'zu', 'er@@', 'hal@@', 'ten.', '</s>']
2024-05-23 15:17:41,348 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:17:41,348 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:17:41,348 - INFO - joeynmt.training - 	Hypothesis: Es erwarten sich in Winter und Kontragen, die Sommer in Sommer Sommer zu erhalten.
2024-05-23 15:17:41,348 - INFO - joeynmt.training - Example #4
2024-05-23 15:17:41,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:17:41,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:17:41,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'w@@', 'ach@@', 'en', 'zeig@@', 'en,', 'dass', 'man', 'ein', 'R@@', 'ei@@', 'he', 'zeig@@', 't', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahr@@', 'en.', '</s>']
2024-05-23 15:17:41,348 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:17:41,348 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:17:41,348 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schwachen zeigen, dass man ein Reihe zeigt über die letzten 25 Jahren.
2024-05-23 15:18:04,876 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.758088, Batch Acc: 0.461876, Tokens per Sec:     3281, Lr: 0.000300
2024-05-23 15:18:26,277 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.906222, Batch Acc: 0.465719, Tokens per Sec:     3638, Lr: 0.000300
2024-05-23 15:18:47,870 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.778745, Batch Acc: 0.466421, Tokens per Sec:     3553, Lr: 0.000300
2024-05-23 15:19:09,066 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.798258, Batch Acc: 0.463898, Tokens per Sec:     3696, Lr: 0.000300
2024-05-23 15:19:29,807 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.900275, Batch Acc: 0.465864, Tokens per Sec:     3579, Lr: 0.000300
2024-05-23 15:19:29,808 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:19:29,808 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:20:55,541 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.40, acc:   0.45, generation: 85.7197[sec], evaluation: 0.0000[sec]
2024-05-23 15:20:55,543 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:20:55,910 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/8000.ckpt
2024-05-23 15:20:55,960 - INFO - joeynmt.training - Example #0
2024-05-23 15:20:55,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:20:55,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:20:55,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'S@@', 'ei@@', 'te', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'die', 'k@@', 'ün@@', 'st@@', 'ig@@', 'sten', 'drei', 'Millionen', 'Jahren', 'die', 'k@@', 'ün@@', 'st@@', 'ig@@', 'sten', 'drei', 'Millionen', 'Jahren', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'L@@', 'ei@@', 'st@@', 'ung@@', 'en,', 'die', '4@@', '8', 'St@@', 'at@@', 'en,', 'die', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:20:55,961 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:20:55,961 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:20:55,961 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Seite ich diese zwei Slide so dass die Künstler die künstigsten drei Millionen Jahren die künstigsten drei Millionen Jahren die Leistung der Leistungen, die 48 Staten, die von 40 Prozent.
2024-05-23 15:20:55,961 - INFO - joeynmt.training - Example #1
2024-05-23 15:20:55,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:20:55,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:20:55,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 'en', 'die', 'Er@@', 'n@@', 'ä@@', 'hr@@', 'ung', 'dieser', 'T@@', 'eil@@', 'ch@@', 'en@@', 'f@@', 'ä@@', 'hr@@', 't', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'ig@@', 'enschaf@@', 't.', '</s>']
2024-05-23 15:20:55,961 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:20:55,961 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:20:55,961 - INFO - joeynmt.training - 	Hypothesis: Aber das verstehen die Ernährung dieser Teilchenfährt Problem weil es nicht die Dicksal der Eigenschaft.
2024-05-23 15:20:55,961 - INFO - joeynmt.training - Example #2
2024-05-23 15:20:55,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:20:55,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:20:55,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'der', 'S@@', 'in@@', 'ne', 'der', 'glob@@', 'ale', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 15:20:55,962 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:20:55,962 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:20:55,962 - INFO - joeynmt.training - 	Hypothesis: Die künstler Künstler ist der Sinne der globale Klimawandel.
2024-05-23 15:20:55,962 - INFO - joeynmt.training - Example #3
2024-05-23 15:20:55,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:20:55,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:20:55,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'S@@', 'omm@@', 'er@@', '.', '</s>']
2024-05-23 15:20:55,962 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:20:55,962 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:20:55,962 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontrennt in Sommer.
2024-05-23 15:20:55,962 - INFO - joeynmt.training - Example #4
2024-05-23 15:20:55,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:20:55,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:20:55,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'au@@', 't', 'ich', 'Ihnen', 'ein', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'f@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'f@@', 'ik@@', '-@@', 'K@@', 'ra@@', 'f@@', 'ik@@', '.', '</s>']
2024-05-23 15:20:55,963 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:20:55,963 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:20:55,963 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schlaut ich Ihnen ein Fast-Fast-Krafast-Krafik-Krafik.
2024-05-23 15:21:17,646 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.914744, Batch Acc: 0.464706, Tokens per Sec:     3547, Lr: 0.000300
2024-05-23 15:21:39,837 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.745993, Batch Acc: 0.464671, Tokens per Sec:     3369, Lr: 0.000300
2024-05-23 15:22:00,966 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.662229, Batch Acc: 0.463602, Tokens per Sec:     3716, Lr: 0.000300
2024-05-23 15:22:23,010 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.860710, Batch Acc: 0.466196, Tokens per Sec:     3507, Lr: 0.000300
2024-05-23 15:22:45,531 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.761766, Batch Acc: 0.462819, Tokens per Sec:     3380, Lr: 0.000300
2024-05-23 15:22:45,531 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:22:45,531 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:23:54,069 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.27, acc:   0.46, generation: 68.5289[sec], evaluation: 0.0000[sec]
2024-05-23 15:23:54,071 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:23:54,294 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/8500.ckpt
2024-05-23 15:23:54,322 - INFO - joeynmt.training - Example #0
2024-05-23 15:23:54,322 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:23:54,322 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:23:54,322 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'ich', 'diesen', 'zwei', 'S@@', 'li@@', 'ma@@', 'g', 'ge@@', 'zeig@@', 't', 'hab@@', 'e,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'is@@', 'ier@@', 'ung', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahren', 'die', 'S@@', 'ei@@', 'te', 'des', 'L@@', 'eu@@', 'ten', '4@@', '8', 'St@@', 'at@@', 'en,', 'die', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:23:54,322 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:23:54,322 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:23:54,322 - INFO - joeynmt.training - 	Hypothesis: Letzten ich diesen zwei Slimag gezeigt habe, dass die arktischen Eisisierung der letzten drei Millionen Jahren die meisten drei Millionen Jahren die Seite des Leuten 48 Staten, die von 40 Prozent.
2024-05-23 15:23:54,322 - INFO - joeynmt.training - Example #1
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'S@@', 'er@@', 'i@@', 'en@@', 'en@@', '-@@', 'Pro@@', 'blem', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die Serienen-Problem dieser besonders Problem zu zeigen, weil es nicht die Dicksal des Eiss des Eiss des Eiss des Eiss des Eiss.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - Example #2
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'ein', 'S@@', 'inn@@', ',', 'der', 'der', 'G@@', 'lob@@', 'al@@', 'es', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist ein Sinn, der der Globales System.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - Example #3
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'mer@@', '.', '</s>']
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - 	Hypothesis: Es wart sich in Winter und Kontrennt in Sommer-Sommer-Sommer-Sommer-Sommmer.
2024-05-23 15:23:54,323 - INFO - joeynmt.training - Example #4
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:23:54,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'w@@', 'es@@', 'entlich', 'wird', 'ein', 'R@@', 'ei@@', 'ch', 'ein', 'R@@', 'ei@@', 'ch', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'lan@@', 'g', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 15:23:54,324 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:23:54,324 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:23:54,324 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schwesentlich wird ein Reich ein Reich der letzten 25 Jahre lang passiert.
2024-05-23 15:24:11,191 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.698958, Batch Acc: 0.471820, Tokens per Sec:     4382, Lr: 0.000300
2024-05-23 15:24:28,437 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.751022, Batch Acc: 0.467522, Tokens per Sec:     4477, Lr: 0.000300
2024-05-23 15:24:44,131 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.787549, Batch Acc: 0.467749, Tokens per Sec:     4896, Lr: 0.000300
2024-05-23 15:25:00,325 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.837210, Batch Acc: 0.471141, Tokens per Sec:     4741, Lr: 0.000300
2024-05-23 15:25:16,433 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     2.059532, Batch Acc: 0.467501, Tokens per Sec:     4684, Lr: 0.000300
2024-05-23 15:25:16,434 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:25:16,434 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:26:26,508 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.19, acc:   0.46, generation: 70.0653[sec], evaluation: 0.0000[sec]
2024-05-23 15:26:26,509 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:26:26,733 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/9000.ckpt
2024-05-23 15:26:26,773 - INFO - joeynmt.training - Example #0
2024-05-23 15:26:26,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:26:26,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:26:26,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'so', 'dass', 'die', 'Ar@@', 'k@@', 't@@', 'sch@@', 'rit@@', 't', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'o@@', 'ft', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:26:26,773 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:26:26,773 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Slide so dass die Künstler so dass die Arktschritt für die meisten drei Millionen Jahren hat die meisten drei Millionen Jahren hat die Leistung von 40 Prozent der Loft von 40 Prozent.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - Example #1
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'änder@@', 'ungen', 'der', 'T@@', 'eil@@', 'chen', 'dieser', 'bes@@', 'tim@@', 'm@@', 'te', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'ier@@', 't.', '</s>']
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Hypothesis: Aber diese Veränderungen der Teilchen dieser bestimmte Problem weil es nicht das Dicksal nicht das Dicksal des Eisiert.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - Example #2
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist in einem Sinn ist in einem Sinn der globalen Klimawandel.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - Example #3
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:26:26,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'st@@', 'er.', '</s>']
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontragenster.
2024-05-23 15:26:26,774 - INFO - joeynmt.training - Example #4
2024-05-23 15:26:26,775 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:26:26,775 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:26:26,775 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'w@@', 'es@@', 'en@@', 'e', 'ich', 'Ihnen', 'ein', 'St@@', 'ück', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'Jahre', 'passi@@', 'er@@', 'te', 'ist,', 'dass', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 15:26:26,775 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:26:26,775 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:26:26,775 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schwesene ich Ihnen ein Stück schneller Fast-Fast-Fast-Jahre passierte ist, dass über die letzten 25 Jahren passiert.
2024-05-23 15:26:43,547 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.765180, Batch Acc: 0.468136, Tokens per Sec:     4442, Lr: 0.000300
2024-05-23 15:26:59,557 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.720586, Batch Acc: 0.475807, Tokens per Sec:     4762, Lr: 0.000300
2024-05-23 15:27:15,465 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     2.010342, Batch Acc: 0.470660, Tokens per Sec:     4737, Lr: 0.000300
2024-05-23 15:27:32,106 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.879985, Batch Acc: 0.468783, Tokens per Sec:     4509, Lr: 0.000300
2024-05-23 15:27:48,517 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     1.825651, Batch Acc: 0.469327, Tokens per Sec:     4679, Lr: 0.000300
2024-05-23 15:27:48,518 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:27:48,518 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:29:03,027 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.11, acc:   0.46, generation: 74.4964[sec], evaluation: 0.0000[sec]
2024-05-23 15:29:03,029 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:29:03,301 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/9500.ckpt
2024-05-23 15:29:03,375 - INFO - joeynmt.training - Example #0
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'j@@', 'äh@@', 'ri@@', 'gen', 'ich', 'diese', 'bei@@', 'den', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is', 'der', 'k@@', 'ün@@', 'st@@', 'lichen', 'E@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'von', '4@@', '8', 'St@@', 'at@@', 'en,', 'hat', 'S@@', 'hr@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent', '</s>']
2024-05-23 15:29:03,376 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:29:03,376 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:29:03,376 - INFO - joeynmt.training - 	Hypothesis: Letzten jährigen ich diese beiden Slide so dass die künstliche Eis der künstlichen Eis der letzten drei Millionen Jahren hat die Größe der Größe der Größe von 48 Staten, hat Shrunk von 40 Prozent
2024-05-23 15:29:03,376 - INFO - joeynmt.training - Example #1
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 'en', 'die', 'R@@', 'ei@@', 'se', 'dieser', 'T@@', 'eil@@', 'chen', 'dieser', 'T@@', 'eil@@', 'ch@@', 'en@@', 'f@@', 'ä@@', 'll@@', 't,', 'weil', 'es', 'nicht', 'die', 'D@@', 'is@@', 'k@@', 'us@@', 's@@', 'ch', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'ig@@', 'enschaf@@', 't', 'ist.', '</s>']
2024-05-23 15:29:03,376 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:29:03,376 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:29:03,376 - INFO - joeynmt.training - 	Hypothesis: Aber das verstehen die Reise dieser Teilchen dieser Teilchenfällt, weil es nicht die Diskussch der Eis des Eis des Eigenschaft ist.
2024-05-23 15:29:03,376 - INFO - joeynmt.training - Example #2
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:29:03,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'der', 'glob@@', 'ale', 'K@@', 'li@@', 'ma@@', 'w@@', 'es@@', 'en@@', 'system@@', '.', '</s>']
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Künstler ist in einem Sinn der globale Klimawesensystem.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - Example #3
2024-05-23 15:29:03,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:29:03,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:29:03,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontragen.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - Example #4
2024-05-23 15:29:03,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:29:03,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:29:03,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'w@@', 'es@@', 'en@@', 'e', 'ich', 'Ihnen', 'eine', 'R@@', 'ei@@', 'he', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'lan@@', 'g', 'passi@@', 'ert', 'ist,', 'was', 'über', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:29:03,377 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schwesene ich Ihnen eine Reihe von der letzten 25 Jahre lang passiert ist, was über den letzten 25 Jahren passiert.
2024-05-23 15:29:19,715 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.738090, Batch Acc: 0.474647, Tokens per Sec:     4613, Lr: 0.000300
2024-05-23 15:29:37,016 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.683891, Batch Acc: 0.472464, Tokens per Sec:     4508, Lr: 0.000300
2024-05-23 15:29:54,867 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.734092, Batch Acc: 0.472598, Tokens per Sec:     4274, Lr: 0.000300
2024-05-23 15:30:12,972 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.775429, Batch Acc: 0.475045, Tokens per Sec:     4334, Lr: 0.000300
2024-05-23 15:30:30,527 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.898037, Batch Acc: 0.476906, Tokens per Sec:     4486, Lr: 0.000300
2024-05-23 15:30:30,527 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:30:30,527 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:31:38,147 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.00, acc:   0.47, generation: 67.6094[sec], evaluation: 0.0000[sec]
2024-05-23 15:31:38,149 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:31:38,408 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/10000.ckpt
2024-05-23 15:31:38,437 - INFO - joeynmt.training - Example #0
2024-05-23 15:31:38,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:31:38,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:31:38,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'L@@', 'ö@@', 's@@', 'er', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'die', 'k@@', 'ün@@', 'st@@', 'ig@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', '4@@', '8', 'St@@', 'at@@', 'en,', 'hat', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Löser so dass die künstler die künstigsten drei Millionen Jahre die meisten drei Millionen Jahre die Größe der Größe der Größe 48 Staten, hat 40 Prozent.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - Example #1
2024-05-23 15:31:38,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:31:38,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:31:38,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'St@@', 'at@@', 't@@', 'eil', 'dieser', 'bes@@', 'onder@@', 'e', 'An@@', 'g@@', 'st', 'dieser', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'en@@', 'heit', 'zeig@@', 't.', '</s>']
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Hypothesis: Aber diese Statteil dieser besondere Angst dieser besondere Problem ist, weil es nicht die Dicksal der Eisenheit zeigt.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - Example #2
2024-05-23 15:31:38,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:31:38,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:31:38,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'ar@@', 'ti@@', 'k@@', 'er', 'ist', 'die', 'K@@', 'li@@', 'ma@@', 'w@@', 'ach@@', 'sen@@', 'e', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eiskartiker ist die Klimawachsene der globalen Klimawandel.
2024-05-23 15:31:38,438 - INFO - joeynmt.training - Example #3
2024-05-23 15:31:38,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:31:38,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:31:38,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 15:31:38,439 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:31:38,439 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:31:38,439 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontragen.
2024-05-23 15:31:38,439 - INFO - joeynmt.training - Example #4
2024-05-23 15:31:38,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:31:38,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:31:38,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'mer@@', 'z', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'eine', 'schn@@', 'ell@@', '-@@', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'p@@', 'i@@', 'd@@', '-@@', 'K@@', 'ra@@', 'p@@', 'p@@', 'u@@', 'li@@', 'er@@', 't.', '</s>']
2024-05-23 15:31:38,439 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:31:38,439 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:31:38,439 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schmerz zeige, dass ich Ihnen eine schnell-Fast-Krapid-Krappuliert.
2024-05-23 15:31:56,962 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.971128, Batch Acc: 0.474837, Tokens per Sec:     4116, Lr: 0.000300
2024-05-23 15:32:14,189 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.813320, Batch Acc: 0.476216, Tokens per Sec:     4508, Lr: 0.000300
2024-05-23 15:32:32,666 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.773081, Batch Acc: 0.477372, Tokens per Sec:     4195, Lr: 0.000300
2024-05-23 15:32:50,899 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.693093, Batch Acc: 0.475869, Tokens per Sec:     4289, Lr: 0.000300
2024-05-23 15:33:08,299 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.711218, Batch Acc: 0.478007, Tokens per Sec:     4402, Lr: 0.000300
2024-05-23 15:33:08,300 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:33:08,300 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:34:23,104 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.93, acc:   0.47, generation: 74.7942[sec], evaluation: 0.0000[sec]
2024-05-23 15:34:23,106 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:34:23,349 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/10500.ckpt
2024-05-23 15:34:23,423 - INFO - joeynmt.training - Example #0
2024-05-23 15:34:23,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:34:23,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:34:23,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'j@@', 'äh@@', 'ri@@', 'ge', 'ich', 'diese', 'bei@@', 'den', 'S@@', 'li@@', 'z@@', 'en@@', 'e', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'ig@@', 'sten', 'drei', 'Millionen', 'Jahre', 'lan@@', 'g', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'o@@', 's@@', 'sen', 'von', '4@@', '8', 'St@@', 'at@@', 'en,', 'hat', 'sich', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'S@@', 'hr@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'S@@', 'hr@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent', '</s>']
2024-05-23 15:34:23,423 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:34:23,423 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:34:23,423 - INFO - joeynmt.training - 	Hypothesis: Letzten jährige ich diese beiden Slizene so dass die künstigsten drei Millionen Jahre lang der letzten drei Millionen Jahre der letzten drei Millionen Jahre der Lossen von 48 Staten, hat sich von 40 Prozent der Shrunk von 40 Prozent der Shrunk von 40 Prozent
2024-05-23 15:34:23,423 - INFO - joeynmt.training - Example #1
2024-05-23 15:34:23,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:34:23,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:34:23,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 'en', 'die', 'Er@@', 'n@@', 'en@@', 'heit', 'dieser', 'bes@@', 'onder@@', 's', 'dieses', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'die', 'D@@', 'ick@@', 's@@', 'weise', 'nicht', 'zeig@@', 't.', '</s>']
2024-05-23 15:34:23,423 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:34:23,423 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:34:23,423 - INFO - joeynmt.training - 	Hypothesis: Aber das verstehen die Ernenheit dieser besonders dieses besonders Problem zu zeigen, weil es die Dicksweise nicht zeigt.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - Example #2
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'in', 'einem', 'S@@', 'in@@', 'n', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'st@@', 'st@@', 'eig@@', 't.', '</s>']
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eis ist in einem Sinn, in einem Sinn der globalen Klimakststeigt.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - Example #3
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'in', 'F@@', 'en@@', 'ster', 'und', 'Kon@@', 'tr@@', 'ag@@', 'e,', '</s>']
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Hypothesis: Es erwart in Fenster und Kontrage,
2024-05-23 15:34:23,424 - INFO - joeynmt.training - Example #4
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:34:23,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Leb@@', 'ens@@', 'z@@', 'eich@@', 'e', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'ch', 'sein', 'wir@@', 'd,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'gesch@@', 'i@@', 'eh@@', 't.', '</s>']
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:34:23,424 - INFO - joeynmt.training - 	Hypothesis: Die nächste Lebenszeiche ich Ihnen ein Reich sein wird, was über die letzten 25 Jahre geschieht.
2024-05-23 15:34:39,766 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     1.715063, Batch Acc: 0.481932, Tokens per Sec:     4627, Lr: 0.000300
2024-05-23 15:34:57,993 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     2.111920, Batch Acc: 0.477502, Tokens per Sec:     4302, Lr: 0.000300
2024-05-23 15:35:16,143 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     1.777806, Batch Acc: 0.481561, Tokens per Sec:     4243, Lr: 0.000300
2024-05-23 15:35:33,522 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     1.906508, Batch Acc: 0.479063, Tokens per Sec:     4424, Lr: 0.000300
2024-05-23 15:35:51,579 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     1.654220, Batch Acc: 0.476646, Tokens per Sec:     4229, Lr: 0.000300
2024-05-23 15:35:51,580 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:35:51,580 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:37:01,064 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.86, acc:   0.47, generation: 69.4754[sec], evaluation: 0.0000[sec]
2024-05-23 15:37:01,067 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:37:01,354 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/11000.ckpt
2024-05-23 15:37:01,401 - INFO - joeynmt.training - Example #0
2024-05-23 15:37:01,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:37:01,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:37:01,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'L@@', 'in@@', 'den', 'so', 'dass', 'die', 'Ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'ver@@', 'mei@@', 'd@@', 'et,', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 's@@', 'ü@@', 'ch@@', 'er', 'die', 'Gr@@', 'öß@@', 'e', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:37:01,401 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:37:01,401 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:37:01,401 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Linden so dass die Arktische Eiskap, die für die meisten drei Millionen Jahre vermeidet, die für die meisten drei Millionen Jahre sücher die Größe von 40 Prozent.
2024-05-23 15:37:01,401 - INFO - joeynmt.training - Example #1
2024-05-23 15:37:01,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:37:01,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:37:01,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'St@@', 'at@@', 'en', 'die', 'Di@@', 'en@@', 'st@@', 'ra@@', 'ß@@', 'e', 'dieses', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's@@', 'sen', 'zeig@@', 'en.', '</s>']
2024-05-23 15:37:01,401 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:37:01,401 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:37:01,401 - INFO - joeynmt.training - 	Hypothesis: Aber diese Staten die Dienstraße dieses besonders Problem ist, weil es nicht die Dicksal des Eisssen zeigen.
2024-05-23 15:37:01,401 - INFO - joeynmt.training - Example #2
2024-05-23 15:37:01,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'a@@', 'p', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'er@@', '-@@', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eiskap ist in einem Sinne des globalen Klimaklimaer-System.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - Example #3
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'z', 'in', 'S@@', 'omm@@', 'er@@', '.', '</s>']
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontragenz in Sommer.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - Example #4
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:37:01,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'werde', 'ich', 'Ihnen', 'ein', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'des', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'gesch@@', 'aff@@', 'en', 'werden,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'gesch@@', 'aff@@', 'en.', '</s>']
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:37:01,402 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schluss werde ich Ihnen ein Fast-Kraft des letzten 25 Jahre geschaffen werden, was über die letzten 25 Jahre geschaffen.
2024-05-23 15:37:09,020 - INFO - joeynmt.training - Epoch   3: total training loss 8150.10
2024-05-23 15:37:09,021 - INFO - joeynmt.training - EPOCH 4
2024-05-23 15:37:18,382 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.677545, Batch Acc: 0.496636, Tokens per Sec:     4462, Lr: 0.000300
2024-05-23 15:37:34,957 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.462707, Batch Acc: 0.502579, Tokens per Sec:     4598, Lr: 0.000300
2024-05-23 15:37:52,096 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     1.556002, Batch Acc: 0.499393, Tokens per Sec:     4614, Lr: 0.000300
2024-05-23 15:38:08,580 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.599007, Batch Acc: 0.494164, Tokens per Sec:     4709, Lr: 0.000300
2024-05-23 15:38:26,342 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.707902, Batch Acc: 0.497294, Tokens per Sec:     4421, Lr: 0.000300
2024-05-23 15:38:26,342 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:38:26,342 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:39:50,004 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.83, acc:   0.48, generation: 83.6495[sec], evaluation: 0.0000[sec]
2024-05-23 15:39:50,009 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:39:50,304 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/11500.ckpt
2024-05-23 15:39:50,363 - INFO - joeynmt.training - Example #0
2024-05-23 15:39:50,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:39:50,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:39:50,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'n', 'ver@@', 'sch@@', 'w@@', 'andel@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'die', 'k@@', 'ün@@', 'st@@', 'ste', 'E@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahren', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'ie@@', 'den', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:39:50,363 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:39:50,363 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:39:50,363 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Slin verschwandelt, dass die künstler die künstste Eis der letzten drei Millionen Jahren die meisten drei Millionen Jahre der Lieden von 40 Prozent.
2024-05-23 15:39:50,363 - INFO - joeynmt.training - Example #1
2024-05-23 15:39:50,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'ern@@', 's@@', 'th@@', 'eit', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'des', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'ick@@', 's', 'nicht', 'die', 'D@@', 'ick@@', 'ick@@', 's@@', 's@@', 'al', 'der', 'E@@', 'is@@', 's@@', 'sen', 'zeig@@', 't.', '</s>']
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die ernstheit dieser besonders Problem des Problem weil es nicht die Dickicks nicht die Dickickssal der Eisssen zeigt.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - Example #2
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ti@@', 'c', 'E@@', 'is', 'ist', 'der', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist.', '</s>']
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Hypothesis: Der ktic Eis ist der Künstler ist in einem Sinne des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des Klimawandel des Klimawandel des Klimawandel des Klimawandel des Klimawandel ist.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - Example #3
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:39:50,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'in', 'W@@', 'in@@', 'ter', 'und', 'in', 'den', 'S@@', 'omm@@', 'er', 'zu', 'seh@@', 'en.', '</s>']
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - 	Hypothesis: Es erwart in Winter und in Winter und in den Sommer zu sehen.
2024-05-23 15:39:50,364 - INFO - joeynmt.training - Example #4
2024-05-23 15:39:50,365 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:39:50,365 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:39:50,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'w@@', 'es@@', 'en@@', 'e', 'ich', 'Ihnen', 'ein', 'R@@', 'pi@@', 'de', 'der', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 'te', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 15:39:50,365 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:39:50,365 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:39:50,365 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schwesene ich Ihnen ein Rpide der in den letzten 25 Jahren passierte über die letzten 25 Jahren passiert.
2024-05-23 15:40:07,599 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.674361, Batch Acc: 0.496457, Tokens per Sec:     4444, Lr: 0.000300
2024-05-23 15:40:26,749 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.645839, Batch Acc: 0.496670, Tokens per Sec:     4117, Lr: 0.000300
2024-05-23 15:40:43,522 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.730071, Batch Acc: 0.497175, Tokens per Sec:     4622, Lr: 0.000300
2024-05-23 15:41:01,807 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.570329, Batch Acc: 0.500569, Tokens per Sec:     4232, Lr: 0.000300
2024-05-23 15:41:18,357 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.687603, Batch Acc: 0.496840, Tokens per Sec:     4732, Lr: 0.000300
2024-05-23 15:41:18,358 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:41:18,358 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:42:46,309 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.75, acc:   0.48, generation: 87.9382[sec], evaluation: 0.0000[sec]
2024-05-23 15:42:46,314 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:42:46,655 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/12000.ckpt
2024-05-23 15:42:46,701 - INFO - joeynmt.training - Example #0
2024-05-23 15:42:46,701 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:42:46,701 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:42:46,701 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'z@@', 'ier@@', 't,', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'te', 'K@@', 'ap@@', ',', 'das', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'L@@', 'u@@', 'ft', 'der', 'L@@', 'o@@', 's@@', 'ar@@', 'ten', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'u@@', 'ft', 'von', '4@@', '0', 'Proz@@', 'ent', 'ges@@', 'tell@@', 't.', '</s>']
2024-05-23 15:42:46,702 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:42:46,702 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:42:46,702 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Sliziert, so dass die künstlerte Kap, das für die meisten drei Millionen Jahre war die letzten drei Millionen Jahre war die Luft der Losarten 40 Prozent der Luft von 40 Prozent gestellt.
2024-05-23 15:42:46,702 - INFO - joeynmt.training - Example #1
2024-05-23 15:42:46,702 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:42:46,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:42:46,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'T@@', 'eil@@', 'ch@@', 'en@@', '-@@', 'Pro@@', 'blem', 'nicht', 'das', 'D@@', 'is@@', 'k@@', 'us@@', 'si@@', 'er@@', 't,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'k@@', 'enn@@', 'ung', 'des', 'E@@', 'is@@', 'sen', 'zeig@@', 'en.', '</s>']
2024-05-23 15:42:46,702 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:42:46,702 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:42:46,702 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die ernsthaft dieses Teilchen-Problem nicht das Diskussiert, weil es nicht die Dickkennung des Eissen zeigen.
2024-05-23 15:42:46,702 - INFO - joeynmt.training - Example #2
2024-05-23 15:42:46,702 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:42:46,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:42:46,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'der', 'der', 'B@@', 'et@@', 'rach@@', 't@@', 'ung', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's@@', 'system@@', 's.', '</s>']
2024-05-23 15:42:46,703 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:42:46,703 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:42:46,703 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eis ist in einem Sinn, der der Betrachtung des globalen Klimawandelssystems.
2024-05-23 15:42:46,703 - INFO - joeynmt.training - Example #3
2024-05-23 15:42:46,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:42:46,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:42:46,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'in', 'W@@', 'in@@', 'ter', 'und', 'in', 'den', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'k@@', 'er', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'kt', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ak@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'kt', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'kt', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ak@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'kt', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@']
2024-05-23 15:42:46,703 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:42:46,703 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:42:46,703 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und in Winter und in den Sommer-Sommer-Winter und Kontragenker in Winter und Kontrackt in Winter und Kontrakten in Winter und Kontrackt in Winter und Kontrennt in Winter und Kontrackt in Winter und Kontrakten in Winter und Kontrackt in Winter und Kontr
2024-05-23 15:42:46,703 - INFO - joeynmt.training - Example #4
2024-05-23 15:42:46,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:42:46,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:42:46,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'werde', 'ich', 'Ihnen', 'einen', 'schn@@', 'ell@@', '-@@', 'F@@', 'ast@@', '-@@', 'vi@@', 'er@@', '-@@', 'K@@', 'op@@', 'i@@', 'di@@', 'er@@', 'te', 'von', 'dem', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 15:42:46,704 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:42:46,704 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:42:46,704 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schluss werde ich Ihnen einen schnell-Fast-vier-Kopidierte von dem letzten 25 Jahren passiert ist.
2024-05-23 15:43:05,857 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.644219, Batch Acc: 0.499897, Tokens per Sec:     3981, Lr: 0.000300
2024-05-23 15:43:22,585 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.639559, Batch Acc: 0.496350, Tokens per Sec:     4644, Lr: 0.000300
2024-05-23 15:43:40,025 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.801072, Batch Acc: 0.497858, Tokens per Sec:     4336, Lr: 0.000300
2024-05-23 15:43:56,228 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.519816, Batch Acc: 0.498552, Tokens per Sec:     4624, Lr: 0.000300
2024-05-23 15:44:13,381 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.584547, Batch Acc: 0.500593, Tokens per Sec:     4524, Lr: 0.000300
2024-05-23 15:44:13,382 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:44:13,382 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:45:34,001 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.71, acc:   0.48, generation: 80.6096[sec], evaluation: 0.0000[sec]
2024-05-23 15:45:34,004 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:45:34,365 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/12500.ckpt
2024-05-23 15:45:34,435 - INFO - joeynmt.training - Example #0
2024-05-23 15:45:34,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:45:34,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:45:34,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'z@@', 'en@@', 'e', 'so', 'dass', 'die', 'Ar@@', 'k@@', 'k@@', 'k@@', 'op@@', ',', 'dass', 'die', 'Ar@@', 'k@@', 'k@@', 'op@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'hat', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'L@@', 'ei@@', 'st@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'ei@@', 'st@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:45:34,435 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:45:34,435 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:45:34,436 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich diese zwei Slizene so dass die Arkkkop, dass die Arkkop, die für die meisten drei Millionen Jahre hat die Größe der Leistung der Leistunk von 40 Prozent der Leistunk von 40 Prozent.
2024-05-23 15:45:34,436 - INFO - joeynmt.training - Example #1
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'Er@@', 'n@@', 'sen@@', 's@@', 'ei@@', 'te', 'dieses', 'bes@@', 'tim@@', 'm@@', 't', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'weise', 'der', 'E@@', 'is@@', 'enschaf@@', 't', 'nicht', 'zeig@@', 'en.', '</s>']
2024-05-23 15:45:34,436 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:45:34,436 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:45:34,436 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die Ernsenseite dieses bestimmt Problem ist, weil es nicht die Dicksweise der Eisenschaft nicht zeigen.
2024-05-23 15:45:34,436 - INFO - joeynmt.training - Example #2
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'ra@@', 'f@@', 't,', 'in', 'einem', 'Ge@@', 'fü@@', 'h@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@']
2024-05-23 15:45:34,436 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:45:34,436 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:45:34,436 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eiskraft, in einem Gefühl des globalen Klimawandel des globalen Klimawandel des Klimawandel des Klimawandel des globalen Klimawandel des Klimawandel des Klimawandel des globalen Klimamawandel des globalen Klimawandel des globalen Klimawandel des globalen Kli
2024-05-23 15:45:34,436 - INFO - joeynmt.training - Example #3
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:45:34,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'k@@', 'en.', '</s>']
2024-05-23 15:45:34,437 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:45:34,437 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:45:34,437 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontracten in Sommer Sommer in Sommer Sommer und Kontragenken.
2024-05-23 15:45:34,437 - INFO - joeynmt.training - Example #4
2024-05-23 15:45:34,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:45:34,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:45:34,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'li@@', 'eg@@', 't', 'ich', 'Ihnen', 'ein', 'F@@', 'ast@@', '-@@', 'K@@', 'op@@', 'f@@', 'st@@', '-@@', 'K@@', 'op@@', 'f@@', 'st@@', '-@@', 'K@@', 'op@@', 'f@@', '-@@', 'K@@', 'op@@', 'f@@', '-@@', 'Jahr@@', 'en.', '</s>']
2024-05-23 15:45:34,437 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:45:34,437 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:45:34,437 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schliegt ich Ihnen ein Fast-Kopfst-Kopfst-Kopf-Kopf-Jahren.
2024-05-23 15:45:51,363 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.764489, Batch Acc: 0.495918, Tokens per Sec:     4431, Lr: 0.000300
2024-05-23 15:46:09,056 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.708025, Batch Acc: 0.500846, Tokens per Sec:     4245, Lr: 0.000300
2024-05-23 15:46:25,609 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.630800, Batch Acc: 0.502738, Tokens per Sec:     4589, Lr: 0.000300
2024-05-23 15:46:42,506 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.745471, Batch Acc: 0.497794, Tokens per Sec:     4534, Lr: 0.000300
2024-05-23 15:47:00,232 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.683383, Batch Acc: 0.496105, Tokens per Sec:     4440, Lr: 0.000300
2024-05-23 15:47:00,233 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:47:00,233 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:48:20,717 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.67, acc:   0.48, generation: 80.4737[sec], evaluation: 0.0000[sec]
2024-05-23 15:48:20,719 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:48:20,933 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/13000.ckpt
2024-05-23 15:48:20,990 - INFO - joeynmt.training - Example #0
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'den', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is@@', 'is@@', 'her', 'das', 'K@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'lan@@', 'g', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'größ@@', 'ten', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:48:20,990 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:48:20,990 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:48:20,990 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Sliden so dass die künstliche Eisisisher das Kap, die für die meisten drei Millionen Jahre lang der Größe der Größe der Größe der größten 40 Prozent.
2024-05-23 15:48:20,990 - INFO - joeynmt.training - Example #1
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'w@@', 'and@@', 'l@@', 'ung', 'dieser', 'T@@', 'eil@@', 'chen', 'dieser', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'en@@', 'b@@', 'ah@@', 'n', 'des', 'E@@', 'is@@', 'b@@', 'ah@@', 'n@@', 's', 'zeig@@', 't.', '</s>']
2024-05-23 15:48:20,990 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:48:20,990 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:48:20,990 - INFO - joeynmt.training - 	Hypothesis: Aber diese Verwandlung dieser Teilchen dieser besonders Problem, weil es nicht die Dicksal der Eisenbahn des Eisbahns zeigt.
2024-05-23 15:48:20,990 - INFO - joeynmt.training - Example #2
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:48:20,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:48:20,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'a@@', 'pp@@', 'e', 'ist', 'in', 'einem', 'S@@', 'inn@@', 'e,', 'der', 'sch@@', 'ö@@', 'pf@@', 'te', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'ale', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@']
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eiskappe ist in einem Sinne, der schöpfte Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimamawandel des globalen Klimawandel des globale Klimawandel des globalen K
2024-05-23 15:48:20,991 - INFO - joeynmt.training - Example #3
2024-05-23 15:48:20,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:48:20,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:48:20,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'au@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'au@@', 'ß@@', 'er@@', 'halb', 'der', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'au@@', 'g@@', 'en.', '</s>']
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontrauten in Sommer und Kontraußerhalb der Sommer und Kontraugen.
2024-05-23 15:48:20,991 - INFO - joeynmt.training - Example #4
2024-05-23 15:48:20,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:48:20,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:48:20,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zei@@', 'gen', 'Sie', 'einen', 'R@@', 'ei@@', 'he', 'von', 'dem', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'des', 'letz@@', 'ten', '2@@', '5', 'Jahr@@', 'en.', '</s>']
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:48:20,991 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schluss zeigen Sie einen Reihe von dem Fast-Fast-Kraft des letzten 25 Jahren.
2024-05-23 15:48:37,677 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.795115, Batch Acc: 0.499229, Tokens per Sec:     4587, Lr: 0.000300
2024-05-23 15:48:53,839 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.754965, Batch Acc: 0.502233, Tokens per Sec:     4654, Lr: 0.000300
2024-05-23 15:49:10,121 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.687068, Batch Acc: 0.495866, Tokens per Sec:     4710, Lr: 0.000300
2024-05-23 15:49:26,293 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     1.819101, Batch Acc: 0.496490, Tokens per Sec:     4705, Lr: 0.000300
2024-05-23 15:49:45,361 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     1.816719, Batch Acc: 0.498294, Tokens per Sec:     4012, Lr: 0.000300
2024-05-23 15:49:45,363 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:49:45,363 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:51:14,139 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.61, acc:   0.49, generation: 88.7662[sec], evaluation: 0.0000[sec]
2024-05-23 15:51:14,141 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:51:14,409 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/13500.ckpt
2024-05-23 15:51:14,510 - INFO - joeynmt.training - Example #0
2024-05-23 15:51:14,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:51:14,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:51:14,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'ten', 'so', 'dass', 'die', 'Ar@@', 'k@@', 't@@', 'sch@@', 'e', 'des', 'K@@', 'ün@@', 'st@@', 'l@@', 'er@@', 's,', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'S@@', 'ei@@', 'te', 'der', 'L@@', 'o@@', 'wer', 'hat', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'w@@', 'ach@@', 'en', '4@@', '0', 'Proz@@', 'ent', 'ges@@', 'am@@', 'ten', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'wor@@', 'den', 'ist.', '</s>']
2024-05-23 15:51:14,510 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:51:14,510 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:51:14,510 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Sliten so dass die Arktsche des Künstlers, die für die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Seite der Lower hat die Größe der Lwachen 40 Prozent gesamten 40 Prozent geworden ist.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - Example #1
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'änder@@', 'ung', 'der', 'Er@@', 'st@@', 'ung', 'dieser', 'Aus@@', 'wirk@@', 'ungen', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 's@@', 'sen', 'der', 'I@@', 'm@@', 'b@@', 'il@@', 'd.', '</s>']
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Hypothesis: Aber diese Veränderung der Erstung dieser Auswirkungen dieser besonders Problem weil es nicht die Dicksal der Eisssen der Imbild.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - Example #2
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'a@@', 'p', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eiskap ist in einem Sinne des globalen Klimawandels der globalen Klimawandel.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - Example #3
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:51:14,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'k@@', 'en.', '</s>']
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontragenken.
2024-05-23 15:51:14,511 - INFO - joeynmt.training - Example #4
2024-05-23 15:51:14,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:51:14,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:51:14,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zei@@', 'gen', 'wird', 'ein', 'ra@@', 'pi@@', 'd', 'der', 'F@@', 'ast@@', '-@@', 'K@@', 'op@@', 'f@@', 'st@@', '-@@', 'K@@', 'op@@', 'f@@', '-@@', 'K@@', 'op@@', 'f@@', '.', '</s>']
2024-05-23 15:51:14,512 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:51:14,512 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:51:14,512 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schluss zeigen wird ein rapid der Fast-Kopfst-Kopf-Kopf.
2024-05-23 15:51:32,690 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     1.744453, Batch Acc: 0.501908, Tokens per Sec:     4223, Lr: 0.000300
2024-05-23 15:51:51,669 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     1.515573, Batch Acc: 0.497082, Tokens per Sec:     4171, Lr: 0.000300
2024-05-23 15:52:08,094 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     1.623028, Batch Acc: 0.498380, Tokens per Sec:     4621, Lr: 0.000300
2024-05-23 15:52:24,495 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.694588, Batch Acc: 0.503453, Tokens per Sec:     4592, Lr: 0.000300
2024-05-23 15:52:41,126 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.726135, Batch Acc: 0.501963, Tokens per Sec:     4504, Lr: 0.000300
2024-05-23 15:52:41,127 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:52:41,128 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:54:18,851 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.61, acc:   0.49, generation: 97.7091[sec], evaluation: 0.0000[sec]
2024-05-23 15:54:19,298 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/14000.ckpt
2024-05-23 15:54:19,344 - INFO - joeynmt.training - Example #0
2024-05-23 15:54:19,344 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:54:19,344 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:54:19,344 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'Mon@@', 'str@@', 'at@@', 'en,', 'die', 'die', 'k@@', 'ün@@', 'st@@', 'ige', 'Zeit', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'o@@', 's@@', 's@@', 'ch', 'der', 'L@@', 'o@@', 's@@', 'er@@', '-@@', 'St@@', 'at@@', 'en,', 'hat', 'die', 'von', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'zeig@@', 't', 'wur@@', 'de.', '</s>']
2024-05-23 15:54:19,345 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:54:19,345 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:54:19,345 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Slide so dass die künstigen Monstraten, die die künstige Zeit die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Lossch der Loser-Staten, hat die von 40 Prozent gezeigt wurde.
2024-05-23 15:54:19,345 - INFO - joeynmt.training - Example #1
2024-05-23 15:54:19,345 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:54:19,345 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:54:19,345 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 't', 'das', 'S@@', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'tim@@', 'm@@', 'tes', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'I@@', 'll@@', 'e.', '</s>']
2024-05-23 15:54:19,345 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:54:19,345 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:54:19,345 - INFO - joeynmt.training - 	Hypothesis: Aber diese Verträgt das Sernsthaft dieses bestimmtes Problem ist, weil es die Dicksal der Ille.
2024-05-23 15:54:19,345 - INFO - joeynmt.training - Example #2
2024-05-23 15:54:19,346 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:54:19,346 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:54:19,346 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ti@@', 'c', 'E@@', 'is@@', 'is@@', 'k@@', 'op@@', ',', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 15:54:19,346 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:54:19,346 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:54:19,346 - INFO - joeynmt.training - 	Hypothesis: Der ktic Eisiskop, in einem Sinn des globalen Klimawandels der globalen Klimawandel.
2024-05-23 15:54:19,346 - INFO - joeynmt.training - Example #3
2024-05-23 15:54:19,346 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:54:19,346 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:54:19,346 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 15:54:19,346 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:54:19,347 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:54:19,347 - INFO - joeynmt.training - 	Hypothesis: Es wart sich in Winter und Kontragen.
2024-05-23 15:54:19,347 - INFO - joeynmt.training - Example #4
2024-05-23 15:54:19,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:54:19,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:54:19,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'ag@@', 'e,', 'dass', 'ich', 'einen', 'R@@', 'ei@@', 'he', 'von', 'der', 'schn@@', 'ell@@', 'er', 'sein,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 15:54:19,347 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:54:19,347 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:54:19,347 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schlage, dass ich einen Reihe von der schneller sein, was über die letzten 25 Jahre passiert ist.
2024-05-23 15:54:43,137 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.617508, Batch Acc: 0.497678, Tokens per Sec:     3298, Lr: 0.000300
2024-05-23 15:55:07,211 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.772208, Batch Acc: 0.505129, Tokens per Sec:     3235, Lr: 0.000300
2024-05-23 15:55:29,535 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.659697, Batch Acc: 0.498815, Tokens per Sec:     3460, Lr: 0.000300
2024-05-23 15:55:50,896 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     1.849744, Batch Acc: 0.500145, Tokens per Sec:     3706, Lr: 0.000300
2024-05-23 15:56:12,500 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.733041, Batch Acc: 0.500900, Tokens per Sec:     3601, Lr: 0.000300
2024-05-23 15:56:12,501 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:56:12,501 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 15:58:00,822 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.49, acc:   0.49, generation: 108.3064[sec], evaluation: 0.0000[sec]
2024-05-23 15:58:00,825 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 15:58:01,187 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/14500.ckpt
2024-05-23 15:58:01,252 - INFO - joeynmt.training - Example #0
2024-05-23 15:58:01,252 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 15:58:01,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 15:58:01,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'ten', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'ten', 'so', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'so', 'de@@', 'mon@@', 'stri@@', 'er@@', 'en,', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'lan@@', 'g', 'der', 'L@@', 'and@@', 'wer@@', 'sten', 'der', 'L@@', 'and@@', 'wer@@', 'k@@', 'er', 'der', 'L@@', 'u@@', 'ft@@', 'wer@@', 'ke', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 15:58:01,252 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 15:58:01,252 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 15:58:01,252 - INFO - joeynmt.training - 	Hypothesis: Letzten Jahr habe ich diese zwei Sliten so dass die Künstler so demonstrieren, die für die meisten drei Millionen Jahre lang der Landwersten der Landwerker der Luftwerke von 40 Prozent.
2024-05-23 15:58:01,252 - INFO - joeynmt.training - Example #1
2024-05-23 15:58:01,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 15:58:01,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 15:58:01,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 'en', 'die', 'Er@@', 'n@@', 'ä@@', 'hr@@', 'ung', 'dieser', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'des', 'E@@', 'is@@', 'sen@@', 's', 'zeig@@', 'en,', 'weil', 'es', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'sen@@', 'k@@', 'en.', '</s>']
2024-05-23 15:58:01,253 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 15:58:01,253 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 15:58:01,253 - INFO - joeynmt.training - 	Hypothesis: Aber das verstehen die Ernährung dieser besonders Problem des Eissens zeigen, weil es die Dicksal der Eissenken.
2024-05-23 15:58:01,253 - INFO - joeynmt.training - Example #2
2024-05-23 15:58:01,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 15:58:01,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 15:58:01,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist.', '</s>']
2024-05-23 15:58:01,254 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 15:58:01,254 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 15:58:01,254 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist in einem Sinne ist in einem Sinne des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel ist.
2024-05-23 15:58:01,254 - INFO - joeynmt.training - Example #3
2024-05-23 15:58:01,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 15:58:01,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 15:58:01,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 15:58:01,254 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 15:58:01,254 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 15:58:01,254 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontracten in Sommer Sommer und Kontragen.
2024-05-23 15:58:01,254 - INFO - joeynmt.training - Example #4
2024-05-23 15:58:01,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 15:58:01,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 15:58:01,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'li@@', 'de', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'he', 'von', 'der', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'des', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 15:58:01,255 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 15:58:01,255 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 15:58:01,255 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schlide zeige, dass ich Ihnen ein Reihe von der Fast-Kraft des letzten 25 Jahren passiert ist.
2024-05-23 15:58:21,972 - INFO - joeynmt.training - Epoch   4, Step:    17100, Batch Loss:     1.686846, Batch Acc: 0.503768, Tokens per Sec:     3683, Lr: 0.000300
2024-05-23 15:58:42,794 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     1.779212, Batch Acc: 0.499676, Tokens per Sec:     3701, Lr: 0.000300
2024-05-23 15:59:03,123 - INFO - joeynmt.training - Epoch   4, Step:    17300, Batch Loss:     1.779710, Batch Acc: 0.502944, Tokens per Sec:     3869, Lr: 0.000300
2024-05-23 15:59:23,765 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     1.807891, Batch Acc: 0.506233, Tokens per Sec:     3813, Lr: 0.000300
2024-05-23 15:59:46,273 - INFO - joeynmt.training - Epoch   4, Step:    17500, Batch Loss:     1.890539, Batch Acc: 0.505316, Tokens per Sec:     3372, Lr: 0.000300
2024-05-23 15:59:46,274 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 15:59:46,274 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:01:15,341 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.46, acc:   0.50, generation: 89.0526[sec], evaluation: 0.0000[sec]
2024-05-23 16:01:15,345 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:01:15,696 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/15000.ckpt
2024-05-23 16:01:15,748 - INFO - joeynmt.training - Example #0
2024-05-23 16:01:15,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:01:15,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:01:15,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'zeig@@', 'te', 'diese', 'zwei', 'S@@', 'li@@', 'n', 'zeig@@', 'te', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'größ@@', 'te', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'eu@@', 'ten', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:01:15,748 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:01:15,749 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:01:15,749 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich zeigte diese zwei Slin zeigte so dass die künstigen Eiskap, die für die meisten drei Millionen Jahre war die größte drei Millionen Jahre war die Größe der Größe der Größe der Größe der Leuten von 40 Prozent.
2024-05-23 16:01:15,749 - INFO - joeynmt.training - Example #1
2024-05-23 16:01:15,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:01:15,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:01:15,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'änder@@', 'ung', 'dieser', 'bes@@', 'onder@@', 's', 'dieser', 'bes@@', 'onder@@', 's', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 's@@', 'sen', 'zeig@@', 't.', '</s>']
2024-05-23 16:01:15,749 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:01:15,749 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:01:15,749 - INFO - joeynmt.training - 	Hypothesis: Aber diese Veränderung dieser besonders dieser besonders dieses besonders Problem, weil es nicht die Dicksal der Eisssen zeigt.
2024-05-23 16:01:15,749 - INFO - joeynmt.training - Example #2
2024-05-23 16:01:15,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:01:15,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:01:15,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'der', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's@@', 'st@@', 'ra@@', 'ß@@', 'e', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'K@@', 'li@@', 'ma@@', 'at@@', 'st@@', 'off@@', '.', '</s>']
2024-05-23 16:01:15,749 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:01:15,750 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:01:15,750 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eis ist in einem Sinn der Klimawandelsstraße des globalen Klimawandel der Klimaatstoff.
2024-05-23 16:01:15,750 - INFO - joeynmt.training - Example #3
2024-05-23 16:01:15,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:01:15,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:01:15,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'en@@', 'z', 'in', 'W@@', 'in@@', 'ter', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'as@@', 'se', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 16:01:15,750 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:01:15,750 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:01:15,750 - INFO - joeynmt.training - 	Hypothesis: Es erwart in Winter und Kontrenz in Winter Sommer und Kontrasse in Winter und Kontragen.
2024-05-23 16:01:15,750 - INFO - joeynmt.training - Example #4
2024-05-23 16:01:15,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:01:15,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:01:15,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zei@@', 'gen', 'wir@@', 'd,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'wird.', '</s>']
2024-05-23 16:01:15,750 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:01:15,750 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:01:15,751 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schluss zeigen wird, was über die letzten 25 Jahre in den letzten 25 Jahren passiert wird.
2024-05-23 16:01:37,704 - INFO - joeynmt.training - Epoch   4, Step:    17600, Batch Loss:     1.621084, Batch Acc: 0.509662, Tokens per Sec:     3446, Lr: 0.000300
2024-05-23 16:01:59,206 - INFO - joeynmt.training - Epoch   4, Step:    17700, Batch Loss:     1.624180, Batch Acc: 0.503895, Tokens per Sec:     3546, Lr: 0.000300
2024-05-23 16:02:21,608 - INFO - joeynmt.training - Epoch   4, Step:    17800, Batch Loss:     1.677500, Batch Acc: 0.506351, Tokens per Sec:     3508, Lr: 0.000300
2024-05-23 16:02:42,576 - INFO - joeynmt.training - Epoch   4, Step:    17900, Batch Loss:     1.779066, Batch Acc: 0.504785, Tokens per Sec:     3628, Lr: 0.000300
2024-05-23 16:03:02,681 - INFO - joeynmt.training - Epoch   4, Step:    18000, Batch Loss:     1.627412, Batch Acc: 0.500136, Tokens per Sec:     3827, Lr: 0.000300
2024-05-23 16:03:02,682 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:03:02,682 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:04:38,588 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.42, acc:   0.50, generation: 95.8911[sec], evaluation: 0.0000[sec]
2024-05-23 16:04:38,593 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:04:38,929 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/15500.ckpt
2024-05-23 16:04:38,994 - INFO - joeynmt.training - Example #0
2024-05-23 16:04:38,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:04:38,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:04:38,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'zwei', 'S@@', 'li@@', 'z@@', 'ier@@', ',', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'L@@', 'age', 'der', 'L@@', 'age', 'der', 'L@@', 'age', 'der', 'L@@', 'age', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:04:38,995 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:04:38,995 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:04:38,995 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden zwei Slizier, dass die künstler die Künstler der letzten drei Millionen Jahre der letzten drei Millionen Jahre die Leistung der Lage der Lage der Lage der Lage von 40 Prozent.
2024-05-23 16:04:38,995 - INFO - joeynmt.training - Example #1
2024-05-23 16:04:38,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:04:38,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:04:38,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schie@@', 'de', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'tim@@', 'm@@', 'ten', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'k@@', 'einen', 'D@@', 'ick@@', 's@@', 'al', 'der', 'I@@', 'm@@', 'st@@', 'off', 'zeig@@', 't.', '</s>']
2024-05-23 16:04:38,995 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:04:38,995 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:04:38,995 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschiede die Ernsthaft dieses bestimmten Problem weil es nicht die Dickkeinen Dicksal der Imstoff zeigt.
2024-05-23 16:04:38,995 - INFO - joeynmt.training - Example #2
2024-05-23 16:04:38,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:04:38,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:04:38,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', 'en@@', 'd,', 'der', 'S@@', 'inn@@', 'e,', 'der', 'S@@', 'inn@@', 'e,', 'der', 'B@@', 'li@@', 'ck', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'er@@', 'es', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 16:04:38,996 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:04:38,996 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:04:38,996 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eisketische Eisend, der Sinne, der Sinne, der Blick des globalen Klimaklimaeres System.
2024-05-23 16:04:38,996 - INFO - joeynmt.training - Example #3
2024-05-23 16:04:38,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:04:38,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:04:38,996 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'au@@', 'm@@', 'er', 'und', 'Kon@@', 'tr@@', 'au@@', 'm@@', 'er', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'k@@', 'en.', '</s>']
2024-05-23 16:04:38,996 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:04:38,996 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:04:38,996 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontraumer und Kontraumer in Winter und Kontragenken.
2024-05-23 16:04:38,996 - INFO - joeynmt.training - Example #4
2024-05-23 16:04:38,996 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:04:38,996 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:04:38,997 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'he', 'von', 'der', 'F@@', 'ast@@', '-@@', 'For@@', 'sch@@', 'ung', 'von', 'dem', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:04:38,997 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:04:38,997 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:04:38,997 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen ein Reihe von der Fast-Forschung von dem letzten 25 Jahre passiert ist.
2024-05-23 16:04:47,879 - INFO - joeynmt.training - Epoch   4: total training loss 7581.16
2024-05-23 16:04:47,880 - INFO - joeynmt.training - EPOCH 5
2024-05-23 16:04:58,347 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.492513, Batch Acc: 0.527924, Tokens per Sec:     3906, Lr: 0.000300
2024-05-23 16:05:17,845 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.530036, Batch Acc: 0.526819, Tokens per Sec:     3897, Lr: 0.000300
2024-05-23 16:05:37,189 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     1.590987, Batch Acc: 0.521104, Tokens per Sec:     3922, Lr: 0.000300
2024-05-23 16:05:57,980 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.564488, Batch Acc: 0.525494, Tokens per Sec:     3710, Lr: 0.000300
2024-05-23 16:06:18,507 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.547653, Batch Acc: 0.525713, Tokens per Sec:     3844, Lr: 0.000300
2024-05-23 16:06:18,507 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:06:18,507 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:07:44,710 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.41, acc:   0.50, generation: 86.1871[sec], evaluation: 0.0000[sec]
2024-05-23 16:07:44,714 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:07:45,042 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/16500.ckpt
2024-05-23 16:07:45,163 - INFO - joeynmt.training - Example #0
2024-05-23 16:07:45,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:07:45,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:07:45,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'ige', 'E@@', 'is@@', 'is', 'des', 'K@@', 'ap@@', 's,', 'die', 'die', 'mei@@', 'sten', 'von', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'au@@', 'er@@', '-@@', 'St@@', 'at@@', 'en,', 'hat', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '0', 'Proz@@', 'ent', 'ist.', '</s>']
2024-05-23 16:07:45,163 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:07:45,163 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:07:45,163 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Slide so dass die künstige Eisis des Kaps, die die meisten von der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Lauer-Staten, hat die Größe 40 Prozent ist.
2024-05-23 16:07:45,163 - INFO - joeynmt.training - Example #1
2024-05-23 16:07:45,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:07:45,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:07:45,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'stat@@', 't', 'der', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'enschaf@@', 't', 'zeig@@', 't.', '</s>']
2024-05-23 16:07:45,164 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:07:45,164 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:07:45,164 - INFO - joeynmt.training - 	Hypothesis: Aber diese Verstatt der ernsthaft dieses besonders Problem zu zeigen, weil es nicht die Dicksal der Eisenschaft zeigt.
2024-05-23 16:07:45,164 - INFO - joeynmt.training - Example #2
2024-05-23 16:07:45,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:07:45,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:07:45,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'K@@', 'ün@@', 'st@@', 'l@@', 'er@@', 's@@', 'at@@', 'z@@', ',', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist.', '</s>']
2024-05-23 16:07:45,164 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:07:45,164 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:07:45,164 - INFO - joeynmt.training - 	Hypothesis: Der Künstlersatz, in einem Sinn des globalen Klimawandel der globalen Klimawandel der globalen Klimawandel der globalen Klimawandel der globalen Klimawandel des globalen Klimawandel der globalen Klimawandel ist.
2024-05-23 16:07:45,164 - INFO - joeynmt.training - Example #3
2024-05-23 16:07:45,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:07:45,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:07:45,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 16:07:45,165 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:07:45,165 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:07:45,165 - INFO - joeynmt.training - 	Hypothesis: Es erwartete sich in Winter und Kontragen.
2024-05-23 16:07:45,165 - INFO - joeynmt.training - Example #4
2024-05-23 16:07:45,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:07:45,165 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:07:45,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'w@@', 'es@@', 'ter@@', 'e', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'he', 'von', 'de@@', 'm,', 'was', 'über', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 16:07:45,165 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:07:45,165 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:07:45,165 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schwestere ich Ihnen ein Reihe von dem, was über den letzten 25 Jahren passiert.
2024-05-23 16:08:05,920 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.554576, Batch Acc: 0.517866, Tokens per Sec:     3588, Lr: 0.000300
2024-05-23 16:08:25,894 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.511186, Batch Acc: 0.520382, Tokens per Sec:     3891, Lr: 0.000300
2024-05-23 16:08:48,458 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.754485, Batch Acc: 0.516953, Tokens per Sec:     3397, Lr: 0.000300
2024-05-23 16:09:10,666 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.596423, Batch Acc: 0.520476, Tokens per Sec:     3404, Lr: 0.000300
2024-05-23 16:09:31,912 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.656995, Batch Acc: 0.514212, Tokens per Sec:     3453, Lr: 0.000300
2024-05-23 16:09:31,914 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:09:31,914 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:10:59,683 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.43, acc:   0.50, generation: 87.7551[sec], evaluation: 0.0000[sec]
2024-05-23 16:11:00,050 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/16000.ckpt
2024-05-23 16:11:00,147 - INFO - joeynmt.training - Example #0
2024-05-23 16:11:00,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:11:00,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:11:00,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'bei@@', 'den', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'lan@@', 'g', 'der', 'größ@@', 'ten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'L@@', 'au@@', 'ten', '4@@', '8', 'St@@', 'at@@', 'en,', 'ist', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:11:00,148 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:11:00,148 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:11:00,148 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese beiden Slide so dass die künstler der künstler der letzten drei Millionen Jahre lang der größten drei Millionen Jahre war die Lauten 48 Staten, ist die Größe 40 Prozent.
2024-05-23 16:11:00,148 - INFO - joeynmt.training - Example #1
2024-05-23 16:11:00,148 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:11:00,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:11:00,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'su@@', 'chen', 'die', 'S@@', 'er@@', 'ie', 'dieser', 'bes@@', 'onder@@', 's', 'Problem@@', 'e', 'zu', 'be@@', 'wei@@', 'sen,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'I@@', 'r@@', 'gend@@', 's.', '</s>']
2024-05-23 16:11:00,148 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:11:00,148 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - 	Hypothesis: Aber diese Versuchen die Serie dieser besonders Probleme zu beweisen, weil es nicht die Dicksal des Irgends.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - Example #2
2024-05-23 16:11:00,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:11:00,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:11:00,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'ein', 'S@@', 'in@@', 'n', 'ist', 'ein', 'S@@', 'inn@@', ',', 'der', 'B@@', 'et@@', 'rach@@', 't@@', 'ungs@@', 'system@@', '.', '</s>']
2024-05-23 16:11:00,149 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - 	Hypothesis: Der künstler ist ein Sinn ist ein Sinn, der Betrachtungssystem.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - Example #3
2024-05-23 16:11:00,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:11:00,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:11:00,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'S@@', 'omm@@', 'er@@', 'ie', 'und', 'Kon@@', 'tr@@', 'enn@@', 'en.', '</s>']
2024-05-23 16:11:00,149 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontracten in Sommerie und Kontrennen.
2024-05-23 16:11:00,149 - INFO - joeynmt.training - Example #4
2024-05-23 16:11:00,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:11:00,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:11:00,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'li@@', 'de', 'zei@@', 'gen', 'Sie', 'ein', 'R@@', 'ei@@', 'h@@', 'en@@', 'der', 'F@@', 'ast@@', '-@@', 'W@@', 'ol@@', 'd@@', 'ung', 'des', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:11:00,150 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:11:00,150 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:11:00,150 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schlide zeigen Sie ein Reihender Fast-Woldung des passiert ist.
2024-05-23 16:11:20,250 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     1.450029, Batch Acc: 0.518993, Tokens per Sec:     3889, Lr: 0.000300
2024-05-23 16:11:41,920 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.504160, Batch Acc: 0.515127, Tokens per Sec:     3547, Lr: 0.000300
2024-05-23 16:12:01,758 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     1.625547, Batch Acc: 0.515094, Tokens per Sec:     3844, Lr: 0.000300
2024-05-23 16:12:22,530 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.664950, Batch Acc: 0.520444, Tokens per Sec:     3664, Lr: 0.000300
2024-05-23 16:12:44,291 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.607074, Batch Acc: 0.525683, Tokens per Sec:     3600, Lr: 0.000300
2024-05-23 16:12:44,293 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:12:44,293 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:14:17,641 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.35, acc:   0.50, generation: 93.3324[sec], evaluation: 0.0000[sec]
2024-05-23 16:14:17,644 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:14:17,978 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/17000.ckpt
2024-05-23 16:14:18,072 - INFO - joeynmt.training - Example #0
2024-05-23 16:14:18,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:14:18,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:14:18,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'K@@', 'un@@', 'st@@', 'e,', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'u@@', 'f@@', 't,', 'die', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'o@@', 'wer', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:14:18,073 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:14:18,073 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:14:18,073 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Slide so dass die Künstler so dass die künstigen der Kunste, die für die meisten drei Millionen Jahre der Luft, die die Größe der Lower von 40 Prozent.
2024-05-23 16:14:18,073 - INFO - joeynmt.training - Example #1
2024-05-23 16:14:18,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:14:18,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:14:18,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Unter@@', 'schie@@', 'de', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't', 'nicht', 'die', 'D@@', 'ick@@', 'e.', '</s>']
2024-05-23 16:14:18,074 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:14:18,074 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:14:18,074 - INFO - joeynmt.training - 	Hypothesis: Aber das Unterschiede ist die ernsthaft dieses speziellen Problem weil es nicht die Dicksal des Eises zeigt nicht die Dicke.
2024-05-23 16:14:18,074 - INFO - joeynmt.training - Example #2
2024-05-23 16:14:18,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:14:18,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:14:18,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ti@@', 'c', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 16:14:18,074 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:14:18,074 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:14:18,074 - INFO - joeynmt.training - 	Hypothesis: Der ktic Eis ist in einem Sinn des globalen Klimawandels des globalen Klimawandels.
2024-05-23 16:14:18,074 - INFO - joeynmt.training - Example #3
2024-05-23 16:14:18,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:14:18,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:14:18,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tr@@', 'au@@', 'st@@', 'er.', '</s>']
2024-05-23 16:14:18,074 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:14:18,075 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:14:18,075 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Verträgung in Sommer und Vertrauster.
2024-05-23 16:14:18,075 - INFO - joeynmt.training - Example #4
2024-05-23 16:14:18,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:14:18,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:14:18,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'li@@', 'de', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'he', 'schn@@', 'ell@@', 'er', 'sein', 'W@@', 'al@@', 'd', 'des', 'gesch@@', 'i@@', 'eht', 'von', 'dem', 'passi@@', 'er@@', 'ten', '2@@', '5', 'Jahren', 'gesch@@', 'i@@', 'eh@@', 't.', '</s>']
2024-05-23 16:14:18,075 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:14:18,075 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:14:18,075 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schlide ich Ihnen ein Reihe schneller sein Wald des geschieht von dem passierten 25 Jahren geschieht.
2024-05-23 16:14:39,389 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     1.713286, Batch Acc: 0.521956, Tokens per Sec:     3522, Lr: 0.000300
2024-05-23 16:14:58,987 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     1.562842, Batch Acc: 0.520283, Tokens per Sec:     3878, Lr: 0.000300
2024-05-23 16:15:18,983 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.516472, Batch Acc: 0.520337, Tokens per Sec:     3784, Lr: 0.000300
2024-05-23 16:15:38,728 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.628994, Batch Acc: 0.517552, Tokens per Sec:     3958, Lr: 0.000300
2024-05-23 16:15:59,015 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.611547, Batch Acc: 0.524372, Tokens per Sec:     3824, Lr: 0.000300
2024-05-23 16:15:59,016 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:15:59,016 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:17:35,091 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.30, acc:   0.50, generation: 96.0601[sec], evaluation: 0.0000[sec]
2024-05-23 16:17:35,094 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:17:35,435 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/17500.ckpt
2024-05-23 16:17:35,505 - INFO - joeynmt.training - Example #0
2024-05-23 16:17:35,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:17:35,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:17:35,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'S@@', 'li@@', 'de', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'K@@', 'ün@@', 'st@@', 'e,', 'die', 'für', 'die', 'mei@@', 'sten', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'eu@@', 'ten', 'der', 'L@@', 'eu@@', 'wer@@', 'k@@', 'ra@@', 'ft', 'von', '4@@', '8', 'St@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:17:35,506 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:17:35,506 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:17:35,506 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Slide so dass die künstigen der künstigen der Künste, die für die meisten der letzten drei Millionen Jahre die Leuten der Leuwerkraft von 48 Staten, Shrunk von 40 Prozent.
2024-05-23 16:17:35,506 - INFO - joeynmt.training - Example #1
2024-05-23 16:17:35,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:17:35,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:17:35,506 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', 'Unter@@', 'nehmen', 'die', 'S@@', 'er@@', 'ie', 'dieser', 'bes@@', 'onder@@', 's', 'dieser', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'en@@', 'st@@', 'off', 'zeig@@', 't.', '</s>']
2024-05-23 16:17:35,506 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:17:35,506 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:17:35,506 - INFO - joeynmt.training - 	Hypothesis: Aber dieses Unternehmen die Serie dieser besonders dieser besondere Problem ist, weil es den Dicksal der Eisenstoff zeigt.
2024-05-23 16:17:35,506 - INFO - joeynmt.training - Example #2
2024-05-23 16:17:35,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:17:35,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:17:35,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'a@@', 'p', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's@@', 'system@@', 's.', '</s>']
2024-05-23 16:17:35,507 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:17:35,507 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:17:35,507 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eiskap ist in einem Sinn des globalen Klimawandelssystems.
2024-05-23 16:17:35,507 - INFO - joeynmt.training - Example #3
2024-05-23 16:17:35,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:17:35,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:17:35,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 'ung', 'im', 'S@@', 'omm@@', 'er', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'st@@', 'er.', '</s>']
2024-05-23 16:17:35,507 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:17:35,507 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:17:35,507 - INFO - joeynmt.training - 	Hypothesis: Es erwart in Winter und Kontrennung im Sommer im Sommer und Kontragenster.
2024-05-23 16:17:35,507 - INFO - joeynmt.training - Example #4
2024-05-23 16:17:35,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:17:35,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:17:35,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'ach@@', 't', 'werde', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'Z@@', 'au@@', 'ber@@', 'ü@@', 'h@@', 'st@@', 'en@@', 'f@@', 'ä@@', 'll@@', 't', 'von', 'dem', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 16:17:35,508 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:17:35,508 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:17:35,508 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schlacht werde ich Ihnen ein schnelles Zauberühstenfällt von dem letzten 25 Jahren passiert.
2024-05-23 16:17:58,221 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     1.686980, Batch Acc: 0.517078, Tokens per Sec:     3317, Lr: 0.000300
2024-05-23 16:18:19,171 - INFO - joeynmt.training - Epoch   5, Step:    20200, Batch Loss:     1.768893, Batch Acc: 0.523482, Tokens per Sec:     3688, Lr: 0.000300
2024-05-23 16:18:41,183 - INFO - joeynmt.training - Epoch   5, Step:    20300, Batch Loss:     1.609751, Batch Acc: 0.528553, Tokens per Sec:     3574, Lr: 0.000300
2024-05-23 16:19:03,024 - INFO - joeynmt.training - Epoch   5, Step:    20400, Batch Loss:     1.651616, Batch Acc: 0.519120, Tokens per Sec:     3525, Lr: 0.000300
2024-05-23 16:19:22,938 - INFO - joeynmt.training - Epoch   5, Step:    20500, Batch Loss:     1.664402, Batch Acc: 0.521972, Tokens per Sec:     3958, Lr: 0.000300
2024-05-23 16:19:22,939 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:19:22,939 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:20:54,881 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.27, acc:   0.50, generation: 91.9273[sec], evaluation: 0.0000[sec]
2024-05-23 16:20:54,884 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:20:55,231 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/19000.ckpt
2024-05-23 16:20:55,339 - INFO - joeynmt.training - Example #0
2024-05-23 16:20:55,339 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:20:55,339 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:20:55,339 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'ol@@', 'li@@', 'den', 'so', 'dass', 'die', 'K@@', 'ar@@', 'k@@', 'ra@@', 'f@@', 't,', 'dass', 'die', 'Ar@@', 'k@@', 'k@@', 'ra@@', 'f@@', 't,', 'die', 'für', 'die', 'mei@@', 'sten', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'and@@', 'k@@', 'ra@@', 'ft', 'von', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'zeig@@', 't', 'wur@@', 'de.', '</s>']
2024-05-23 16:20:55,340 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:20:55,340 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:20:55,340 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Solliden so dass die Karkraft, dass die Arkkraft, die für die meisten der letzten drei Millionen Jahre der Landkraft von 40 Prozent gezeigt wurde.
2024-05-23 16:20:55,340 - INFO - joeynmt.training - Example #1
2024-05-23 16:20:55,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:20:55,340 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:20:55,340 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Unter@@', 'nehmen', 'der', 'die', 'Di@@', 'es@@', 'es', 'Pro@@', 'blem', 'dieser', 'bes@@', 'onder@@', 's', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is', 'des', 'I@@', 'ic@@', 'es', 'zeig@@', 't.', '</s>']
2024-05-23 16:20:55,340 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:20:55,340 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:20:55,340 - INFO - joeynmt.training - 	Hypothesis: Aber das Unternehmen der die Dieses Problem dieser besonders Probleme, weil es nicht die Dicksal des Eis des Iices zeigt.
2024-05-23 16:20:55,340 - INFO - joeynmt.training - Example #2
2024-05-23 16:20:55,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:20:55,340 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:20:55,340 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'st@@', 'off@@', 'e', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'st@@', 'off@@', '.', '</s>']
2024-05-23 16:20:55,341 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:20:55,341 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:20:55,341 - INFO - joeynmt.training - 	Hypothesis: Die Künstler ist in einem Sinn des globalen Klimakstoffe des globalen Klimakstoff.
2024-05-23 16:20:55,341 - INFO - joeynmt.training - Example #3
2024-05-23 16:20:55,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:20:55,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:20:55,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'f@@', 't.', '</s>']
2024-05-23 16:20:55,341 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:20:55,341 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:20:55,341 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Winter Sommer und Kontraft.
2024-05-23 16:20:55,341 - INFO - joeynmt.training - Example #4
2024-05-23 16:20:55,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:20:55,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:20:55,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'A@@', 'k@@', 'ti@@', 'k@@', 'el', 'ist,', 'der', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:20:55,342 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:20:55,342 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:20:55,342 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein schneller Aktikel ist, der die letzten 25 Jahre passiert ist.
2024-05-23 16:21:19,324 - INFO - joeynmt.training - Epoch   5, Step:    20600, Batch Loss:     1.650617, Batch Acc: 0.525662, Tokens per Sec:     3012, Lr: 0.000300
2024-05-23 16:21:52,940 - INFO - joeynmt.training - Epoch   5, Step:    20700, Batch Loss:     1.733701, Batch Acc: 0.520081, Tokens per Sec:     2239, Lr: 0.000300
2024-05-23 16:22:13,948 - INFO - joeynmt.training - Epoch   5, Step:    20800, Batch Loss:     1.598165, Batch Acc: 0.519294, Tokens per Sec:     3637, Lr: 0.000300
2024-05-23 16:22:34,855 - INFO - joeynmt.training - Epoch   5, Step:    20900, Batch Loss:     1.696315, Batch Acc: 0.521107, Tokens per Sec:     3600, Lr: 0.000300
2024-05-23 16:22:54,553 - INFO - joeynmt.training - Epoch   5, Step:    21000, Batch Loss:     1.559742, Batch Acc: 0.520178, Tokens per Sec:     3832, Lr: 0.000300
2024-05-23 16:22:54,554 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:22:54,554 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:24:37,277 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.50, generation: 102.7080[sec], evaluation: 0.0000[sec]
2024-05-23 16:24:37,701 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/18000.ckpt
2024-05-23 16:24:37,761 - INFO - joeynmt.training - Example #0
2024-05-23 16:24:37,761 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:24:37,761 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:24:37,761 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'ol@@', 'li@@', 'den', 'so', 'de@@', 'mon@@', 'stri@@', 'er@@', 'en,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'K@@', 'un@@', 'st@@', 'a@@', 'at@@', ',', 'wel@@', 'che', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'o@@', 'wer', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'o@@', 'wer', '4@@', '0', 'Proz@@', 'ent', 'ges@@', 'am@@', 't', 'wur@@', 'den.', '</s>']
2024-05-23 16:24:37,761 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:24:37,761 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:24:37,761 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Solliden so demonstrieren, dass die künstigen der Kunstaat, welche für die meisten drei Millionen Jahre die Lower 40 Prozent der Lower 40 Prozent gesamt wurden.
2024-05-23 16:24:37,762 - INFO - joeynmt.training - Example #1
2024-05-23 16:24:37,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:24:37,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:24:37,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'Er@@', 'n@@', 'ä@@', 'hr@@', 'ung', 'dieser', 'bes@@', 'tim@@', 'm@@', 'ten', 'Pro@@', 'blem', 'weil', 'es', 'die', 'D@@', 'ick@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'e', 'der', 'I@@', 'm', 'E@@', 'is@@', 'e', 'der', 'I@@', 'm', 'E@@', 'is@@', 'e', 'zeig@@', 't.', '</s>']
2024-05-23 16:24:37,762 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:24:37,762 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:24:37,762 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die Ernährung dieser bestimmten Problem weil es die Dickicksal der Eise der Im Eise der Im Eise zeigt.
2024-05-23 16:24:37,762 - INFO - joeynmt.training - Example #2
2024-05-23 16:24:37,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:24:37,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:24:37,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'der', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 16:24:37,762 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:24:37,762 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:24:37,763 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eis ist in einem Sinn des globalen Klimawandels der globalen Klimawandel.
2024-05-23 16:24:37,763 - INFO - joeynmt.training - Example #3
2024-05-23 16:24:37,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:24:37,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:24:37,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 16:24:37,763 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:24:37,763 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:24:37,763 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und vertragen.
2024-05-23 16:24:37,763 - INFO - joeynmt.training - Example #4
2024-05-23 16:24:37,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:24:37,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:24:37,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'werde', 'ich', 'Ihnen', 'ein', 'R@@', 'a@@', 'pi@@', 'd', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:24:37,763 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:24:37,763 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:24:37,763 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schluss werde ich Ihnen ein Rapid von der letzten 25 Jahren passiert ist, was über die letzten 25 Jahren passiert ist.
2024-05-23 16:24:58,995 - INFO - joeynmt.training - Epoch   5, Step:    21100, Batch Loss:     1.601826, Batch Acc: 0.521183, Tokens per Sec:     3596, Lr: 0.000300
2024-05-23 16:25:19,549 - INFO - joeynmt.training - Epoch   5, Step:    21200, Batch Loss:     1.604162, Batch Acc: 0.523746, Tokens per Sec:     3702, Lr: 0.000300
2024-05-23 16:25:40,800 - INFO - joeynmt.training - Epoch   5, Step:    21300, Batch Loss:     1.702691, Batch Acc: 0.513783, Tokens per Sec:     3436, Lr: 0.000300
2024-05-23 16:26:01,947 - INFO - joeynmt.training - Epoch   5, Step:    21400, Batch Loss:     1.687737, Batch Acc: 0.521126, Tokens per Sec:     3731, Lr: 0.000300
2024-05-23 16:26:22,432 - INFO - joeynmt.training - Epoch   5, Step:    21500, Batch Loss:     1.602465, Batch Acc: 0.520815, Tokens per Sec:     3801, Lr: 0.000300
2024-05-23 16:26:22,433 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:26:22,433 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:27:53,472 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.20, acc:   0.51, generation: 91.0242[sec], evaluation: 0.0000[sec]
2024-05-23 16:27:53,475 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:27:53,854 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/18500.ckpt
2024-05-23 16:27:53,916 - INFO - joeynmt.training - Example #0
2024-05-23 16:27:53,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:27:53,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:27:53,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'den', 'so', 'dass', 'die', 'k@@', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'lan@@', 'g', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'o@@', 'wer', '4@@', '8', 'St@@', 'at@@', 'en,', 'die', 'für', 'die', 'L@@', 'o@@', 'wer', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:27:53,916 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:27:53,917 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:27:53,917 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Sliden so dass die karktischen Eiskap, die die meisten drei Millionen Jahre lang der letzten drei Millionen Jahre der Lower 48 Staten, die für die Lower 40 Prozent.
2024-05-23 16:27:53,917 - INFO - joeynmt.training - Example #1
2024-05-23 16:27:53,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:27:53,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:27:53,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Ver@@', 'stän@@', 'd@@', 'n@@', 'is', 'dieser', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'zu', 'ver@@', 'bes@@', 'ser@@', 'n.', '</s>']
2024-05-23 16:27:53,917 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:27:53,917 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:27:53,917 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Verständnis dieser besondere Problem zu verbessern.
2024-05-23 16:27:53,917 - INFO - joeynmt.training - Example #2
2024-05-23 16:27:53,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:27:53,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:27:53,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'K@@', 'l@@', 'ein@@', 's@@', 'at@@', 'z@@', ',', 'in', 'einem', 'S@@', 'inn@@', ',', 'in', 'einem', 'S@@', 'inn@@', ',', 'der', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'er@@', 'en.', '</s>']
2024-05-23 16:27:53,918 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:27:53,918 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:27:53,918 - INFO - joeynmt.training - 	Hypothesis: Das Kleinsatz, in einem Sinn, in einem Sinn, der Klimaklimaklimaklimaeren.
2024-05-23 16:27:53,918 - INFO - joeynmt.training - Example #3
2024-05-23 16:27:53,918 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:27:53,918 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:27:53,918 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'kt', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'au@@', 'm@@', 'er', 'zu', 'ver@@', 'ein@@', 'en.', '</s>']
2024-05-23 16:27:53,918 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:27:53,918 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:27:53,918 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrackt in Winter und Kontraumer zu vereinen.
2024-05-23 16:27:53,918 - INFO - joeynmt.training - Example #4
2024-05-23 16:27:53,918 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:27:53,918 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:27:53,918 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'R@@', 'a@@', 'pi@@', 'd', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:27:53,918 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:27:53,919 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:27:53,919 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schluss zeige, dass Sie ein Rapid von der letzten 25 Jahren passiert ist, was über die letzten 25 Jahren passiert ist.
2024-05-23 16:28:19,715 - INFO - joeynmt.training - Epoch   5, Step:    21600, Batch Loss:     1.679195, Batch Acc: 0.520378, Tokens per Sec:     2964, Lr: 0.000300
2024-05-23 16:28:42,219 - INFO - joeynmt.training - Epoch   5, Step:    21700, Batch Loss:     1.696887, Batch Acc: 0.519285, Tokens per Sec:     3360, Lr: 0.000300
2024-05-23 16:29:04,242 - INFO - joeynmt.training - Epoch   5, Step:    21800, Batch Loss:     1.755934, Batch Acc: 0.516556, Tokens per Sec:     3453, Lr: 0.000300
2024-05-23 16:29:25,344 - INFO - joeynmt.training - Epoch   5, Step:    21900, Batch Loss:     1.408528, Batch Acc: 0.520161, Tokens per Sec:     3702, Lr: 0.000300
2024-05-23 16:32:55,450 - INFO - joeynmt.training - Epoch   5, Step:    22000, Batch Loss:     1.517352, Batch Acc: 0.519671, Tokens per Sec:      362, Lr: 0.000300
2024-05-23 16:32:55,452 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:32:55,452 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:34:27,168 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.65, ppl:   5.19, acc:   0.51, generation: 91.7007[sec], evaluation: 0.0000[sec]
2024-05-23 16:34:27,172 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:34:27,562 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/19500.ckpt
2024-05-23 16:34:27,627 - INFO - joeynmt.training - Example #0
2024-05-23 16:34:27,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:34:27,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:34:27,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 't@@', 'ten', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'te', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'te', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'o@@', 'm@@', 'i@@', 'es@@', 't@@', 'eil', 'der', 'L@@', 'o@@', 'wer@@', 'ke', '4@@', '8', 'St@@', 'at@@', 'en,', 'die', 'für', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'wor@@', 'den', 'ist.', '</s>']
2024-05-23 16:34:27,627 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:34:27,628 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:34:27,628 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich diese beiden Schlitten so dass die künstlerte die künstlerte der Größe der Lomiesteil der Lowerke 48 Staten, die für 40 Prozent geworden ist.
2024-05-23 16:34:27,628 - INFO - joeynmt.training - Example #1
2024-05-23 16:34:27,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:34:27,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:34:27,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Ver@@', 'änder@@', 'ung', 'dieses', 'bes@@', 'tim@@', 'm@@', 'es', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'des', 'E@@', 'is@@', 'b@@', 'ah@@', 'n@@', 'b@@', 'ar', 'zeig@@', 't.', '</s>']
2024-05-23 16:34:27,628 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:34:27,628 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:34:27,628 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Veränderung dieses bestimmes Problem zu zeigen, weil es die Dicksal der Eis der Eis des Eisbahnbar zeigt.
2024-05-23 16:34:27,628 - INFO - joeynmt.training - Example #2
2024-05-23 16:34:27,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:34:27,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:34:27,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ti@@', 'c', 'E@@', 'is@@', 'is@@', 'e', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'der', 'der', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 't.', '</s>']
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Hypothesis: Die ktic Eisise ist in einem Sinn, der der Klimaklimawandelt.
2024-05-23 16:34:27,629 - INFO - joeynmt.training - Example #3
2024-05-23 16:34:27,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:34:27,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:34:27,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'au@@', 'm@@', 'er', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 'en.', '</s>']
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontrennt in Winter und Kontraumer in Winter und Kontrennt in Winter und Kontrennt in Winter und Kontrennt in Winter und Kontrennen.
2024-05-23 16:34:27,629 - INFO - joeynmt.training - Example #4
2024-05-23 16:34:27,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:34:27,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:34:27,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'einen', 'schn@@', 'ell@@', '-@@', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'K@@', 'u@@', 'ge@@', 'l', 'von', 'dem', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:34:27,629 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:34:27,630 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich einen schnell-Fast-Fast-Kugel von dem letzten 25 Jahren passiert ist.
2024-05-23 16:34:49,984 - INFO - joeynmt.training - Epoch   5, Step:    22100, Batch Loss:     1.609901, Batch Acc: 0.524920, Tokens per Sec:     3467, Lr: 0.000300
2024-05-23 16:35:10,444 - INFO - joeynmt.training - Epoch   5, Step:    22200, Batch Loss:     1.640919, Batch Acc: 0.528789, Tokens per Sec:     3774, Lr: 0.000300
2024-05-23 16:35:30,053 - INFO - joeynmt.training - Epoch   5, Step:    22300, Batch Loss:     1.614876, Batch Acc: 0.525733, Tokens per Sec:     3928, Lr: 0.000300
2024-05-23 16:35:50,152 - INFO - joeynmt.training - Epoch   5, Step:    22400, Batch Loss:     1.549614, Batch Acc: 0.517170, Tokens per Sec:     3745, Lr: 0.000300
2024-05-23 16:36:09,917 - INFO - joeynmt.training - Epoch   5, Step:    22500, Batch Loss:     1.706861, Batch Acc: 0.520760, Tokens per Sec:     3912, Lr: 0.000300
2024-05-23 16:36:09,918 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:36:09,918 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:37:29,125 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.51, generation: 79.1928[sec], evaluation: 0.0000[sec]
2024-05-23 16:37:29,129 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:37:29,493 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/20000.ckpt
2024-05-23 16:37:29,560 - INFO - joeynmt.training - Example #0
2024-05-23 16:37:29,560 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:37:29,560 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:37:29,560 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'de', 'so', 'dass', 'das', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'K@@', 'un@@', 'st@@', 'a@@', 'un@@', 'en', 'der', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'S@@', 'ei@@', 'te', 'der', 'L@@', 'o@@', 's@@', 'er@@', '-@@', 'St@@', 'at@@', 'en,', 'hat', 'sich', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:37:29,561 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:37:29,561 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:37:29,561 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Slide so dass das künstler die künstler der Kunstaunen der für die meisten drei Millionen Jahre die Seite der Loser-Staten, hat sich von 40 Prozent.
2024-05-23 16:37:29,561 - INFO - joeynmt.training - Example #1
2024-05-23 16:37:29,561 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:37:29,561 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:37:29,561 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'änder@@', 'ungen', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.', '</s>']
2024-05-23 16:37:29,561 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:37:29,561 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:37:29,561 - INFO - joeynmt.training - 	Hypothesis: Aber diese Veränderungen die Ernsthaft dieses speziellen Problem zu zeigen, weil es nicht die Dicksal des Eises zeigt.
2024-05-23 16:37:29,561 - INFO - joeynmt.training - Example #2
2024-05-23 16:37:29,562 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:37:29,562 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:37:29,562 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'K@@', 'li@@', 'ma@@', '-@@', 'K@@', 'li@@', 'ma@@', '-@@', 'Sy@@', 'st@@', 'em', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', '-@@', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 16:37:29,562 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:37:29,562 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:37:29,563 - INFO - joeynmt.training - 	Hypothesis: Die künstler ist in einem Sinne des Klima-Klima-System des globalen Klima-System.
2024-05-23 16:37:29,563 - INFO - joeynmt.training - Example #3
2024-05-23 16:37:29,563 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:37:29,563 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:37:29,563 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'f@@', 't.', '</s>']
2024-05-23 16:37:29,563 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:37:29,563 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:37:29,563 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Winter und Kontraft.
2024-05-23 16:37:29,563 - INFO - joeynmt.training - Example #4
2024-05-23 16:37:29,563 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:37:29,563 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:37:29,563 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zeig@@', 'en,', 'dass', 'Sie', 'eine', 'R@@', 'ei@@', 'he', 'von', 'dem', 'er@@', 'sten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:37:29,563 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:37:29,563 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:37:29,563 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schluss zeigen, dass Sie eine Reihe von dem ersten 25 Jahren passiert ist.
2024-05-23 16:37:45,094 - INFO - joeynmt.training - Epoch   5: total training loss 7280.80
2024-05-23 16:37:45,094 - INFO - joeynmt.training - EPOCH 6
2024-05-23 16:37:48,829 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.673465, Batch Acc: 0.527444, Tokens per Sec:     4010, Lr: 0.000300
2024-05-23 16:38:07,949 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.523716, Batch Acc: 0.538258, Tokens per Sec:     4112, Lr: 0.000300
2024-05-23 16:38:27,995 - INFO - joeynmt.training - Epoch   6, Step:    22800, Batch Loss:     1.631506, Batch Acc: 0.541562, Tokens per Sec:     3850, Lr: 0.000300
2024-05-23 16:38:48,759 - INFO - joeynmt.training - Epoch   6, Step:    22900, Batch Loss:     1.369350, Batch Acc: 0.541685, Tokens per Sec:     3695, Lr: 0.000300
2024-05-23 16:39:10,171 - INFO - joeynmt.training - Epoch   6, Step:    23000, Batch Loss:     1.570676, Batch Acc: 0.542611, Tokens per Sec:     3643, Lr: 0.000300
2024-05-23 16:39:10,172 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:39:10,172 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:40:39,422 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.51, generation: 89.2349[sec], evaluation: 0.0000[sec]
2024-05-23 16:40:39,752 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/21000.ckpt
2024-05-23 16:40:39,901 - INFO - joeynmt.training - Example #0
2024-05-23 16:40:39,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:40:39,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:40:39,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'F@@', 'ol@@', 'ge', 'ge@@', 'zeig@@', 't,', 'so', 'dass', 'der', 'k@@', 'ün@@', 'st@@', 'igen', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'wel@@', 'che', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'L@@', 'eh@@', 'er', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:40:39,902 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:40:39,902 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:40:39,902 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folge gezeigt, so dass der künstigen Eiskap, welche für die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Leistung der Leher von 40 Prozent.
2024-05-23 16:40:39,902 - INFO - joeynmt.training - Example #1
2024-05-23 16:40:39,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:40:39,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:40:39,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'ick@@', 's@@', 'al', 'dieses', 'bes@@', 'tim@@', 'm@@', 'tes', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'der', 'I@@', 'I@@', 'I@@', 'm', 'E@@', 'is', 'des', 'I@@', 'll@@', 's.', '</s>']
2024-05-23 16:40:39,902 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:40:39,902 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:40:39,902 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die Ernsthicksal dieses bestimmtes Problem zu zeigen, weil es den Dicksal der IIIm Eis des Ills.
2024-05-23 16:40:39,902 - INFO - joeynmt.training - Example #2
2024-05-23 16:40:39,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:40:39,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:40:39,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'ige', 'E@@', 'is', 'ist', 'K@@', 'a@@', 'p', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 16:40:39,903 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:40:39,903 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:40:39,903 - INFO - joeynmt.training - 	Hypothesis: Der künstige Eis ist Kap in einem Sinn des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klimawandels.
2024-05-23 16:40:39,903 - INFO - joeynmt.training - Example #3
2024-05-23 16:40:39,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:40:39,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:40:39,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't.', '</s>']
2024-05-23 16:40:39,903 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:40:39,903 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:40:39,903 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontracten in Winter und Kontrennt.
2024-05-23 16:40:39,903 - INFO - joeynmt.training - Example #4
2024-05-23 16:40:39,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:40:39,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:40:39,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'Z@@', 'au@@', 'ber@@', ',', 'was', 'über', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:40:39,904 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:40:39,904 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:40:39,904 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass Sie ein schnelles Zauber, was über den letzten 25 Jahren passiert ist.
2024-05-23 16:41:00,334 - INFO - joeynmt.training - Epoch   6, Step:    23100, Batch Loss:     1.628054, Batch Acc: 0.540703, Tokens per Sec:     3727, Lr: 0.000300
2024-05-23 16:41:21,529 - INFO - joeynmt.training - Epoch   6, Step:    23200, Batch Loss:     1.462431, Batch Acc: 0.537587, Tokens per Sec:     3667, Lr: 0.000300
2024-05-23 16:41:42,536 - INFO - joeynmt.training - Epoch   6, Step:    23300, Batch Loss:     1.543571, Batch Acc: 0.538441, Tokens per Sec:     3739, Lr: 0.000300
2024-05-23 16:42:02,971 - INFO - joeynmt.training - Epoch   6, Step:    23400, Batch Loss:     1.522130, Batch Acc: 0.536634, Tokens per Sec:     3818, Lr: 0.000300
2024-05-23 16:42:24,225 - INFO - joeynmt.training - Epoch   6, Step:    23500, Batch Loss:     1.361029, Batch Acc: 0.538888, Tokens per Sec:     3591, Lr: 0.000300
2024-05-23 16:42:24,226 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:42:24,226 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:43:59,379 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.51, generation: 95.1395[sec], evaluation: 0.0000[sec]
2024-05-23 16:43:59,800 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/20500.ckpt
2024-05-23 16:43:59,881 - INFO - joeynmt.training - Example #0
2024-05-23 16:43:59,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:43:59,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:43:59,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 't@@', 'z', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'K@@', 'un@@', 'st@@', 'st@@', 'a@@', 'at@@', ',', 'das', 'das', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'L@@', 'ie@@', 'f@@', 'e,', 'die', 'für', 'die', 'mei@@', 'sten', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'ie@@', 'der', 'hat', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'macht', 'hat.', '</s>']
2024-05-23 16:43:59,882 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:43:59,882 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:43:59,882 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlitz gezeigt, dass die Kunststaat, das das die künstigen der Liefe, die für die meisten 40 Prozent der Lieder hat 40 Prozent gemacht hat.
2024-05-23 16:43:59,882 - INFO - joeynmt.training - Example #1
2024-05-23 16:43:59,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:43:59,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:43:59,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'Er@@', 'st@@', 'en@@', 'lo@@', 's', 'dieses', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'zu', 'versteh@@', 'en,', 'weil', 'es', 'das', 'D@@', 'ick@@', 's@@', 'al', 'nicht', 'zeig@@', 't.', '</s>']
2024-05-23 16:43:59,883 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:43:59,883 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:43:59,883 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die Erstenlos dieses besonders Problem zu verstehen, weil es das Dicksal nicht zeigt.
2024-05-23 16:43:59,883 - INFO - joeynmt.training - Example #2
2024-05-23 16:43:59,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:43:59,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:43:59,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'st@@', 'off@@', 'e', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 16:43:59,883 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:43:59,883 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:43:59,883 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eis ist in einem Sinn, das Herz des globalen Klimaklimakstoffe System.
2024-05-23 16:43:59,883 - INFO - joeynmt.training - Example #3
2024-05-23 16:43:59,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:43:59,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:43:59,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 16:43:59,884 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:43:59,884 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:43:59,884 - INFO - joeynmt.training - 	Hypothesis: Es erwartete in Winter und Kontracten in Winter und Kontragen.
2024-05-23 16:43:59,884 - INFO - joeynmt.training - Example #4
2024-05-23 16:43:59,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:43:59,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:43:59,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'w@@', 'es@@', 'en@@', 'e', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'Z@@', 'au@@', 'ber@@', 'es', 'von', 'de@@', 'm,', 'was', 'über', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 16:43:59,884 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:43:59,884 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:43:59,884 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schwesene ich Ihnen ein schnelles Zauberes von dem, was über den letzten 25 Jahre passiert.
2024-05-23 16:44:22,856 - INFO - joeynmt.training - Epoch   6, Step:    23600, Batch Loss:     1.414220, Batch Acc: 0.538901, Tokens per Sec:     3279, Lr: 0.000300
2024-05-23 16:44:44,257 - INFO - joeynmt.training - Epoch   6, Step:    23700, Batch Loss:     1.376116, Batch Acc: 0.536314, Tokens per Sec:     3674, Lr: 0.000300
2024-05-23 16:45:04,585 - INFO - joeynmt.training - Epoch   6, Step:    23800, Batch Loss:     1.538322, Batch Acc: 0.537991, Tokens per Sec:     3718, Lr: 0.000300
2024-05-23 16:45:27,237 - INFO - joeynmt.training - Epoch   6, Step:    23900, Batch Loss:     1.591632, Batch Acc: 0.539671, Tokens per Sec:     3360, Lr: 0.000300
2024-05-23 16:45:53,650 - INFO - joeynmt.training - Epoch   6, Step:    24000, Batch Loss:     1.536798, Batch Acc: 0.532373, Tokens per Sec:     2959, Lr: 0.000300
2024-05-23 16:45:53,650 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:45:53,650 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:47:27,646 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.17, acc:   0.51, generation: 93.9814[sec], evaluation: 0.0000[sec]
2024-05-23 16:47:27,649 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:47:28,069 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/21500.ckpt
2024-05-23 16:47:28,200 - INFO - joeynmt.training - Example #0
2024-05-23 16:47:28,200 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:47:28,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:47:28,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 'ck', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'Ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'en@@', 'b@@', 'ah@@', 'n', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'eh@@', 'er', 'der', 'L@@', 'ei@@', 'st@@', 'ung', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'ei@@', 'st@@', 'ung', 'von', '4@@', '0', 'Proz@@', 'ent', 'ges@@', 'ag@@', 't', 'hat.', '</s>']
2024-05-23 16:47:28,200 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:47:28,200 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:47:28,200 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlick von 40 Prozent der Arktischen Eisenbahn der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Leher der Leistung von 40 Prozent der Leistung von 40 Prozent gesagt hat.
2024-05-23 16:47:28,200 - INFO - joeynmt.training - Example #1
2024-05-23 16:47:28,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:47:28,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:47:28,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', 'Unter@@', 'such@@', 'ungen', 'zu', 'versteh@@', 'en,', 'weil', 'es', 'das', 'Er@@', 'n@@', 's@@', 'ei@@', ',', 'weil', 'es', 'den', 'S@@', 'ie@@', 'g', 'des', 'I@@', 'st@@', 'es', 'zeig@@', 't', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'zeig@@', 't.', '</s>']
2024-05-23 16:47:28,201 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:47:28,201 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:47:28,201 - INFO - joeynmt.training - 	Hypothesis: Aber dieses Untersuchungen zu verstehen, weil es das Ernsei, weil es den Sieg des Istes zeigt nicht das Dicksal zeigt.
2024-05-23 16:47:28,201 - INFO - joeynmt.training - Example #2
2024-05-23 16:47:28,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:47:28,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:47:28,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'ti@@', 'k@@', 'er.', '</s>']
2024-05-23 16:47:28,201 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:47:28,201 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:47:28,201 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Eis ist in einem Sinn, das Herz des globalen Klimaklimaktiker.
2024-05-23 16:47:28,202 - INFO - joeynmt.training - Example #3
2024-05-23 16:47:28,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:47:28,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:47:28,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't.', '</s>']
2024-05-23 16:47:28,202 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:47:28,202 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:47:28,202 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrennt.
2024-05-23 16:47:28,202 - INFO - joeynmt.training - Example #4
2024-05-23 16:47:28,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:47:28,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:47:28,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'w@@', 'ell@@', 'en@@', 'st@@', 'a@@', 'b', 'wird', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'er', 'sein,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:47:28,202 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:47:28,202 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:47:28,202 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schwellenstab wird ein schnelles schneller sein, was über die letzten 25 Jahren passiert ist.
2024-05-23 16:47:49,854 - INFO - joeynmt.training - Epoch   6, Step:    24100, Batch Loss:     1.495436, Batch Acc: 0.535544, Tokens per Sec:     3555, Lr: 0.000300
2024-05-23 16:48:12,069 - INFO - joeynmt.training - Epoch   6, Step:    24200, Batch Loss:     1.441868, Batch Acc: 0.538720, Tokens per Sec:     3458, Lr: 0.000300
2024-05-23 16:48:32,775 - INFO - joeynmt.training - Epoch   6, Step:    24300, Batch Loss:     1.478085, Batch Acc: 0.534308, Tokens per Sec:     3716, Lr: 0.000300
2024-05-23 16:48:52,977 - INFO - joeynmt.training - Epoch   6, Step:    24400, Batch Loss:     1.637787, Batch Acc: 0.539009, Tokens per Sec:     3817, Lr: 0.000300
2024-05-23 16:49:13,501 - INFO - joeynmt.training - Epoch   6, Step:    24500, Batch Loss:     1.570178, Batch Acc: 0.541138, Tokens per Sec:     3695, Lr: 0.000300
2024-05-23 16:49:13,502 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:49:13,502 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:50:46,470 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.12, acc:   0.51, generation: 92.9538[sec], evaluation: 0.0000[sec]
2024-05-23 16:50:46,473 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:50:46,834 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/22000.ckpt
2024-05-23 16:50:46,970 - INFO - joeynmt.training - Example #0
2024-05-23 16:50:46,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:50:46,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:50:46,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'ge@@', 'zeig@@', 't', 'hab@@', 'e,', 'dass', 'die', 'Ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'ten', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 16:50:46,971 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:50:46,971 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:50:46,971 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schluss gezeigt habe, dass die Arktische Eis der letzten drei Millionen Jahre die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Leiten 40 Prozent.
2024-05-23 16:50:46,971 - INFO - joeynmt.training - Example #1
2024-05-23 16:50:46,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:50:46,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:50:46,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', 'Unter@@', 'nehmen', 'der', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'das', 'ern@@', 'st', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'I@@', 'm@@', 'es', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'e.', '</s>']
2024-05-23 16:50:46,972 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:50:46,972 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:50:46,972 - INFO - joeynmt.training - 	Hypothesis: Aber dieses Unternehmen der ernsthaft das ernst dieses speziellen Problem ist, weil es nicht die Dicksal des Imes nicht das Dicksal der Eise.
2024-05-23 16:50:46,972 - INFO - joeynmt.training - Example #2
2024-05-23 16:50:46,972 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:50:46,972 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:50:46,972 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 't@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'der', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'system@@', '.', '</s>']
2024-05-23 16:50:46,972 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:50:46,972 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:50:46,972 - INFO - joeynmt.training - 	Hypothesis: Die ktische Eis ist in einem Sinn der Klimaklimaksystem.
2024-05-23 16:50:46,972 - INFO - joeynmt.training - Example #3
2024-05-23 16:50:46,972 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:50:46,972 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:50:46,972 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 16:50:46,973 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:50:46,973 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:50:46,973 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontracten in Winter und Kontragen.
2024-05-23 16:50:46,973 - INFO - joeynmt.training - Example #4
2024-05-23 16:50:46,973 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:50:46,973 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:50:46,973 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'man', 'eine', 'ra@@', 'k@@', 'pi@@', 'de', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'von', 'de@@', 'm,', 'was', 'über', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'gesch@@', 'a@@', 'h', 'ist.', '</s>']
2024-05-23 16:50:46,973 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:50:46,973 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:50:46,973 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass man eine rakpide Fast-Kraft von dem, was über den letzten 25 Jahren geschah ist.
2024-05-23 16:51:07,881 - INFO - joeynmt.training - Epoch   6, Step:    24600, Batch Loss:     1.543654, Batch Acc: 0.533942, Tokens per Sec:     3648, Lr: 0.000300
2024-05-23 16:51:27,996 - INFO - joeynmt.training - Epoch   6, Step:    24700, Batch Loss:     1.549737, Batch Acc: 0.536284, Tokens per Sec:     3732, Lr: 0.000300
2024-05-23 16:51:48,379 - INFO - joeynmt.training - Epoch   6, Step:    24800, Batch Loss:     1.514161, Batch Acc: 0.540696, Tokens per Sec:     3733, Lr: 0.000300
2024-05-23 16:52:09,337 - INFO - joeynmt.training - Epoch   6, Step:    24900, Batch Loss:     1.505161, Batch Acc: 0.533636, Tokens per Sec:     3775, Lr: 0.000300
2024-05-23 16:52:30,331 - INFO - joeynmt.training - Epoch   6, Step:    25000, Batch Loss:     1.602474, Batch Acc: 0.538056, Tokens per Sec:     3730, Lr: 0.000300
2024-05-23 16:52:30,332 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:52:30,332 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:54:02,521 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.07, acc:   0.52, generation: 92.1751[sec], evaluation: 0.0000[sec]
2024-05-23 16:54:02,525 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:54:02,912 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/23000.ckpt
2024-05-23 16:54:02,985 - INFO - joeynmt.training - Example #0
2024-05-23 16:54:02,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:54:02,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:54:02,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Sch@@', 'li@@', 't@@', 'ten', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'K@@', 'un@@', 'st@@', 'e,', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'von', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'ei@@', 'st@@', 'un@@', 'den@@', 'schaft', 'von', '4@@', '8', 'Proz@@', 'ent', 'war.', '</s>']
2024-05-23 16:54:02,986 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:54:02,986 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:54:02,986 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Schlitten so dass die künstler die künstigen der Kunste, die für die meisten drei Millionen Jahre die meisten von der Größe der Leistundenschaft von 48 Prozent war.
2024-05-23 16:54:02,986 - INFO - joeynmt.training - Example #1
2024-05-23 16:54:02,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:54:02,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:54:02,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schie@@', 'd@@', 'liche', 'Ver@@', 'stän@@', 'd@@', 'n@@', 'is', 'dieser', 'bes@@', 'onder@@', 'e', 'Problem@@', ',', 'weil', 'es', 'das', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 16:54:02,986 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:54:02,986 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:54:02,986 - INFO - joeynmt.training - 	Hypothesis: Aber das unterschiedliche Verständnis dieser besondere Problem, weil es das nicht das Dicksal des Eiss.
2024-05-23 16:54:02,986 - INFO - joeynmt.training - Example #2
2024-05-23 16:54:02,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:54:02,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:54:02,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'system@@', '.', '</s>']
2024-05-23 16:54:02,987 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:54:02,987 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:54:02,987 - INFO - joeynmt.training - 	Hypothesis: Der künstler ist in einem Sinn des globalen Klimaklimaklimaklimaksystem.
2024-05-23 16:54:02,987 - INFO - joeynmt.training - Example #3
2024-05-23 16:54:02,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:54:02,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:54:02,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'ta@@', 'kt', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'ta@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 't.', '</s>']
2024-05-23 16:54:02,987 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:54:02,987 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:54:02,987 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakten in Winter und Kontakt in Winter und Kontakten in Winter und Kontrakt.
2024-05-23 16:54:02,987 - INFO - joeynmt.training - Example #4
2024-05-23 16:54:02,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:54:02,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:54:02,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'w@@', 'es@@', 'en@@', 'e', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'er', 'sein,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 16:54:02,988 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:54:02,988 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:54:02,988 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schwesene ich Ihnen ein schnelles schneller sein, was über die letzten 25 Jahren passiert ist.
2024-05-23 16:54:25,267 - INFO - joeynmt.training - Epoch   6, Step:    25100, Batch Loss:     1.627858, Batch Acc: 0.531726, Tokens per Sec:     3401, Lr: 0.000300
2024-05-23 16:54:46,976 - INFO - joeynmt.training - Epoch   6, Step:    25200, Batch Loss:     1.508596, Batch Acc: 0.531759, Tokens per Sec:     3500, Lr: 0.000300
2024-05-23 16:55:09,165 - INFO - joeynmt.training - Epoch   6, Step:    25300, Batch Loss:     1.552788, Batch Acc: 0.534886, Tokens per Sec:     3521, Lr: 0.000300
2024-05-23 16:55:29,735 - INFO - joeynmt.training - Epoch   6, Step:    25400, Batch Loss:     1.430182, Batch Acc: 0.539123, Tokens per Sec:     3739, Lr: 0.000300
2024-05-23 16:55:51,665 - INFO - joeynmt.training - Epoch   6, Step:    25500, Batch Loss:     1.512417, Batch Acc: 0.537577, Tokens per Sec:     3481, Lr: 0.000300
2024-05-23 16:55:51,667 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:55:51,667 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 16:57:27,023 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.62, ppl:   5.05, acc:   0.52, generation: 95.3427[sec], evaluation: 0.0000[sec]
2024-05-23 16:57:27,027 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 16:57:27,376 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/23500.ckpt
2024-05-23 16:57:27,541 - INFO - joeynmt.training - Example #0
2024-05-23 16:57:27,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 16:57:27,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 16:57:27,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 't@@', 'ten', 'so', 'dass', 'die', 'T@@', 'at', 'der', 'T@@', 'ie@@', 'f@@', 'e,', 'dass', 'die', 'die', 'mei@@', 'sten', 'von', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'ei@@', 'ter', 'der', 'L@@', 'ei@@', 'ter', 'hat', 'die', '4@@', '0', 'Proz@@', 'ent', 'ist.', '</s>']
2024-05-23 16:57:27,542 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 16:57:27,542 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 16:57:27,542 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlitten so dass die Tat der Tiefe, dass die die meisten von der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Größe der Leiter der Leiter hat die 40 Prozent ist.
2024-05-23 16:57:27,542 - INFO - joeynmt.training - Example #1
2024-05-23 16:57:27,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 16:57:27,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 16:57:27,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schie@@', 'de', 'be@@', 'fin@@', 'det', 'die', 'Er@@', 'n@@', 'ä@@', 'hr@@', 'ung', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'I@@', 'st@@', 'es', 'zeig@@', 't.', '</s>']
2024-05-23 16:57:27,542 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 16:57:27,542 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 16:57:27,542 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschiede befindet die Ernährung dieses besonders Problem, weil es nicht die Dicksal des Istes zeigt.
2024-05-23 16:57:27,543 - INFO - joeynmt.training - Example #2
2024-05-23 16:57:27,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 16:57:27,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 16:57:27,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'K@@', 'a@@', 'p', 'ist', 'die', 'K@@', 'a@@', 'p', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'B@@', 'eg@@', 'ri@@', 'f@@', 'f', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'st@@', 'off@@', '.', '</s>']
2024-05-23 16:57:27,543 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 16:57:27,543 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 16:57:27,543 - INFO - joeynmt.training - 	Hypothesis: Das Kap ist die Kap ist in einem Sinn, das Begriff des globalen Klimakstoff.
2024-05-23 16:57:27,543 - INFO - joeynmt.training - Example #3
2024-05-23 16:57:27,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 16:57:27,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 16:57:27,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te@@', 'l', '</s>']
2024-05-23 16:57:27,543 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 16:57:27,543 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 16:57:27,543 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Sommer und Kontrakte in Sommer und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakten in Winter und Kontrakten in Winter und Kontraktel
2024-05-23 16:57:27,544 - INFO - joeynmt.training - Example #4
2024-05-23 16:57:27,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 16:57:27,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 16:57:27,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'gesch@@', 'i@@', 'eh@@', 't.', '</s>']
2024-05-23 16:57:27,544 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 16:57:27,544 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 16:57:27,544 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schluss zeige, dass ich Ihnen ein schnelles schnelles von dem, was über die letzten 25 Jahren geschieht.
2024-05-23 16:57:50,579 - INFO - joeynmt.training - Epoch   6, Step:    25600, Batch Loss:     1.516439, Batch Acc: 0.534367, Tokens per Sec:     3295, Lr: 0.000300
2024-05-23 16:58:11,263 - INFO - joeynmt.training - Epoch   6, Step:    25700, Batch Loss:     1.593727, Batch Acc: 0.529997, Tokens per Sec:     3620, Lr: 0.000300
2024-05-23 16:58:33,442 - INFO - joeynmt.training - Epoch   6, Step:    25800, Batch Loss:     1.477450, Batch Acc: 0.538973, Tokens per Sec:     3451, Lr: 0.000300
2024-05-23 16:58:56,157 - INFO - joeynmt.training - Epoch   6, Step:    25900, Batch Loss:     1.257852, Batch Acc: 0.533438, Tokens per Sec:     3442, Lr: 0.000300
2024-05-23 16:59:18,903 - INFO - joeynmt.training - Epoch   6, Step:    26000, Batch Loss:     1.565403, Batch Acc: 0.536535, Tokens per Sec:     3379, Lr: 0.000300
2024-05-23 16:59:18,904 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 16:59:18,904 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:00:51,405 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.02, acc:   0.52, generation: 92.4871[sec], evaluation: 0.0000[sec]
2024-05-23 17:00:51,407 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:00:51,852 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/22500.ckpt
2024-05-23 17:00:51,959 - INFO - joeynmt.training - Example #0
2024-05-23 17:00:51,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:00:51,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:00:51,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'F@@', 'li@@', 'eg@@', 'en,', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'von', '4@@', '0', 'Proz@@', 'ent', 'ges@@', 'p@@', 'iel@@', 't', 'hat,', 'hat', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:00:51,959 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:00:51,959 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:00:51,959 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Fliegen, so dass die künstigen der Künstler der letzten drei Millionen Jahre die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Leistung von 40 Prozent gespielt hat, hat von 40 Prozent.
2024-05-23 17:00:51,959 - INFO - joeynmt.training - Example #1
2024-05-23 17:00:51,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:00:51,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:00:51,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'such@@', 't', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'I@@', 'ic@@', 's.', '</s>']
2024-05-23 17:00:51,960 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:00:51,960 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:00:51,960 - INFO - joeynmt.training - 	Hypothesis: Aber das untersucht die ernsthaft dieses besonders Problem, weil es den Dicksal nicht die Dicksal des Iics.
2024-05-23 17:00:51,960 - INFO - joeynmt.training - Example #2
2024-05-23 17:00:51,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:00:51,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:00:51,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'k@@', 'ün@@', 'st@@', 'liche', 'S@@', 'tim@@', 'm@@', 'e,', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'ß@@', 'em@@', '.', '</s>']
2024-05-23 17:00:51,960 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:00:51,960 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:00:51,960 - INFO - joeynmt.training - 	Hypothesis: Die künstliche Stimme, ist in einem Sinne des globalen Klimaklimaklima des globalen Klimaklimaklimaßem.
2024-05-23 17:00:51,960 - INFO - joeynmt.training - Example #3
2024-05-23 17:00:51,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:00:51,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:00:51,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'ver@@', 'tr@@', 'au@@', 'en.', '</s>']
2024-05-23 17:00:51,961 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:00:51,961 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:00:51,961 - INFO - joeynmt.training - 	Hypothesis: Es erwartete in Winter und Kontrakte in Sommer vertrauen.
2024-05-23 17:00:51,961 - INFO - joeynmt.training - Example #4
2024-05-23 17:00:51,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:00:51,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:00:51,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'ag@@', 'z@@', 'eich@@', 'n@@', 'ung', 'sein,', 'dass', 'ich', 'einen', 'schn@@', 'ell@@', 'er', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:00:51,961 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:00:51,961 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:00:51,961 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schlagzeichnung sein, dass ich einen schneller von der letzten 25 Jahre passiert ist.
2024-05-23 17:01:13,041 - INFO - joeynmt.training - Epoch   6, Step:    26100, Batch Loss:     1.478806, Batch Acc: 0.538553, Tokens per Sec:     3556, Lr: 0.000300
2024-05-23 17:01:33,364 - INFO - joeynmt.training - Epoch   6, Step:    26200, Batch Loss:     1.557456, Batch Acc: 0.537791, Tokens per Sec:     3742, Lr: 0.000300
2024-05-23 17:01:54,181 - INFO - joeynmt.training - Epoch   6, Step:    26300, Batch Loss:     1.408488, Batch Acc: 0.540836, Tokens per Sec:     3749, Lr: 0.000300
2024-05-23 17:02:14,133 - INFO - joeynmt.training - Epoch   6, Step:    26400, Batch Loss:     1.420481, Batch Acc: 0.534283, Tokens per Sec:     3842, Lr: 0.000300
2024-05-23 17:02:34,497 - INFO - joeynmt.training - Epoch   6, Step:    26500, Batch Loss:     1.561867, Batch Acc: 0.539251, Tokens per Sec:     3742, Lr: 0.000300
2024-05-23 17:02:34,498 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:02:34,498 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:04:16,709 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.00, acc:   0.52, generation: 102.1963[sec], evaluation: 0.0000[sec]
2024-05-23 17:04:16,713 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:04:17,103 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/24000.ckpt
2024-05-23 17:04:17,265 - INFO - joeynmt.training - Example #0
2024-05-23 17:04:17,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:04:17,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:04:17,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'ag@@', 'z@@', 'eit', 'ich', 'diesen', 'bei@@', 'den', 'S@@', 'ti@@', 'f@@', 'iz@@', 'z@@', 'ier@@', 't,', 'dass', 'der', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'ere', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:04:17,265 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:04:17,265 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:04:17,265 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlagzeit ich diesen beiden Stifizziert, dass der meisten drei Millionen Jahre die meisten der letzten drei Millionen Jahre die Größe der niedrigere 40 Prozent.
2024-05-23 17:04:17,266 - INFO - joeynmt.training - Example #1
2024-05-23 17:04:17,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:04:17,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:04:17,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'versteh@@', 'en', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'tim@@', 'm@@', 'tes', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'der', 'I@@', 'm@@', 'p@@', 'unk@@', 't', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'I@@', 'm@@', 'b@@', 's.', '</s>']
2024-05-23 17:04:17,266 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:04:17,266 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:04:17,266 - INFO - joeynmt.training - 	Hypothesis: Aber diese verstehen die Ernsthaft dieses bestimmtes Problem weil es den Dicksal der Impunkt nicht die Dicksal des Imbs.
2024-05-23 17:04:17,266 - INFO - joeynmt.training - Example #2
2024-05-23 17:04:17,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:04:17,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:04:17,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', 'e', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'al@@', 'es', 'Sy@@', 'st@@', 'em', 'an@@', 'zu@@', 'g@@', '.', '</s>']
2024-05-23 17:04:17,266 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:04:17,266 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:04:17,266 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eissche ist in einem Sinn des globalen Klimakales System anzug.
2024-05-23 17:04:17,267 - INFO - joeynmt.training - Example #3
2024-05-23 17:04:17,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:04:17,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:04:17,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ag@@', 'ung', 'in', 'W@@', 'in@@', 'ter', 'und', 'B@@', 'eh@@', 'in@@', 'der@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'B@@', 'eh@@', 'äl@@', 'ter', 'und', 'B@@', 'es@@', 'ch@@', 'r@@', 'au@@', 't.', '</s>']
2024-05-23 17:04:17,267 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:04:17,267 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:04:17,267 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Vertragung in Winter und Behindert in Winter und Kontrakte in Winter und Kontrakte in Winter und Behälter und Beschraut.
2024-05-23 17:04:17,267 - INFO - joeynmt.training - Example #4
2024-05-23 17:04:17,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:04:17,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:04:17,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:04:17,267 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:04:17,268 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:04:17,268 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schluss zeige, dass ich Ihnen ein schneller Fast-Kraft von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 17:04:38,512 - INFO - joeynmt.training - Epoch   6, Step:    26600, Batch Loss:     1.620921, Batch Acc: 0.535735, Tokens per Sec:     3590, Lr: 0.000300
2024-05-23 17:04:58,844 - INFO - joeynmt.training - Epoch   6, Step:    26700, Batch Loss:     1.643895, Batch Acc: 0.530987, Tokens per Sec:     3738, Lr: 0.000300
2024-05-23 17:05:19,335 - INFO - joeynmt.training - Epoch   6, Step:    26800, Batch Loss:     1.445343, Batch Acc: 0.527987, Tokens per Sec:     3737, Lr: 0.000300
2024-05-23 17:05:43,927 - INFO - joeynmt.training - Epoch   6, Step:    26900, Batch Loss:     1.489073, Batch Acc: 0.538396, Tokens per Sec:     3136, Lr: 0.000300
2024-05-23 17:06:10,612 - INFO - joeynmt.training - Epoch   6, Step:    27000, Batch Loss:     1.494752, Batch Acc: 0.540421, Tokens per Sec:     2814, Lr: 0.000300
2024-05-23 17:06:10,614 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:06:10,615 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:07:48,386 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.00, acc:   0.52, generation: 97.7565[sec], evaluation: 0.0000[sec]
2024-05-23 17:07:48,772 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/24500.ckpt
2024-05-23 17:07:48,900 - INFO - joeynmt.training - Example #0
2024-05-23 17:07:48,900 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:07:48,900 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:07:48,900 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'af@@', 'en', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'b@@', 't,', 'dass', 'der', 'k@@', 'ün@@', 'st@@', 'igen', 'E@@', 'is@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'ad@@', 'en,', 'die', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:07:48,901 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:07:48,901 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:07:48,901 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlafen gezeigt, dass die künstlerbt, dass der künstigen Eisiskap, die für die meisten drei Millionen Jahre die Leistaden, die von 40 Prozent.
2024-05-23 17:07:48,901 - INFO - joeynmt.training - Example #1
2024-05-23 17:07:48,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:07:48,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:07:48,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'elle', 'Pro@@', 'blem', 'weil', 'es', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's@@', 'sen', 'zeig@@', 't', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's@@', 'sen', 'zeig@@', 't.', '</s>']
2024-05-23 17:07:48,901 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:07:48,901 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:07:48,901 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht das Ernsthaft dieses spezielle Problem weil es das Dicksal des Eisssen zeigt nicht das Dicksal des Eisssen zeigt.
2024-05-23 17:07:48,901 - INFO - joeynmt.training - Example #2
2024-05-23 17:07:48,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:07:48,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:07:48,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', '-@@', 'E@@', 'is@@', 'k@@', 'a@@', 'pp@@', 'e', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 17:07:48,902 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:07:48,902 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:07:48,902 - INFO - joeynmt.training - 	Hypothesis: Der künstler-Eiskappe ist in einem Sinne des globalen Klimawandels.
2024-05-23 17:07:48,902 - INFO - joeynmt.training - Example #3
2024-05-23 17:07:48,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:07:48,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:07:48,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'S@@', 'ach@@', 'e.', '</s>']
2024-05-23 17:07:48,902 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:07:48,902 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:07:48,902 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontrakte in Sommer und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter Sache.
2024-05-23 17:07:48,902 - INFO - joeynmt.training - Example #4
2024-05-23 17:07:48,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:07:48,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:07:48,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'des', 'W@@', 'es@@', 'en@@', 's,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:07:48,903 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:07:48,903 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:07:48,903 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass Sie ein schnelles Fast-Kraft des Wesens, was über die letzten 25 Jahren passiert ist.
2024-05-23 17:08:07,997 - INFO - joeynmt.training - Epoch   6: total training loss 6982.60
2024-05-23 17:08:07,998 - INFO - joeynmt.training - EPOCH 7
2024-05-23 17:08:09,715 - INFO - joeynmt.training - Epoch   7, Step:    27100, Batch Loss:     1.382792, Batch Acc: 0.556101, Tokens per Sec:     3443, Lr: 0.000300
2024-05-23 17:08:30,309 - INFO - joeynmt.training - Epoch   7, Step:    27200, Batch Loss:     1.368819, Batch Acc: 0.552065, Tokens per Sec:     3747, Lr: 0.000300
2024-05-23 17:08:50,594 - INFO - joeynmt.training - Epoch   7, Step:    27300, Batch Loss:     1.426659, Batch Acc: 0.556736, Tokens per Sec:     3740, Lr: 0.000300
2024-05-23 17:09:10,743 - INFO - joeynmt.training - Epoch   7, Step:    27400, Batch Loss:     1.611510, Batch Acc: 0.552680, Tokens per Sec:     3803, Lr: 0.000300
2024-05-23 17:09:30,672 - INFO - joeynmt.training - Epoch   7, Step:    27500, Batch Loss:     1.477234, Batch Acc: 0.550062, Tokens per Sec:     3743, Lr: 0.000300
2024-05-23 17:09:30,673 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:09:30,673 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:11:07,873 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.01, acc:   0.52, generation: 97.1842[sec], evaluation: 0.0000[sec]
2024-05-23 17:11:08,241 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/25000.ckpt
2024-05-23 17:11:08,349 - INFO - joeynmt.training - Example #0
2024-05-23 17:11:08,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:11:08,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:11:08,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 't@@', 'ten', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'k@@', 'ün@@', 'st@@', 'igen', 'E@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'L@@', 'o@@', 's@@', '-@@', 'St@@', 'at@@', 'en,', 'hat', 'sich', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:11:08,350 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:11:08,350 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:11:08,350 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlitten so dass die künstigen der künstigen Eis der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Leistung der Los-Staten, hat sich von 40 Prozent.
2024-05-23 17:11:08,350 - INFO - joeynmt.training - Example #1
2024-05-23 17:11:08,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:11:08,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:11:08,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schei@@', 'den', 'das', 'ern@@', 'st', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'zu', 'be@@', 'wus@@', 'st', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 'ch', 'des', 'E@@', 'is@@', 'es', 'nicht', 'zeig@@', 't.', '</s>']
2024-05-23 17:11:08,350 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:11:08,350 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:11:08,351 - INFO - joeynmt.training - 	Hypothesis: Aber das unterscheiden das ernst dieses speziellen Problems zu bewusst dieses speziellen Problem, weil es nicht das Dickch des Eises nicht zeigt.
2024-05-23 17:11:08,351 - INFO - joeynmt.training - Example #2
2024-05-23 17:11:08,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:11:08,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:11:08,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is@@', 'sch@@', 'er', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'der', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'ist.', '</s>']
2024-05-23 17:11:08,351 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:11:08,351 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:11:08,351 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eisisscher ist in einem Sinn, der Klimawandel des Klimawandel des Klimawandel des Klimawandel ist.
2024-05-23 17:11:08,351 - INFO - joeynmt.training - Example #3
2024-05-23 17:11:08,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:11:08,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:11:08,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'zu', 'kon@@', 'tra@@', 'f@@', 't.', '</s>']
2024-05-23 17:11:08,351 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:11:08,351 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:11:08,352 - INFO - joeynmt.training - 	Hypothesis: Es erwartete in Winter und Kontrakte in Winter und Kontrakte zu kontraft.
2024-05-23 17:11:08,352 - INFO - joeynmt.training - Example #4
2024-05-23 17:11:08,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:11:08,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:11:08,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'A@@', 'k@@', 'tiv@@', 'ität', 'ist,', 'was', 'passi@@', 'ert', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:11:08,352 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:11:08,352 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:11:08,352 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich Ihnen ein schnelles schnelles Aktivität ist, was passiert über die letzten 25 Jahren passiert ist.
2024-05-23 17:11:28,723 - INFO - joeynmt.training - Epoch   7, Step:    27600, Batch Loss:     1.491067, Batch Acc: 0.553130, Tokens per Sec:     3624, Lr: 0.000300
2024-05-23 17:11:50,007 - INFO - joeynmt.training - Epoch   7, Step:    27700, Batch Loss:     1.466015, Batch Acc: 0.554762, Tokens per Sec:     3592, Lr: 0.000300
2024-05-23 17:12:11,768 - INFO - joeynmt.training - Epoch   7, Step:    27800, Batch Loss:     1.481271, Batch Acc: 0.555327, Tokens per Sec:     3457, Lr: 0.000300
2024-05-23 17:12:31,897 - INFO - joeynmt.training - Epoch   7, Step:    27900, Batch Loss:     1.299103, Batch Acc: 0.552777, Tokens per Sec:     3936, Lr: 0.000300
2024-05-23 17:12:51,981 - INFO - joeynmt.training - Epoch   7, Step:    28000, Batch Loss:     1.502215, Batch Acc: 0.549124, Tokens per Sec:     3791, Lr: 0.000300
2024-05-23 17:12:51,984 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:12:51,984 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:14:23,096 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   4.98, acc:   0.52, generation: 91.0976[sec], evaluation: 0.0000[sec]
2024-05-23 17:14:23,099 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:14:23,460 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/25500.ckpt
2024-05-23 17:14:23,541 - INFO - joeynmt.training - Example #0
2024-05-23 17:14:23,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:14:23,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:14:23,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'de', 'so', 'dass', 'das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is', 'zeig@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'k@@', 'ar@@', 't@@', 'ischen', 'E@@', 'is@@', 'es', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'war', 'war', 'die', 'L@@', 'um@@', 'm@@', 'er', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:14:23,542 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:14:23,542 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:14:23,542 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Slide so dass das künstliche Eisis zeigt, dass die künstigen der kartischen Eises für die meisten drei Millionen Jahre war war die Lummer 40 Prozent.
2024-05-23 17:14:23,542 - INFO - joeynmt.training - Example #1
2024-05-23 17:14:23,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:14:23,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:14:23,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'das', 'Er@@', 'n@@', 's@@', 'th@@', 'ik@@', 'an@@', 'ische', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'k@@', 'u@@', 'li@@', 'er@@', 'en,', 'die', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'e.', '</s>']
2024-05-23 17:14:23,542 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:14:23,542 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:14:23,542 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht das Ernsthikanische Probleme, weil es nicht die Dicksal des Eiskulieren, die es nicht die Dicke.
2024-05-23 17:14:23,542 - INFO - joeynmt.training - Example #2
2024-05-23 17:14:23,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:14:23,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:14:23,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is@@', 'is@@', 'er', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 17:14:23,543 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:14:23,543 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:14:23,543 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eisisiser ist in einem Sinn des globalen Klimawandels.
2024-05-23 17:14:23,543 - INFO - joeynmt.training - Example #3
2024-05-23 17:14:23,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:14:23,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:14:23,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'er@@', 'b@@', 'ung.', '</s>']
2024-05-23 17:14:23,543 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:14:23,543 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:14:23,543 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakte in Werbung.
2024-05-23 17:14:23,543 - INFO - joeynmt.training - Example #4
2024-05-23 17:14:23,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:14:23,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:14:23,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zei@@', 'gen', 'wer@@', 'de,', 'wird', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ahr@@', 'ra@@', 'pi@@', 'd', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:14:23,544 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:14:23,544 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:14:23,544 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeigen werde, wird ein schnelles Fahrrapid von dem, was über die letzten 25 Jahre passiert ist.
2024-05-23 17:14:44,416 - INFO - joeynmt.training - Epoch   7, Step:    28100, Batch Loss:     1.522276, Batch Acc: 0.553601, Tokens per Sec:     3747, Lr: 0.000300
2024-05-23 17:15:04,888 - INFO - joeynmt.training - Epoch   7, Step:    28200, Batch Loss:     1.466647, Batch Acc: 0.551686, Tokens per Sec:     3756, Lr: 0.000300
2024-05-23 17:15:25,884 - INFO - joeynmt.training - Epoch   7, Step:    28300, Batch Loss:     1.520236, Batch Acc: 0.547248, Tokens per Sec:     3581, Lr: 0.000300
2024-05-23 17:15:47,115 - INFO - joeynmt.training - Epoch   7, Step:    28400, Batch Loss:     1.323774, Batch Acc: 0.549079, Tokens per Sec:     3720, Lr: 0.000300
2024-05-23 17:16:08,693 - INFO - joeynmt.training - Epoch   7, Step:    28500, Batch Loss:     1.584132, Batch Acc: 0.550194, Tokens per Sec:     3553, Lr: 0.000300
2024-05-23 17:16:08,694 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:16:08,694 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:17:48,132 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.97, acc:   0.52, generation: 99.4243[sec], evaluation: 0.0000[sec]
2024-05-23 17:17:48,136 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:17:48,472 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/26000.ckpt
2024-05-23 17:17:48,649 - INFO - joeynmt.training - Example #0
2024-05-23 17:17:48,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:17:48,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:17:48,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'so', 'dass', 'die', 'K@@', 'ar@@', 'z@@', ',', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'ig', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'lan@@', 'g', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'M@@', 'enge', 'der', 'L@@', 'and@@', 'wer@@', 'k@@', 'er', 'der', 'L@@', 'and@@', 'k@@', 'ar@@', 'ten', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:17:48,650 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:17:48,650 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:17:48,650 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schluss so dass die Karz, dass die künstig der letzten drei Millionen Jahre lang der letzten drei Millionen Jahre die Menge der Landwerker der Landkarten 40 Prozent.
2024-05-23 17:17:48,650 - INFO - joeynmt.training - Example #1
2024-05-23 17:17:48,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:17:48,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:17:48,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ü@@', 'tz@@', 'ung', 'dieses', 'spe@@', 'zi@@', 'elle', 'Problem@@', ',', 'weil', 'es', 'das', 'D@@', 'ick@@', 's@@', 'al', 'dieses', 'T@@', 'ick@@', 's@@', 'al', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't', 'hat.', '</s>']
2024-05-23 17:17:48,651 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:17:48,651 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:17:48,651 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung dieses spezielle Problem, weil es das Dicksal dieses Ticksal nicht die Dicksal des Eises zeigt hat.
2024-05-23 17:17:48,651 - INFO - joeynmt.training - Example #2
2024-05-23 17:17:48,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:17:48,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:17:48,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is@@', 'is@@', 'en@@', 'k@@', 'ad@@', 'e,', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'system@@', 's.', '</s>']
2024-05-23 17:17:48,651 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:17:48,651 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:17:48,651 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eisisisenkade, in einem Sinn des globalen Klimaklimaksystems.
2024-05-23 17:17:48,651 - INFO - joeynmt.training - Example #3
2024-05-23 17:17:48,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:17:48,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:17:48,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'S@@', 'omm@@', 'er@@', '.', '</s>']
2024-05-23 17:17:48,652 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:17:48,652 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:17:48,652 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Winter und Kontrakte in Winter und Kontrakten in Winter und Kontrakten in Winter und Kontrakte in Winter und Kontrakten in Winter Sommer.
2024-05-23 17:17:48,652 - INFO - joeynmt.training - Example #4
2024-05-23 17:17:48,652 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:17:48,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:17:48,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'B@@', 'ild', 'zei@@', 'gen', 'wer@@', 'de,', 'wird', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'Er@@', 'st@@', 'a@@', 'un@@', 'liche', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 17:17:48,652 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:17:48,652 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:17:48,652 - INFO - joeynmt.training - 	Hypothesis: Das nächste Bild zeigen werde, wird ein schnelles schnelles Erstaunliche von dem, was über die letzten 25 Jahren passiert.
2024-05-23 17:18:11,232 - INFO - joeynmt.training - Epoch   7, Step:    28600, Batch Loss:     1.408068, Batch Acc: 0.550118, Tokens per Sec:     3366, Lr: 0.000300
2024-05-23 17:18:33,140 - INFO - joeynmt.training - Epoch   7, Step:    28700, Batch Loss:     1.418052, Batch Acc: 0.546142, Tokens per Sec:     3405, Lr: 0.000300
2024-05-23 17:18:54,748 - INFO - joeynmt.training - Epoch   7, Step:    28800, Batch Loss:     1.481737, Batch Acc: 0.549227, Tokens per Sec:     3473, Lr: 0.000300
2024-05-23 17:19:16,883 - INFO - joeynmt.training - Epoch   7, Step:    28900, Batch Loss:     1.444054, Batch Acc: 0.550680, Tokens per Sec:     3503, Lr: 0.000300
2024-05-23 17:19:38,429 - INFO - joeynmt.training - Epoch   7, Step:    29000, Batch Loss:     1.421879, Batch Acc: 0.545753, Tokens per Sec:     3529, Lr: 0.000300
2024-05-23 17:19:38,430 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:19:38,431 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:21:13,536 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.94, acc:   0.52, generation: 95.0914[sec], evaluation: 0.0000[sec]
2024-05-23 17:21:13,538 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:21:13,938 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/27500.ckpt
2024-05-23 17:21:14,010 - INFO - joeynmt.training - Example #0
2024-05-23 17:21:14,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:21:14,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:21:14,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'diese', 'zwei', 'S@@', 'li@@', 'den', 'hier', 'zeig@@', 'te,', 'dass', 'das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'war', 'der', 'Gr@@', 'öß@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'es', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'es', 'wur@@', 'de,', 'hat', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:21:14,011 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:21:14,011 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:21:14,011 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich diese zwei Sliden hier zeigte, dass das künstliche Eisis der letzten drei Millionen Jahre für die meisten drei Millionen Jahre war der Größe der niedriges der niedriges wurde, hat 40 Prozent.
2024-05-23 17:21:14,011 - INFO - joeynmt.training - Example #1
2024-05-23 17:21:14,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:21:14,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:21:14,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'such@@', 't', 'das', 'ern@@', 'st', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 'k@@', 'einen', 'D@@', 'ick@@', 'ken', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't', 'nicht', 'die', 'D@@', 'ick@@', 'k@@', 'n@@', 'ie', 'zeig@@', 't.', '</s>']
2024-05-23 17:21:14,011 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:21:14,011 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:21:14,011 - INFO - joeynmt.training - 	Hypothesis: Aber das untersucht das ernst dieses besonders Problem, weil es den Dickkeinen Dickken nicht die Dicksal des Eises zeigt nicht die Dickknie zeigt.
2024-05-23 17:21:14,011 - INFO - joeynmt.training - Example #2
2024-05-23 17:21:14,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:21:14,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:21:14,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', 'e', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'der', 'sch@@', 'ö@@', 'pf@@', 'ung', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 17:21:14,012 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:21:14,012 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:21:14,012 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eissche ist in einem Sinn, der schöpfung des globalen Klimawandels.
2024-05-23 17:21:14,012 - INFO - joeynmt.training - Example #3
2024-05-23 17:21:14,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:21:14,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:21:14,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'f@@', '.', '</s>']
2024-05-23 17:21:14,012 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:21:14,012 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:21:14,012 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakten in Winter und Kontraf.
2024-05-23 17:21:14,012 - INFO - joeynmt.training - Example #4
2024-05-23 17:21:14,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:21:14,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:21:14,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:21:14,013 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:21:14,013 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:21:14,013 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein schnelles Fast-Kraft von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 17:21:35,948 - INFO - joeynmt.training - Epoch   7, Step:    29100, Batch Loss:     1.494771, Batch Acc: 0.546179, Tokens per Sec:     3426, Lr: 0.000300
2024-05-23 17:21:57,638 - INFO - joeynmt.training - Epoch   7, Step:    29200, Batch Loss:     1.608203, Batch Acc: 0.550071, Tokens per Sec:     3621, Lr: 0.000300
2024-05-23 17:22:19,615 - INFO - joeynmt.training - Epoch   7, Step:    29300, Batch Loss:     1.486641, Batch Acc: 0.551853, Tokens per Sec:     3510, Lr: 0.000300
2024-05-23 17:22:41,835 - INFO - joeynmt.training - Epoch   7, Step:    29400, Batch Loss:     1.629529, Batch Acc: 0.551221, Tokens per Sec:     3431, Lr: 0.000300
2024-05-23 17:23:03,987 - INFO - joeynmt.training - Epoch   7, Step:    29500, Batch Loss:     1.552032, Batch Acc: 0.553372, Tokens per Sec:     3636, Lr: 0.000300
2024-05-23 17:23:03,988 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:23:03,988 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:24:46,236 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.91, acc:   0.52, generation: 102.2047[sec], evaluation: 0.0000[sec]
2024-05-23 17:24:46,246 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:24:46,878 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/27000.ckpt
2024-05-23 17:24:47,001 - INFO - joeynmt.training - Example #0
2024-05-23 17:24:47,001 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:24:47,001 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:24:47,001 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'K@@', 'un@@', 'st@@', 'e,', 'die', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'B@@', 'l@@', 'ut@@', 'gef@@', 'ä@@', 'hr@@', 'un@@', 'ken', 'hat.', '</s>']
2024-05-23 17:24:47,002 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:24:47,002 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:24:47,002 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese beiden Schluss so dass die künstigen der Kunste, die die meisten drei Millionen Jahre der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Blutgefährunken hat.
2024-05-23 17:24:47,002 - INFO - joeynmt.training - Example #1
2024-05-23 17:24:47,002 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:24:47,002 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:24:47,002 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'st@@', 'ü@@', 'tz@@', 't', 'die', 'S@@', 'er@@', 'ie', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'das', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't', 'nicht', 'zeig@@', 't.', '</s>']
2024-05-23 17:24:47,003 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:24:47,003 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:24:47,003 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterstützt die Serie dieses speziellen Problem zu zeigen, weil es das nicht die Dicksal des Eises zeigt nicht zeigt.
2024-05-23 17:24:47,003 - INFO - joeynmt.training - Example #2
2024-05-23 17:24:47,003 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:24:47,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:24:47,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'K@@', 'l@@', 'er@@', 'es@@', '-@@', 'K@@', 'l@@', 'ag@@', 's@@', 'l@@', 'ag@@', 'er', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', '-@@', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 17:24:47,003 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:24:47,003 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:24:47,003 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Kleres-Klagslager des globalen Klimaklima-System.
2024-05-23 17:24:47,003 - INFO - joeynmt.training - Example #3
2024-05-23 17:24:47,003 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:24:47,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:24:47,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 17:24:47,004 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:24:47,004 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:24:47,004 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontracten in Winter und Kontragen.
2024-05-23 17:24:47,004 - INFO - joeynmt.training - Example #4
2024-05-23 17:24:47,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:24:47,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:24:47,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'vor@@', 'wä@@', 'r@@', 'ts', 'passi@@', 'ert', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:24:47,004 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:24:47,004 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:24:47,004 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein schneller Fast-Fast-vorwärts passiert über die letzten 25 Jahren passiert ist.
2024-05-23 17:25:10,238 - INFO - joeynmt.training - Epoch   7, Step:    29600, Batch Loss:     1.478022, Batch Acc: 0.544211, Tokens per Sec:     3178, Lr: 0.000300
2024-05-23 17:25:32,146 - INFO - joeynmt.training - Epoch   7, Step:    29700, Batch Loss:     1.394346, Batch Acc: 0.549682, Tokens per Sec:     3669, Lr: 0.000300
2024-05-23 17:25:53,867 - INFO - joeynmt.training - Epoch   7, Step:    29800, Batch Loss:     1.397290, Batch Acc: 0.552088, Tokens per Sec:     3584, Lr: 0.000300
2024-05-23 17:26:15,285 - INFO - joeynmt.training - Epoch   7, Step:    29900, Batch Loss:     1.296515, Batch Acc: 0.551764, Tokens per Sec:     3492, Lr: 0.000300
2024-05-23 17:26:36,870 - INFO - joeynmt.training - Epoch   7, Step:    30000, Batch Loss:     1.459153, Batch Acc: 0.549883, Tokens per Sec:     3555, Lr: 0.000300
2024-05-23 17:26:36,871 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:26:36,871 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:28:22,954 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.89, acc:   0.53, generation: 106.0628[sec], evaluation: 0.0000[sec]
2024-05-23 17:28:22,958 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:28:23,394 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/26500.ckpt
2024-05-23 17:28:23,548 - INFO - joeynmt.training - Example #0
2024-05-23 17:28:23,549 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:28:23,549 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:28:23,549 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'ä@@', 'tz@@', 'lich', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'ige', 'der', 'K@@', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'zeig@@', 't', 'hat.', '</s>']
2024-05-23 17:28:23,550 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:28:23,550 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:28:23,550 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlätzlich gezeigt, dass die künstige der Karktischen Eiskap, die für die meisten drei Millionen Jahre die Größe 40 Prozent gezeigt hat.
2024-05-23 17:28:23,550 - INFO - joeynmt.training - Example #1
2024-05-23 17:28:23,550 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:28:23,550 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:28:23,550 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieser', 'Unter@@', 'such@@', 'ungen', 'ist', 'die', 'Er@@', 'n@@', 's@@', 'ei@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'elle', 'Pro@@', 'blem', 'weil', 'es', 'das', 'nicht', 'das', 'D@@', 'ick@@', 'iert', 'der', 'E@@', 'is@@', 'en@@', 'b@@', 'ah@@', 'n', 'zeig@@', 't.', '</s>']
2024-05-23 17:28:23,550 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:28:23,550 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:28:23,550 - INFO - joeynmt.training - 	Hypothesis: Aber dieser Untersuchungen ist die Ernseitigkeit dieses spezielle Problem weil es das nicht das Dickiert der Eisenbahn zeigt.
2024-05-23 17:28:23,550 - INFO - joeynmt.training - Example #2
2024-05-23 17:28:23,550 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:28:23,550 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:28:23,550 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'E@@', 'is@@', 'k@@', 'a@@', 'p', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'das', 'B@@', 'li@@', 'ck', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', '-@@', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 17:28:23,551 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:28:23,551 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:28:23,551 - INFO - joeynmt.training - 	Hypothesis: Der künstler Eiskap ist in einem Sinn, das das Blick des globalen Klima-System.
2024-05-23 17:28:23,551 - INFO - joeynmt.training - Example #3
2024-05-23 17:28:23,551 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:28:23,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:28:23,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tra@@', 'k@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 17:28:23,551 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:28:23,551 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:28:23,551 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Vertrakten in Winter und Vertragen.
2024-05-23 17:28:23,551 - INFO - joeynmt.training - Example #4
2024-05-23 17:28:23,551 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:28:23,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:28:23,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'K@@', 'ra@@', 'ft', 'sein,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:28:23,552 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:28:23,552 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:28:23,552 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich ein schneller Fast-Kraft sein, was über die letzten 25 Jahren passiert ist.
2024-05-23 17:28:47,107 - INFO - joeynmt.training - Epoch   7, Step:    30100, Batch Loss:     1.867724, Batch Acc: 0.551728, Tokens per Sec:     3183, Lr: 0.000300
2024-05-23 17:29:09,266 - INFO - joeynmt.training - Epoch   7, Step:    30200, Batch Loss:     1.547552, Batch Acc: 0.545611, Tokens per Sec:     3486, Lr: 0.000300
2024-05-23 17:29:31,896 - INFO - joeynmt.training - Epoch   7, Step:    30300, Batch Loss:     1.581908, Batch Acc: 0.543995, Tokens per Sec:     3386, Lr: 0.000300
2024-05-23 17:29:54,635 - INFO - joeynmt.training - Epoch   7, Step:    30400, Batch Loss:     1.570622, Batch Acc: 0.544130, Tokens per Sec:     3448, Lr: 0.000300
2024-05-23 17:30:17,526 - INFO - joeynmt.training - Epoch   7, Step:    30500, Batch Loss:     1.414297, Batch Acc: 0.545919, Tokens per Sec:     3311, Lr: 0.000300
2024-05-23 17:30:17,527 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:30:17,527 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:32:00,790 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.88, acc:   0.52, generation: 103.2477[sec], evaluation: 0.0000[sec]
2024-05-23 17:32:00,793 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:32:01,202 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/28000.ckpt
2024-05-23 17:32:01,301 - INFO - joeynmt.training - Example #0
2024-05-23 17:32:01,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:32:01,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:32:01,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'F@@', 'ol@@', 'ge', 'so', 'dass', 'das', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'sch@@', 'rei@@', 'b@@', 't,', 'dass', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'lan@@', 'g', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '8', 'Proz@@', 'ent', 'der', 'L@@', 'ö@@', 'st@@', 'k@@', 'er@@', 'b@@', 't', 'hat.', '</s>']
2024-05-23 17:32:01,301 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:32:01,301 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:32:01,301 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folge so dass das arktische Eisschreibt, dass die meisten drei Millionen Jahre lang der letzten drei Millionen Jahre die Größe 48 Prozent der Löstkerbt hat.
2024-05-23 17:32:01,301 - INFO - joeynmt.training - Example #1
2024-05-23 17:32:01,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:32:01,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:32:01,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Problem@@', 'e', 'der', 'Er@@', 'n@@', 's@@', 'ei@@', 'ts', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 'wer@@', 'k', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's@@', 'sen', 'zei@@', 't.', '</s>']
2024-05-23 17:32:01,302 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:32:01,302 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:32:01,302 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Probleme der Ernseits dieses speziellen Problem weil es den Dickwerk nicht die Dicksal des Eisssen zeit.
2024-05-23 17:32:01,302 - INFO - joeynmt.training - Example #2
2024-05-23 17:32:01,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:32:01,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:32:01,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', 'e', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'ist', 'das', 'sch@@', 'w@@', 'ach@@', 'sen@@', ';', '</s>']
2024-05-23 17:32:01,302 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:32:01,302 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:32:01,302 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eissche ist in einem Sinn des globalen Klima des globalen Klima des globalen Klima des globalen Klima ist das schwachsen;
2024-05-23 17:32:01,303 - INFO - joeynmt.training - Example #3
2024-05-23 17:32:01,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:32:01,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:32:01,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 17:32:01,303 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:32:01,303 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:32:01,303 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Winter und Kontragen.
2024-05-23 17:32:01,303 - INFO - joeynmt.training - Example #4
2024-05-23 17:32:01,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:32:01,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:32:01,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'he', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:32:01,303 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:32:01,303 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:32:01,303 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen ein Reihe von der letzten 25 Jahren passiert ist.
2024-05-23 17:32:23,557 - INFO - joeynmt.training - Epoch   7, Step:    30600, Batch Loss:     1.436234, Batch Acc: 0.545304, Tokens per Sec:     3363, Lr: 0.000300
2024-05-23 17:32:45,818 - INFO - joeynmt.training - Epoch   7, Step:    30700, Batch Loss:     1.508907, Batch Acc: 0.547471, Tokens per Sec:     3544, Lr: 0.000300
2024-05-23 17:33:07,974 - INFO - joeynmt.training - Epoch   7, Step:    30800, Batch Loss:     1.425848, Batch Acc: 0.546761, Tokens per Sec:     3500, Lr: 0.000300
2024-05-23 17:33:30,332 - INFO - joeynmt.training - Epoch   7, Step:    30900, Batch Loss:     1.674731, Batch Acc: 0.545941, Tokens per Sec:     3449, Lr: 0.000300
2024-05-23 17:33:51,978 - INFO - joeynmt.training - Epoch   7, Step:    31000, Batch Loss:     1.571026, Batch Acc: 0.547076, Tokens per Sec:     3497, Lr: 0.000300
2024-05-23 17:33:51,979 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:33:51,979 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:35:35,366 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.87, acc:   0.53, generation: 103.3730[sec], evaluation: 0.0000[sec]
2024-05-23 17:35:35,369 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:35:35,797 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/28500.ckpt
2024-05-23 17:35:35,866 - INFO - joeynmt.training - Example #0
2024-05-23 17:35:35,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:35:35,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:35:35,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'F@@', 'ol@@', 'ge', 'zeig@@', 't,', 'dass', 'D@@', 'em@@', 'on@@', 'str@@', 'at', 'dass', 'die', 'Ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'is@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahren', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'au@@', 't@@', 'om@@', 'at@@', 'ischen', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'zeig@@', 't', 'hat.', '</s>']
2024-05-23 17:35:35,867 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:35:35,867 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:35:35,867 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folge zeigt, dass Demonstrat dass die Arktische Eisisiskap, die für die meisten drei Millionen Jahren die Größe der Lautomatischen 40 Prozent gezeigt hat.
2024-05-23 17:35:35,867 - INFO - joeynmt.training - Example #1
2024-05-23 17:35:35,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:35:35,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:35:35,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schei@@', 'det', 'die', 'Er@@', 'n@@', 's@@', 'ei@@', 'ts', 'dieser', 'T@@', 'eil@@', 'chen', 'dieses', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'weil', 'es', 'das', 'E@@', 'is@@', 'en@@', 'th@@', 'al@@', 'ten', 'ist.', '</s>']
2024-05-23 17:35:35,868 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:35:35,868 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:35:35,868 - INFO - joeynmt.training - 	Hypothesis: Aber das unterscheidet die Ernseits dieser Teilchen dieses besonders Problem weil es das Eisenthalten ist.
2024-05-23 17:35:35,868 - INFO - joeynmt.training - Example #2
2024-05-23 17:35:35,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:35:35,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:35:35,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'ige', 'E@@', 'is@@', 'sch@@', 'rei@@', 'b@@', 't@@', 'em', 'S@@', 'inn@@', 'e,', 'im', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 17:35:35,868 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:35:35,868 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:35:35,868 - INFO - joeynmt.training - 	Hypothesis: Das künstige Eisschreibtem Sinne, im Sinn des globalen Klimawandels des globalen Klimawandels.
2024-05-23 17:35:35,868 - INFO - joeynmt.training - Example #3
2024-05-23 17:35:35,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:35:35,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:35:35,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'kon@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 17:35:35,869 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:35:35,869 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:35:35,869 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und kontragen.
2024-05-23 17:35:35,869 - INFO - joeynmt.training - Example #4
2024-05-23 17:35:35,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:35:35,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:35:35,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zei@@', 'gen', 'Sie', 'ein', 'R@@', 'ei@@', 'he', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:35:35,869 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:35:35,869 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:35:35,869 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen Sie ein Reihe von der letzten 25 Jahren passiert ist, was über die letzten 25 Jahren passiert ist.
2024-05-23 17:35:58,144 - INFO - joeynmt.training - Epoch   7, Step:    31100, Batch Loss:     1.629470, Batch Acc: 0.547510, Tokens per Sec:     3417, Lr: 0.000300
2024-05-23 17:36:19,764 - INFO - joeynmt.training - Epoch   7, Step:    31200, Batch Loss:     1.411716, Batch Acc: 0.546327, Tokens per Sec:     3568, Lr: 0.000300
2024-05-23 17:36:42,001 - INFO - joeynmt.training - Epoch   7, Step:    31300, Batch Loss:     1.559454, Batch Acc: 0.546987, Tokens per Sec:     3431, Lr: 0.000300
2024-05-23 17:37:06,929 - INFO - joeynmt.training - Epoch   7, Step:    31400, Batch Loss:     1.564086, Batch Acc: 0.550179, Tokens per Sec:     3141, Lr: 0.000300
2024-05-23 17:37:32,411 - INFO - joeynmt.training - Epoch   7, Step:    31500, Batch Loss:     1.416277, Batch Acc: 0.544020, Tokens per Sec:     3055, Lr: 0.000300
2024-05-23 17:37:32,411 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:37:32,412 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:39:04,625 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.85, acc:   0.53, generation: 92.1995[sec], evaluation: 0.0000[sec]
2024-05-23 17:39:04,629 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:39:04,968 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/29000.ckpt
2024-05-23 17:39:05,086 - INFO - joeynmt.training - Example #0
2024-05-23 17:39:05,086 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:39:05,086 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:39:05,086 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 't@@', 'z', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'nen', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'mei@@', 'sten', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', '4@@', '8', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:39:05,086 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:39:05,086 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:39:05,087 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlitz so dass die künstlernen gezeigt, dass die meisten der letzten drei Millionen Jahre die meisten 48 Millionen Jahre die Größe der 40 Prozent.
2024-05-23 17:39:05,087 - INFO - joeynmt.training - Example #1
2024-05-23 17:39:05,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:39:05,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:39:05,087 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', 'Unter@@', 'sch@@', 'i@@', 'ed', 'dieses', 'bes@@', 'tim@@', 'm@@', 'es', 'Pro@@', 'blem', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'zeig@@', 't.', '</s>']
2024-05-23 17:39:05,087 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:39:05,087 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:39:05,087 - INFO - joeynmt.training - 	Hypothesis: Aber dieses Unterschied dieses bestimmes Problem dieses speziellen Problem weil es das Dickte nicht die Dicksal zeigt.
2024-05-23 17:39:05,087 - INFO - joeynmt.training - Example #2
2024-05-23 17:39:05,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:39:05,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:39:05,087 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', ',', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 17:39:05,087 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:39:05,088 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:39:05,088 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eissch, in einem Sinn des globalen Klimawandels des globalen Klimawandel des globalen Klimawandels.
2024-05-23 17:39:05,088 - INFO - joeynmt.training - Example #3
2024-05-23 17:39:05,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:39:05,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:39:05,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'au@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'f@@', '.', '</s>']
2024-05-23 17:39:05,088 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:39:05,088 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:39:05,088 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und vertraut in Winter und Kontraf.
2024-05-23 17:39:05,088 - INFO - joeynmt.training - Example #4
2024-05-23 17:39:05,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:39:05,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:39:05,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'A@@', 'k@@', 'tiv@@', 'ität', 'von', 'de@@', 'm,', 'was', 'über', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:39:05,088 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:39:05,089 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:39:05,089 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich Ihnen ein schneller Aktivität von dem, was über den letzten 25 Jahren passiert ist.
2024-05-23 17:39:28,090 - INFO - joeynmt.training - Epoch   7, Step:    31600, Batch Loss:     1.461142, Batch Acc: 0.548398, Tokens per Sec:     3284, Lr: 0.000300
2024-05-23 17:39:29,636 - INFO - joeynmt.training - Epoch   7: total training loss 6787.37
2024-05-23 17:39:29,636 - INFO - joeynmt.training - EPOCH 8
2024-05-23 17:39:48,785 - INFO - joeynmt.training - Epoch   8, Step:    31700, Batch Loss:     1.342237, Batch Acc: 0.575005, Tokens per Sec:     3732, Lr: 0.000300
2024-05-23 17:40:09,566 - INFO - joeynmt.training - Epoch   8, Step:    31800, Batch Loss:     1.468058, Batch Acc: 0.564264, Tokens per Sec:     3704, Lr: 0.000300
2024-05-23 17:40:31,141 - INFO - joeynmt.training - Epoch   8, Step:    31900, Batch Loss:     1.366844, Batch Acc: 0.570716, Tokens per Sec:     3494, Lr: 0.000300
2024-05-23 17:40:53,277 - INFO - joeynmt.training - Epoch   8, Step:    32000, Batch Loss:     1.517140, Batch Acc: 0.566363, Tokens per Sec:     3481, Lr: 0.000300
2024-05-23 17:40:53,279 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:40:53,279 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:42:36,346 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.53, generation: 103.0516[sec], evaluation: 0.0000[sec]
2024-05-23 17:42:36,759 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/29500.ckpt
2024-05-23 17:42:36,858 - INFO - joeynmt.training - Example #0
2024-05-23 17:42:36,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:42:36,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:42:36,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'zeig@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'se', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'L@@', 'auf@@', 'en', 'hat.', '</s>']
2024-05-23 17:42:36,859 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:42:36,859 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:42:36,859 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schluss zeigt, dass die künktischen Eisse der letzten drei Millionen Jahre die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Leistung der Laufen hat.
2024-05-23 17:42:36,859 - INFO - joeynmt.training - Example #1
2024-05-23 17:42:36,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:42:36,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:42:36,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', 'Unter@@', 'such@@', 'ungen', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'tim@@', 'm@@', 'te', 'Pro@@', 'blem', 'der', 'bes@@', 'tim@@', 'm@@', 't', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'I@@', 'c@@', 'ke', 'zeig@@', 't.', '</s>']
2024-05-23 17:42:36,859 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:42:36,859 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:42:36,859 - INFO - joeynmt.training - 	Hypothesis: Aber dieses Untersuchungen die Ernsthaft dieses bestimmte Problem der bestimmt nicht die Dicksal des Icke zeigt.
2024-05-23 17:42:36,859 - INFO - joeynmt.training - Example #2
2024-05-23 17:42:36,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:42:36,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:42:36,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ti@@', 'k', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'im', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'ist.', '</s>']
2024-05-23 17:42:36,860 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:42:36,860 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:42:36,860 - INFO - joeynmt.training - 	Hypothesis: Der ktik ist in einem Sinn im Sinne des globalen Klimawandels des globalen Klimawandels des globalen Klimawandels ist.
2024-05-23 17:42:36,860 - INFO - joeynmt.training - Example #3
2024-05-23 17:42:36,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:42:36,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:42:36,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'au@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'au@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'au@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'ä@@', 'g@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'au@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'ä@@', 'g@@', 't.', '</s>']
2024-05-23 17:42:36,860 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:42:36,860 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:42:36,860 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und vertraut in Winter und vertraut sich in Winter und vertraut sich in Winter und verträgt sich in Winter und vertraut sich in Winter und verträgt.
2024-05-23 17:42:36,860 - INFO - joeynmt.training - Example #4
2024-05-23 17:42:36,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:42:36,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:42:36,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zei@@', 'gen', 'Sie', 'ein', 'R@@', 'a@@', 'pi@@', 'd', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:42:36,861 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:42:36,861 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:42:36,861 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeigen Sie ein Rapid von der letzten 25 Jahren passiert ist, was in den letzten 25 Jahren passiert ist.
2024-05-23 17:43:03,617 - INFO - joeynmt.training - Epoch   8, Step:    32100, Batch Loss:     1.496857, Batch Acc: 0.563860, Tokens per Sec:     2848, Lr: 0.000300
2024-05-23 17:43:26,420 - INFO - joeynmt.training - Epoch   8, Step:    32200, Batch Loss:     1.446828, Batch Acc: 0.559151, Tokens per Sec:     3317, Lr: 0.000300
2024-05-23 17:43:49,901 - INFO - joeynmt.training - Epoch   8, Step:    32300, Batch Loss:     1.549883, Batch Acc: 0.561095, Tokens per Sec:     3329, Lr: 0.000300
2024-05-23 17:44:12,095 - INFO - joeynmt.training - Epoch   8, Step:    32400, Batch Loss:     1.317461, Batch Acc: 0.561474, Tokens per Sec:     3414, Lr: 0.000300
2024-05-23 17:44:35,137 - INFO - joeynmt.training - Epoch   8, Step:    32500, Batch Loss:     1.462567, Batch Acc: 0.564705, Tokens per Sec:     3353, Lr: 0.000300
2024-05-23 17:44:35,138 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:44:35,138 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:46:14,675 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.53, generation: 99.5228[sec], evaluation: 0.0000[sec]
2024-05-23 17:46:15,160 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/30000.ckpt
2024-05-23 17:46:15,272 - INFO - joeynmt.training - Example #0
2024-05-23 17:46:15,272 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:46:15,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:46:15,272 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'K@@', 'ar@@', 'k@@', 't@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'die', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'L@@', 'age', 'von', '4@@', '0', 'Proz@@', 'ent', 'ver@@', 'lang@@', 't.', '</s>']
2024-05-23 17:46:15,272 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:46:15,272 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:46:15,272 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schluss gezeigt, dass die Karktis der letzten drei Millionen Jahre die meisten der letzten drei Millionen Jahre die meisten drei Millionen Jahre die die Leistung der Lage von 40 Prozent verlangt.
2024-05-23 17:46:15,272 - INFO - joeynmt.training - Example #1
2024-05-23 17:46:15,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:46:15,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:46:15,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schie@@', 'd@@', 'liche', 'Problem@@', 'e', 'dieses', 'Pro@@', 'blem', 'dieser', 'spe@@', 'zi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 't', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's', 'zeig@@', 't', 'hat.', '</s>']
2024-05-23 17:46:15,273 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:46:15,273 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:46:15,273 - INFO - joeynmt.training - 	Hypothesis: Aber das unterschiedliche Probleme dieses Problem dieser speziellen Problem, weil es den Dickt nicht die Dicksal des Eiss zeigt hat.
2024-05-23 17:46:15,273 - INFO - joeynmt.training - Example #2
2024-05-23 17:46:15,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:46:15,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:46:15,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'te', 'K@@', 'a@@', 'p', 'ist', 'in', 'ge@@', 'wis@@', 'ser', 'Wei@@', 'se', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'tion@@', 's@@', 'system', 'zu', 'ver@@', 'sch@@', 'm@@', 'utz@@', 'en.', '</s>']
2024-05-23 17:46:15,273 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:46:15,273 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:46:15,273 - INFO - joeynmt.training - 	Hypothesis: Das künstlerte Kap ist in gewisser Weise des globalen Klimaklimaktionssystem zu verschmutzen.
2024-05-23 17:46:15,273 - INFO - joeynmt.training - Example #3
2024-05-23 17:46:15,274 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:46:15,274 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:46:15,274 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 't.', '</s>']
2024-05-23 17:46:15,274 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:46:15,274 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:46:15,274 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Verträgt in Winter und Verträgt.
2024-05-23 17:46:15,274 - INFO - joeynmt.training - Example #4
2024-05-23 17:46:15,274 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:46:15,274 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:46:15,274 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'W@@', 'es@@', 'en', 'von', 'de@@', 'm,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:46:15,274 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:46:15,274 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:46:15,274 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein schnelles schnelles Wesen von dem, was in den letzten 25 Jahre passiert ist.
2024-05-23 17:46:38,082 - INFO - joeynmt.training - Epoch   8, Step:    32600, Batch Loss:     1.452828, Batch Acc: 0.563825, Tokens per Sec:     3229, Lr: 0.000300
2024-05-23 17:47:01,009 - INFO - joeynmt.training - Epoch   8, Step:    32700, Batch Loss:     1.523929, Batch Acc: 0.565168, Tokens per Sec:     3390, Lr: 0.000300
2024-05-23 17:47:23,286 - INFO - joeynmt.training - Epoch   8, Step:    32800, Batch Loss:     1.362767, Batch Acc: 0.559931, Tokens per Sec:     3511, Lr: 0.000300
2024-05-23 17:47:44,677 - INFO - joeynmt.training - Epoch   8, Step:    32900, Batch Loss:     1.544647, Batch Acc: 0.559894, Tokens per Sec:     3549, Lr: 0.000300
2024-05-23 17:48:06,452 - INFO - joeynmt.training - Epoch   8, Step:    33000, Batch Loss:     1.377536, Batch Acc: 0.563326, Tokens per Sec:     3621, Lr: 0.000300
2024-05-23 17:48:06,453 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:48:06,453 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:49:43,153 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.86, acc:   0.53, generation: 96.6861[sec], evaluation: 0.0000[sec]
2024-05-23 17:49:43,545 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/30500.ckpt
2024-05-23 17:49:43,695 - INFO - joeynmt.training - Example #0
2024-05-23 17:49:43,695 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:49:43,695 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:49:43,695 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'F@@', 'ol@@', 'gen@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'ge@@', 'zeig@@', 't,', 'dass', 'das', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'en@@', 'b@@', 'ah@@', 'n', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'M@@', 'enge', 'der', 'L@@', 'ei@@', 'st@@', 'ung', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'es', 'um', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 17:49:43,695 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:49:43,696 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:49:43,696 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folgenden Schluss gezeigt, dass das arktischen Eisenbahn der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Menge der Leistung der niedriges um 40 Prozent.
2024-05-23 17:49:43,696 - INFO - joeynmt.training - Example #1
2024-05-23 17:49:43,696 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:49:43,696 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:49:43,696 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schei@@', 'det', 'sich', 'das', 'ern@@', 's@@', 'th@@', 'af@@', 't@@', 'n@@', 'is', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'te', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'e.', '</s>']
2024-05-23 17:49:43,696 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:49:43,696 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:49:43,696 - INFO - joeynmt.training - 	Hypothesis: Aber das unterscheidet sich das ernsthaftnis dieses speziellen Problem weil es nicht die Dickte nicht die Dicksal der Eise.
2024-05-23 17:49:43,696 - INFO - joeynmt.training - Example #2
2024-05-23 17:49:43,696 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:49:43,696 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:49:43,696 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'sch@@', ',', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'ist.', '</s>']
2024-05-23 17:49:43,697 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:49:43,697 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:49:43,697 - INFO - joeynmt.training - 	Hypothesis: Das arktische Eissch, in einem Sinn des globalen Klimawandels des globalen Klimawandels des globalen Klimawandels ist.
2024-05-23 17:49:43,697 - INFO - joeynmt.training - Example #3
2024-05-23 17:49:43,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:49:43,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:49:43,697 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 't.', '</s>']
2024-05-23 17:49:43,697 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:49:43,697 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:49:43,697 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Sommer und Kontrakte in Sommer und Kontrakte in Sommer und Kontrakte in Sommer und Kontrakt.
2024-05-23 17:49:43,697 - INFO - joeynmt.training - Example #4
2024-05-23 17:49:43,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:49:43,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:49:43,697 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'vor@@', 'wä@@', 'r@@', 'ts', 'passi@@', 'ert', 'ist,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 17:49:43,698 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:49:43,698 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:49:43,698 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich Ihnen ein schneller Fast-Fast-vorwärts passiert ist, was in den letzten 25 Jahre passiert.
2024-05-23 17:50:06,638 - INFO - joeynmt.training - Epoch   8, Step:    33100, Batch Loss:     1.369917, Batch Acc: 0.563980, Tokens per Sec:     3282, Lr: 0.000300
2024-05-23 17:50:28,715 - INFO - joeynmt.training - Epoch   8, Step:    33200, Batch Loss:     1.299941, Batch Acc: 0.562509, Tokens per Sec:     3577, Lr: 0.000300
2024-05-23 17:50:50,530 - INFO - joeynmt.training - Epoch   8, Step:    33300, Batch Loss:     1.485218, Batch Acc: 0.559093, Tokens per Sec:     3471, Lr: 0.000300
2024-05-23 17:51:13,112 - INFO - joeynmt.training - Epoch   8, Step:    33400, Batch Loss:     1.493943, Batch Acc: 0.559414, Tokens per Sec:     3382, Lr: 0.000300
2024-05-23 17:51:40,383 - INFO - joeynmt.training - Epoch   8, Step:    33500, Batch Loss:     1.471496, Batch Acc: 0.560465, Tokens per Sec:     2854, Lr: 0.000300
2024-05-23 17:51:40,384 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:51:40,384 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:53:27,973 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.84, acc:   0.53, generation: 107.5739[sec], evaluation: 0.0000[sec]
2024-05-23 17:53:27,977 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:53:28,356 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/31000.ckpt
2024-05-23 17:53:28,491 - INFO - joeynmt.training - Example #0
2024-05-23 17:53:28,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:53:28,492 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:53:28,492 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 'mm@@', 'ern@@', ',', 'so', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'ok@@', ',', 'die', 'am', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'ei@@', 'st@@', 'ung', 'von', '4@@', '0', 'Proz@@', 'ent', 'war.', '</s>']
2024-05-23 17:53:28,492 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:53:28,492 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:53:28,492 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlimmern, so dass die arktischen Eisschok, die am meisten drei Millionen Jahre für die meisten drei Millionen Jahre die Größe der Leistung von 40 Prozent war.
2024-05-23 17:53:28,492 - INFO - joeynmt.training - Example #1
2024-05-23 17:53:28,492 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:53:28,492 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:53:28,492 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Ver@@', 'stän@@', 'd@@', 'n@@', 'is', 'dieses', 'bes@@', 'onder@@', 's', 'Pro@@', 'blem', 'weil', 'es', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'ist.', '</s>']
2024-05-23 17:53:28,492 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:53:28,493 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:53:28,493 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Verständnis dieses besonders Problem weil es das Dickte nicht das Dicksal ist.
2024-05-23 17:53:28,493 - INFO - joeynmt.training - Example #2
2024-05-23 17:53:28,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:53:28,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:53:28,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'K@@', 'et@@', 'te', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 17:53:28,493 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:53:28,493 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:53:28,493 - INFO - joeynmt.training - 	Hypothesis: Der künstler Kette ist in einem Sinn des globalen Klimawandels des globalen Klimawandels.
2024-05-23 17:53:28,493 - INFO - joeynmt.training - Example #3
2024-05-23 17:53:28,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:53:28,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:53:28,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'zu', 'kon@@', 'tra@@', 'k@@', 'en.', '</s>']
2024-05-23 17:53:28,493 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:53:28,494 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:53:28,494 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrennt in Winter und Kontrakten in Sommer Sommer zu kontraken.
2024-05-23 17:53:28,494 - INFO - joeynmt.training - Example #4
2024-05-23 17:53:28,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:53:28,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:53:28,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'vor@@', 'wä@@', 'r@@', 'ts', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 17:53:28,494 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:53:28,494 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:53:28,494 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich Ihnen ein schnelles Fast-vorwärts von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 17:53:51,132 - INFO - joeynmt.training - Epoch   8, Step:    33600, Batch Loss:     1.377937, Batch Acc: 0.557986, Tokens per Sec:     3370, Lr: 0.000300
2024-05-23 17:54:13,251 - INFO - joeynmt.training - Epoch   8, Step:    33700, Batch Loss:     1.403664, Batch Acc: 0.555874, Tokens per Sec:     3520, Lr: 0.000300
2024-05-23 17:54:35,419 - INFO - joeynmt.training - Epoch   8, Step:    33800, Batch Loss:     1.500924, Batch Acc: 0.556232, Tokens per Sec:     3422, Lr: 0.000300
2024-05-23 17:54:57,256 - INFO - joeynmt.training - Epoch   8, Step:    33900, Batch Loss:     1.424641, Batch Acc: 0.558025, Tokens per Sec:     3575, Lr: 0.000300
2024-05-23 17:55:19,472 - INFO - joeynmt.training - Epoch   8, Step:    34000, Batch Loss:     1.551990, Batch Acc: 0.559563, Tokens per Sec:     3457, Lr: 0.000300
2024-05-23 17:55:19,473 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:55:19,473 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 17:56:50,305 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.82, acc:   0.53, generation: 90.8177[sec], evaluation: 0.0000[sec]
2024-05-23 17:56:50,307 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 17:56:50,658 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/32000.ckpt
2024-05-23 17:56:50,801 - INFO - joeynmt.training - Example #0
2024-05-23 17:56:50,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 17:56:50,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 17:56:50,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'S@@', 'li@@', 'den', 'so', 'dass', 'es', 'de@@', 'mon@@', 'stri@@', 'er@@', 'en,', 'dass', 'der', 'k@@', 'ün@@', 'st@@', 'ig@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'B@@', 'r@@', 'ü@@', 'c@@', 'ke', 'von', '4@@', '0', 'Proz@@', 'ent', 'ver@@', 'sch@@', 'r@@', 'eit@@', 'en.', '</s>']
2024-05-23 17:56:50,802 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 17:56:50,802 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 17:56:50,802 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Sliden so dass es demonstrieren, dass der künstigsten drei Millionen Jahre die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Brücke von 40 Prozent verschreiten.
2024-05-23 17:56:50,802 - INFO - joeynmt.training - Example #1
2024-05-23 17:56:50,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 17:56:50,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 17:56:50,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'such@@', 't', 'die', 'Er@@', 'n@@', 'en@@', 'nen', 'dieser', 'spe@@', 'zi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'das', 'Problem@@', ',', 'den', 'Sie', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.', '</s>']
2024-05-23 17:56:50,803 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 17:56:50,803 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 17:56:50,803 - INFO - joeynmt.training - 	Hypothesis: Aber das untersucht die Ernennen dieser speziellen Problem, weil es das Problem, den Sie nicht das Dicksal der Eis des Eises zeigt.
2024-05-23 17:56:50,803 - INFO - joeynmt.training - Example #2
2024-05-23 17:56:50,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 17:56:50,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 17:56:50,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'K@@', 'a@@', 'pp@@', 'e', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'Sch@@', 'l@@', 'us@@', 's@@', '-@@', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'er@@', 'ungs@@', 'system@@', '.', '</s>']
2024-05-23 17:56:50,803 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 17:56:50,803 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 17:56:50,803 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Kappe ist in einem Sinn, das Schluss-Klimaklimaklimaerungssystem.
2024-05-23 17:56:50,803 - INFO - joeynmt.training - Example #3
2024-05-23 17:56:50,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 17:56:50,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 17:56:50,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't@@', 'ete', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'enn@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ag@@', 'en@@', 'k@@', 'en.', '</s>']
2024-05-23 17:56:50,804 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 17:56:50,804 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 17:56:50,804 - INFO - joeynmt.training - 	Hypothesis: Es erwartete in Winter und Kontrennt in Winter und Kontragenken.
2024-05-23 17:56:50,804 - INFO - joeynmt.training - Example #4
2024-05-23 17:56:50,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 17:56:50,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 17:56:50,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'von', 'der', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 17:56:50,804 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 17:56:50,804 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 17:56:50,804 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass Sie ein schnelles schnelles von der letzten 25 Jahren passiert.
2024-05-23 17:57:12,619 - INFO - joeynmt.training - Epoch   8, Step:    34100, Batch Loss:     1.393887, Batch Acc: 0.553503, Tokens per Sec:     3440, Lr: 0.000300
2024-05-23 17:57:38,942 - INFO - joeynmt.training - Epoch   8, Step:    34200, Batch Loss:     1.458188, Batch Acc: 0.558619, Tokens per Sec:     3023, Lr: 0.000300
2024-05-23 17:58:00,869 - INFO - joeynmt.training - Epoch   8, Step:    34300, Batch Loss:     1.395927, Batch Acc: 0.561000, Tokens per Sec:     3440, Lr: 0.000300
2024-05-23 17:58:23,132 - INFO - joeynmt.training - Epoch   8, Step:    34400, Batch Loss:     1.552136, Batch Acc: 0.557286, Tokens per Sec:     3484, Lr: 0.000300
2024-05-23 17:58:45,258 - INFO - joeynmt.training - Epoch   8, Step:    34500, Batch Loss:     1.710781, Batch Acc: 0.557512, Tokens per Sec:     3463, Lr: 0.000300
2024-05-23 17:58:45,260 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 17:58:45,260 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:00:28,430 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.81, acc:   0.53, generation: 103.1562[sec], evaluation: 0.0000[sec]
2024-05-23 18:00:28,435 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:00:28,889 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/32500.ckpt
2024-05-23 18:00:29,082 - INFO - joeynmt.training - Example #0
2024-05-23 18:00:29,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:00:29,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:00:29,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Sch@@', 'li@@', 'mm@@', 'el@@', ',', 'dass', 'das', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'en@@', 'z', 'der', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'St@@', 'at@@', 'es', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'hat@@', 'te.', '</s>']
2024-05-23 18:00:29,083 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:00:29,083 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:00:29,083 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Schlimmel, dass das ktische Eisenz der ktischen Eiskap, die für die meisten drei Millionen Jahre die meisten drei Millionen Jahre die States der niedriger hatte.
2024-05-23 18:00:29,083 - INFO - joeynmt.training - Example #1
2024-05-23 18:00:29,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:00:29,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:00:29,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter', 'den', 'St@@', 'at@@', 'en,', 'die', 'das', 'Er@@', 'n@@', 'ste', 'dieses', 'spe@@', 'zi@@', 'elle', 'Pro@@', 'blem', 'weil', 'es', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'die', 'D@@', 'ick@@', 'n@@', 'er', 'zeig@@', 't', 'wird.', '</s>']
2024-05-23 18:00:29,084 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:00:29,084 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:00:29,084 - INFO - joeynmt.training - 	Hypothesis: Aber diese unter den Staten, die das Ernste dieses spezielle Problem weil es das Dickte nicht die Dickner zeigt wird.
2024-05-23 18:00:29,084 - INFO - joeynmt.training - Example #2
2024-05-23 18:00:29,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:00:29,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:00:29,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 't@@', 'isch@@', 'er', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'inn@@', 'e,', 'der', 'L@@', 'ei@@', 'd,', 'das', 'L@@', 'ei@@', 'd', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 18:00:29,084 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:00:29,084 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:00:29,084 - INFO - joeynmt.training - 	Hypothesis: Der ktischer Eis ist in einem Sinne, der Leid, das Leid des globalen Klimawandels.
2024-05-23 18:00:29,084 - INFO - joeynmt.training - Example #3
2024-05-23 18:00:29,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:00:29,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:00:29,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'B@@', 'au@@', 'm@@', 'er', 'in', 'W@@', 'in@@', 'ter', 'und', 'B@@', 'in@@', 'tra@@', 'k@@', 'te', 'und', 'B@@', 'au@@', 'st@@', 'er@@', 't.', '</s>']
2024-05-23 18:00:29,085 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:00:29,085 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:00:29,085 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Winter und Baumer in Winter und Bintrakte und Baustert.
2024-05-23 18:00:29,085 - INFO - joeynmt.training - Example #4
2024-05-23 18:00:29,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:00:29,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:00:29,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'K@@', 'rie@@', 'g', 'des', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:00:29,085 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:00:29,085 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:00:29,085 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein schneller Fast-Fast-Krieg des letzten 25 Jahre passiert ist.
2024-05-23 18:00:51,495 - INFO - joeynmt.training - Epoch   8, Step:    34600, Batch Loss:     1.483669, Batch Acc: 0.559137, Tokens per Sec:     3381, Lr: 0.000300
2024-05-23 18:01:14,626 - INFO - joeynmt.training - Epoch   8, Step:    34700, Batch Loss:     1.439870, Batch Acc: 0.557182, Tokens per Sec:     3340, Lr: 0.000300
2024-05-23 18:01:38,716 - INFO - joeynmt.training - Epoch   8, Step:    34800, Batch Loss:     1.511214, Batch Acc: 0.559107, Tokens per Sec:     3179, Lr: 0.000300
2024-05-23 18:02:02,034 - INFO - joeynmt.training - Epoch   8, Step:    34900, Batch Loss:     1.626017, Batch Acc: 0.557526, Tokens per Sec:     3347, Lr: 0.000300
2024-05-23 18:02:24,620 - INFO - joeynmt.training - Epoch   8, Step:    35000, Batch Loss:     1.434721, Batch Acc: 0.560277, Tokens per Sec:     3411, Lr: 0.000300
2024-05-23 18:02:24,621 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:02:24,621 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:04:13,146 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.77, acc:   0.53, generation: 108.5110[sec], evaluation: 0.0000[sec]
2024-05-23 18:04:13,149 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:04:13,650 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/33000.ckpt
2024-05-23 18:04:13,761 - INFO - joeynmt.training - Example #0
2024-05-23 18:04:13,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:04:13,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:04:13,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Sch@@', 'l@@', 'ag@@', 'z@@', 'eit', 'ich', 'diese', 'zwei', 'Sch@@', 'l@@', 'ag@@', 'e,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'das', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'ad@@', 'en,', 'die', 'von', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'age', 'von', '4@@', '0', 'Proz@@', 'ent', 'ge@@', 'zeig@@', 't', 'hat.', '</s>']
2024-05-23 18:04:13,762 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:04:13,762 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:04:13,762 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Schlagzeit ich diese zwei Schlage, dass die arktische Eiskap, das für die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Leistaden, die von 40 Prozent der Lage von 40 Prozent gezeigt hat.
2024-05-23 18:04:13,762 - INFO - joeynmt.training - Example #1
2024-05-23 18:04:13,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:04:13,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:04:13,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'such@@', 't', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'das', 'D@@', 'ick@@', 'te', 'des', 'E@@', 'is@@', 'is@@', 'en@@', 'd.', '</s>']
2024-05-23 18:04:13,763 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:04:13,763 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:04:13,763 - INFO - joeynmt.training - 	Hypothesis: Aber diese untersucht die Ernsthaft dieses besonders Problem, weil es das Dickte nicht das Dickte des Eisisend.
2024-05-23 18:04:13,763 - INFO - joeynmt.training - Example #2
2024-05-23 18:04:13,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:04:13,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:04:13,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'k@@', 'a@@', 'pp@@', 'e', 'ist', 'in', 'ge@@', 'wis@@', 'se', 'S@@', 'inn@@', 'e,', 'der', 'Sch@@', 'ö@@', 'pf@@', 'ung', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@', 'ma@@', 'wan@@', 'de@@', 'l', 'des', 'K@@', 'li@@']
2024-05-23 18:04:13,763 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:04:13,763 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:04:13,763 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eiskappe ist in gewisse Sinne, der Schöpfung des globalen Klimawandel des Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des Klimawandel des Klimawandel des Klimawandel des Klimawandel des Klimawandel des Kli
2024-05-23 18:04:13,763 - INFO - joeynmt.training - Example #3
2024-05-23 18:04:13,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:04:13,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:04:13,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'in', 'W@@', 'in@@', 'ter', 'und', 'in', 'S@@', 'omm@@', 'er', 'zu', 'ver@@', 'tr@@', 'au@@', 'en.', '</s>']
2024-05-23 18:04:13,764 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:04:13,764 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:04:13,764 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Winter und in Winter und in Sommer zu vertrauen.
2024-05-23 18:04:13,764 - INFO - joeynmt.training - Example #4
2024-05-23 18:04:13,764 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:04:13,764 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:04:13,764 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'Vor@@', 'wä@@', 'r@@', 't@@', 'eil', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:04:13,764 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:04:13,764 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:04:13,764 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich Ihnen ein schneller Fast-Vorwärteil von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 18:04:35,722 - INFO - joeynmt.training - Epoch   8, Step:    35100, Batch Loss:     1.541620, Batch Acc: 0.552502, Tokens per Sec:     3403, Lr: 0.000300
2024-05-23 18:04:56,984 - INFO - joeynmt.training - Epoch   8, Step:    35200, Batch Loss:     1.494299, Batch Acc: 0.553869, Tokens per Sec:     3644, Lr: 0.000300
2024-05-23 18:05:19,360 - INFO - joeynmt.training - Epoch   8, Step:    35300, Batch Loss:     1.495624, Batch Acc: 0.556680, Tokens per Sec:     3392, Lr: 0.000300
2024-05-23 18:05:40,855 - INFO - joeynmt.training - Epoch   8, Step:    35400, Batch Loss:     1.342511, Batch Acc: 0.562753, Tokens per Sec:     3568, Lr: 0.000300
2024-05-23 18:06:02,267 - INFO - joeynmt.training - Epoch   8, Step:    35500, Batch Loss:     1.414037, Batch Acc: 0.556543, Tokens per Sec:     3537, Lr: 0.000300
2024-05-23 18:06:02,268 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:06:02,268 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:07:41,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.76, acc:   0.53, generation: 99.1116[sec], evaluation: 0.0000[sec]
2024-05-23 18:07:41,398 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:07:41,776 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/31500.ckpt
2024-05-23 18:07:41,851 - INFO - joeynmt.training - Example #0
2024-05-23 18:07:41,851 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:07:41,851 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:07:41,851 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'ag@@', ',', 'dass', 'das', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'B@@', 'l@@', 'um@@', 'e', 'der', 'L@@', 'auf@@', 'en', 'ge@@', 'zeig@@', 't', 'hat,', 'hat', 'sich', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:07:41,852 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:07:41,852 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:07:41,852 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlag, dass das künstler die künstler der letzten drei Millionen Jahre die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Blume der Laufen gezeigt hat, hat sich von 40 Prozent.
2024-05-23 18:07:41,852 - INFO - joeynmt.training - Example #1
2024-05-23 18:07:41,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:07:41,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:07:41,852 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Aus@@', 'g@@', 'ang@@', 's@@', 'ei@@', 'te', 'dieses', 'spe@@', 'zi@@', 'elle', 'Pro@@', 'blem', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'das', 'D@@', 'ick@@', 'e.', '</s>']
2024-05-23 18:07:41,852 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:07:41,852 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:07:41,852 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Ausgangseite dieses spezielle Problem weil es nicht das Dickte nicht das Dickte nicht das Dicke.
2024-05-23 18:07:41,852 - INFO - joeynmt.training - Example #2
2024-05-23 18:07:41,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:07:41,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:07:41,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', 'e', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 18:07:41,853 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:07:41,853 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:07:41,853 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eissche ist in einem Sinn des globalen Klimawandels des globalen Klimawandels des Klimawandels des globalen Klimawandels des globalen Klimawandels.
2024-05-23 18:07:41,853 - INFO - joeynmt.training - Example #3
2024-05-23 18:07:41,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:07:41,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:07:41,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 't', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tr@@', 'ac@@', 'ten', 'er@@', 'war@@', 'ten.', '</s>']
2024-05-23 18:07:41,853 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:07:41,853 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:07:41,853 - INFO - joeynmt.training - 	Hypothesis: Es erwart sich in Winter und Kontracten in Winter und Kontracten erwarten.
2024-05-23 18:07:41,853 - INFO - joeynmt.training - Example #4
2024-05-23 18:07:41,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:07:41,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:07:41,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'Sch@@', 'n@@', 'ell@@', '-@@', 'Vor@@', 'stell@@', 'ung', 'von', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ahr@@', 'ung', 'von', 'was', 'passi@@', 'ert', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:07:41,854 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:07:41,854 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:07:41,854 - INFO - joeynmt.training - 	Hypothesis: Die nächste Schnell-Vorstellung von Ihnen ein schneller Fahrung von was passiert über die letzten 25 Jahre passiert ist.
2024-05-23 18:08:04,260 - INFO - joeynmt.training - Epoch   8, Step:    35600, Batch Loss:     1.297585, Batch Acc: 0.550121, Tokens per Sec:     3378, Lr: 0.000300
2024-05-23 18:08:28,303 - INFO - joeynmt.training - Epoch   8, Step:    35700, Batch Loss:     1.608367, Batch Acc: 0.555110, Tokens per Sec:     3244, Lr: 0.000300
2024-05-23 18:08:50,871 - INFO - joeynmt.training - Epoch   8, Step:    35800, Batch Loss:     1.482529, Batch Acc: 0.551388, Tokens per Sec:     3464, Lr: 0.000300
2024-05-23 18:09:12,620 - INFO - joeynmt.training - Epoch   8, Step:    35900, Batch Loss:     1.513804, Batch Acc: 0.554712, Tokens per Sec:     3457, Lr: 0.000300
2024-05-23 18:09:34,916 - INFO - joeynmt.training - Epoch   8, Step:    36000, Batch Loss:     1.391856, Batch Acc: 0.554035, Tokens per Sec:     3461, Lr: 0.000300
2024-05-23 18:09:34,916 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:09:34,916 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:11:03,589 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.75, acc:   0.53, generation: 88.6589[sec], evaluation: 0.0000[sec]
2024-05-23 18:11:03,592 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:11:03,943 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/33500.ckpt
2024-05-23 18:11:04,099 - INFO - joeynmt.training - Example #0
2024-05-23 18:11:04,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:11:04,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:11:04,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'n@@', 'it@@', 't@@', 'stell@@', 't,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'is@@', 'is@@', 'k@@', 'ap@@', ',', 'das', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'ei@@', 'st@@', 'en@@', 'st@@', 'a@@', 'at@@', 'en,', 'hat', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:11:04,099 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:11:04,100 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:11:04,100 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schnittstellt, dass die arktischen Eisisiskap, das für die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Leistenstaaten, hat von 40 Prozent.
2024-05-23 18:11:04,100 - INFO - joeynmt.training - Example #1
2024-05-23 18:11:04,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:11:04,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:11:04,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schie@@', 'de', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'tim@@', 'm@@', 'es', 'Problem@@', ',', 'weil', 'es', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 18:11:04,100 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:11:04,100 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:11:04,100 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschiede die Ernsthaft dieses bestimmes Problem, weil es das Dicksal des Eiss.
2024-05-23 18:11:04,100 - INFO - joeynmt.training - Example #2
2024-05-23 18:11:04,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:11:04,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:11:04,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is@@', 'sch@@', 'ä@@', 'tz@@', 't', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'k@@', 'lim@@', 'ischen', 'K@@', 'lim@@', 'a', 'des', 'k@@', 'un@@', 'st@@', 's.', '</s>']
2024-05-23 18:11:04,101 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:11:04,101 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:11:04,101 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eisisschätzt in einem Sinn des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des klimischen Klima des kunsts.
2024-05-23 18:11:04,101 - INFO - joeynmt.training - Example #3
2024-05-23 18:11:04,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:11:04,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:11:04,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'im', 'S@@', 'omm@@', 'er', 'im', 'S@@', 'omm@@', 'er', 'im', 'S@@', 'omm@@', 'er', 'im', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-23 18:11:04,101 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:11:04,101 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:11:04,101 - INFO - joeynmt.training - 	Hypothesis: Es erwartet im Sommer im Sommer im Sommer im Sommer.
2024-05-23 18:11:04,101 - INFO - joeynmt.training - Example #4
2024-05-23 18:11:04,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:11:04,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:11:04,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'werde', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'he', 'von', 'dem', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 'te', 'ist,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 18:11:04,102 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:11:04,102 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:11:04,102 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schluss werde ich Ihnen ein Reihe von dem was über die letzten 25 Jahren passierte ist, was über die letzten 25 Jahren passiert.
2024-05-23 18:11:24,608 - INFO - joeynmt.training - Epoch   8, Step:    36100, Batch Loss:     1.325982, Batch Acc: 0.561098, Tokens per Sec:     3714, Lr: 0.000300
2024-05-23 18:11:28,487 - INFO - joeynmt.training - Epoch   8: total training loss 6613.10
2024-05-23 18:11:28,487 - INFO - joeynmt.training - EPOCH 9
2024-05-23 18:11:45,908 - INFO - joeynmt.training - Epoch   9, Step:    36200, Batch Loss:     1.419046, Batch Acc: 0.579877, Tokens per Sec:     3635, Lr: 0.000300
2024-05-23 18:12:07,859 - INFO - joeynmt.training - Epoch   9, Step:    36300, Batch Loss:     1.573693, Batch Acc: 0.575825, Tokens per Sec:     3649, Lr: 0.000300
2024-05-23 18:12:30,731 - INFO - joeynmt.training - Epoch   9, Step:    36400, Batch Loss:     1.426434, Batch Acc: 0.576628, Tokens per Sec:     3403, Lr: 0.000300
2024-05-23 18:12:52,317 - INFO - joeynmt.training - Epoch   9, Step:    36500, Batch Loss:     1.475360, Batch Acc: 0.579853, Tokens per Sec:     3585, Lr: 0.000300
2024-05-23 18:12:52,318 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:12:52,318 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:14:28,135 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.77, acc:   0.54, generation: 95.8013[sec], evaluation: 0.0000[sec]
2024-05-23 18:14:28,544 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/34000.ckpt
2024-05-23 18:14:28,679 - INFO - joeynmt.training - Example #0
2024-05-23 18:14:28,679 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:14:28,679 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:14:28,679 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Sch@@', 'l@@', 'us@@', 's', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'E@@', 'is@@', 'sch@@', 'ap@@', ',', 'die', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'es', 'der', 'L@@', 'age', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:14:28,680 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:14:28,680 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:14:28,680 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Schluss gezeigt, dass die künstigen Eisschap, die für die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Größe der niedriges der Lage von 40 Prozent.
2024-05-23 18:14:28,680 - INFO - joeynmt.training - Example #1
2024-05-23 18:14:28,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:14:28,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:14:28,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schie@@', 'de', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'elle', 'Pro@@', 'blem', 'weil', 'es', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.', '</s>']
2024-05-23 18:14:28,680 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:14:28,680 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:14:28,680 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschiede die ernsthaft dieses spezielle Problem weil es das Dickte nicht das Dicksal des Eises zeigt.
2024-05-23 18:14:28,680 - INFO - joeynmt.training - Example #2
2024-05-23 18:14:28,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:14:28,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:14:28,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'lich', 'ist', 'das', 'Sch@@', 'wi@@', 'es@@', 'o', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'Sch@@', 'l@@', 'af@@', 'e', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'es@@', 'en@@', 'system@@', 's.', '</s>']
2024-05-23 18:14:28,681 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:14:28,681 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:14:28,681 - INFO - joeynmt.training - 	Hypothesis: Das künstlich ist das Schwieso ist in einem Sinn, das Schlafe des globalen Klimawesensystems.
2024-05-23 18:14:28,681 - INFO - joeynmt.training - Example #3
2024-05-23 18:14:28,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:14:28,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:14:28,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'im', 'S@@', 'omm@@', 'er', 'im', 'S@@', 'omm@@', 'er', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tr@@', 'ä@@', 'g@@', 't.', '</s>']
2024-05-23 18:14:28,681 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:14:28,681 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:14:28,681 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verträgung im Sommer im Sommer im Sommer und Konträgt.
2024-05-23 18:14:28,681 - INFO - joeynmt.training - Example #4
2024-05-23 18:14:28,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:14:28,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:14:28,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Sch@@', 'l@@', 'us@@', 's', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'Vor@@', 'stell@@', 'ung', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:14:28,682 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:14:28,682 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:14:28,682 - INFO - joeynmt.training - 	Hypothesis: Das nächste Schluss zeige, dass Sie ein schnelles Fast-Vorstellung von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 18:14:50,884 - INFO - joeynmt.training - Epoch   9, Step:    36600, Batch Loss:     1.362844, Batch Acc: 0.575309, Tokens per Sec:     3420, Lr: 0.000300
2024-05-23 18:15:12,465 - INFO - joeynmt.training - Epoch   9, Step:    36700, Batch Loss:     1.352000, Batch Acc: 0.577914, Tokens per Sec:     3587, Lr: 0.000300
2024-05-23 18:15:33,703 - INFO - joeynmt.training - Epoch   9, Step:    36800, Batch Loss:     1.256725, Batch Acc: 0.573115, Tokens per Sec:     3741, Lr: 0.000300
2024-05-23 18:15:55,833 - INFO - joeynmt.training - Epoch   9, Step:    36900, Batch Loss:     1.448575, Batch Acc: 0.568446, Tokens per Sec:     3401, Lr: 0.000300
2024-05-23 18:16:17,777 - INFO - joeynmt.training - Epoch   9, Step:    37000, Batch Loss:     1.531095, Batch Acc: 0.568027, Tokens per Sec:     3532, Lr: 0.000300
2024-05-23 18:16:17,779 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:16:17,779 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:17:53,333 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.77, acc:   0.53, generation: 95.5401[sec], evaluation: 0.0000[sec]
2024-05-23 18:17:53,740 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/34500.ckpt
2024-05-23 18:17:53,882 - INFO - joeynmt.training - Example #0
2024-05-23 18:17:53,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:17:53,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:17:53,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'habe', 'diese', 'zwei', 'Di@@', 'as@@', 's', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'E@@', 'is@@', 'en@@', '-@@', 'E@@', 'is@@', 'en@@', '-@@', 'E@@', 'is@@', 'en@@', '-@@', 'E@@', 'is@@', 'en@@', '-@@', 'E@@', 'is@@', 'en@@', '-@@', 'F@@', 'l@@', 'ü@@', 'gel@@', 'er', 'die', 'größ@@', 'te', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '4@@', '8', 'Sta@@', 'aten', 'von', '4@@', '0', 'Proz@@', 'ent', 'war.', '</s>']
2024-05-23 18:17:53,883 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:17:53,883 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:17:53,883 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich habe diese zwei Diass gezeigt, dass die künstigen Eisen-Eisen-Eisen-Eisen-Eisen-Flügeler die größte 48 Staaten, Shrunk von 48 Staaten von 40 Prozent war.
2024-05-23 18:17:53,883 - INFO - joeynmt.training - Example #1
2024-05-23 18:17:53,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:17:53,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:17:53,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Ver@@', 'änder@@', 'ungen', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'versteh@@', 'en,', 'weil', 'es', 'das', 'nicht', 'das', 'D@@', 'ick@@', 'ch', 'des', 'E@@', 'is@@', 's@@', 'sen', 'zeig@@', 't', 'wird.', '</s>']
2024-05-23 18:17:53,883 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:17:53,883 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:17:53,883 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Veränderungen dieses speziellen Problem verstehen, weil es das nicht das Dickch des Eisssen zeigt wird.
2024-05-23 18:17:53,883 - INFO - joeynmt.training - Example #2
2024-05-23 18:17:53,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:17:53,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:17:53,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'z@@', 'eit', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n']
2024-05-23 18:17:53,884 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:17:53,884 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:17:53,884 - INFO - joeynmt.training - 	Hypothesis: Das arktische Eiszeit ist in einem Sinn des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima ist in einem Sinn des globalen Klima ist in einem Sinn
2024-05-23 18:17:53,884 - INFO - joeynmt.training - Example #3
2024-05-23 18:17:53,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:17:53,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:17:53,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'er@@', 'weiter@@', 't.', '</s>']
2024-05-23 18:17:53,884 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:17:53,884 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:17:53,884 - INFO - joeynmt.training - 	Hypothesis: Es erwartet Winter und Kontrakte in Winter und Kontrakte erweitert.
2024-05-23 18:17:53,884 - INFO - joeynmt.training - Example #4
2024-05-23 18:17:53,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:17:53,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:17:53,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'Di@@', 'a@@', '-@@', 'F@@', 'ast@@', '-@@', 'Vor@@', 'stell@@', 'ung', 'von', 'einem', 'schn@@', 'ell@@', 'er', 'werden', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:17:53,885 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:17:53,885 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:17:53,885 - INFO - joeynmt.training - 	Hypothesis: Das nächste Dia-Fast-Vorstellung von einem schneller werden von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 18:18:16,405 - INFO - joeynmt.training - Epoch   9, Step:    37100, Batch Loss:     1.573983, Batch Acc: 0.570139, Tokens per Sec:     3293, Lr: 0.000300
2024-05-23 18:18:38,482 - INFO - joeynmt.training - Epoch   9, Step:    37200, Batch Loss:     1.412706, Batch Acc: 0.565827, Tokens per Sec:     3362, Lr: 0.000300
2024-05-23 18:18:59,959 - INFO - joeynmt.training - Epoch   9, Step:    37300, Batch Loss:     1.405530, Batch Acc: 0.568270, Tokens per Sec:     3555, Lr: 0.000300
2024-05-23 18:19:21,161 - INFO - joeynmt.training - Epoch   9, Step:    37400, Batch Loss:     1.385385, Batch Acc: 0.566899, Tokens per Sec:     3592, Lr: 0.000300
2024-05-23 18:19:42,251 - INFO - joeynmt.training - Epoch   9, Step:    37500, Batch Loss:     1.344374, Batch Acc: 0.566618, Tokens per Sec:     3722, Lr: 0.000300
2024-05-23 18:19:42,253 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:19:42,253 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:21:12,796 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.74, acc:   0.53, generation: 90.5297[sec], evaluation: 0.0000[sec]
2024-05-23 18:21:12,799 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:21:13,184 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/37000.ckpt
2024-05-23 18:21:13,326 - INFO - joeynmt.training - Example #0
2024-05-23 18:21:13,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:21:13,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:21:13,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Di@@', 'a@@', 'st@@', 'ra@@', 'hl@@', 'en,', 'so', 'dass', 'das', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'B@@', 'au@@', 'm@@', 'k@@', 'ra@@', 'ft', 'von', '4@@', '8', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:21:13,327 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:21:13,327 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:21:13,327 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Diastrahlen, so dass das arktischen Eis der letzten drei Millionen Jahre die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Baumkraft von 48 Prozent.
2024-05-23 18:21:13,327 - INFO - joeynmt.training - Example #1
2024-05-23 18:21:13,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:21:13,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:21:13,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', 'unter@@', 'schei@@', 'den', 'den', 'den', 'das', 'Er@@', 'n@@', 'st', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 'ch', 'des', 'E@@', 'is@@', 'sen@@', 's', 'zeig@@', 't.', '</s>']
2024-05-23 18:21:13,328 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:21:13,328 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:21:13,328 - INFO - joeynmt.training - 	Hypothesis: Aber dieses unterscheiden den den das Ernst dieses speziellen Problem, weil es nicht das Dickch des Eissens zeigt.
2024-05-23 18:21:13,328 - INFO - joeynmt.training - Example #2
2024-05-23 18:21:13,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:21:13,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:21:13,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', '-@@', 'Sy@@', 'st@@', 'em', 'an@@', 'den', 'Her@@', 'z@@', '-@@', 'K@@', 'li@@', 'ma@@', '-@@', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 18:21:13,328 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:21:13,328 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:21:13,328 - INFO - joeynmt.training - 	Hypothesis: Das arktischen Eis ist in einem Sinn des globalen Klimaklima-System anden Herz-Klima-System.
2024-05-23 18:21:13,328 - INFO - joeynmt.training - Example #3
2024-05-23 18:21:13,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:21:13,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:21:13,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'er@@', 'war@@', 'en.', '</s>']
2024-05-23 18:21:13,329 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:21:13,329 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:21:13,329 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontrakte in Winter und Kontrakte erwaren.
2024-05-23 18:21:13,329 - INFO - joeynmt.training - Example #4
2024-05-23 18:21:13,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:21:13,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:21:13,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'Vor@@', 'wä@@', 'r@@', 'ts', 'von', 'dem', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:21:13,329 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:21:13,329 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:21:13,329 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass Sie ein schnelles Fast-Fast-Vorwärts von dem passiert ist.
2024-05-23 18:21:35,077 - INFO - joeynmt.training - Epoch   9, Step:    37600, Batch Loss:     1.474144, Batch Acc: 0.566556, Tokens per Sec:     3414, Lr: 0.000300
2024-05-23 18:21:56,378 - INFO - joeynmt.training - Epoch   9, Step:    37700, Batch Loss:     1.398108, Batch Acc: 0.566744, Tokens per Sec:     3637, Lr: 0.000300
2024-05-23 18:22:18,851 - INFO - joeynmt.training - Epoch   9, Step:    37800, Batch Loss:     1.455587, Batch Acc: 0.566384, Tokens per Sec:     3448, Lr: 0.000300
2024-05-23 18:22:40,355 - INFO - joeynmt.training - Epoch   9, Step:    37900, Batch Loss:     1.368493, Batch Acc: 0.566055, Tokens per Sec:     3543, Lr: 0.000300
2024-05-23 18:23:02,049 - INFO - joeynmt.training - Epoch   9, Step:    38000, Batch Loss:     1.421703, Batch Acc: 0.568309, Tokens per Sec:     3637, Lr: 0.000300
2024-05-23 18:23:02,050 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:23:02,050 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:24:36,311 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.74, acc:   0.53, generation: 94.2466[sec], evaluation: 0.0000[sec]
2024-05-23 18:24:36,313 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:24:36,704 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/35000.ckpt
2024-05-23 18:24:36,943 - INFO - joeynmt.training - Example #0
2024-05-23 18:24:36,944 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:24:36,944 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:24:36,944 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'n@@', 'it@@', 't@@', 'stell@@', 't,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'E@@', 'is@@', 'sch@@', 'ap@@', ',', 'die', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'größ@@', 'te', '4@@', '8', 'Sta@@', 'aten', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:24:36,944 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:24:36,944 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:24:36,944 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schnittstellt, dass die künstigen Eisschap, die die meisten drei Millionen Jahre die meisten drei Millionen Jahre die größte 48 Staaten von 40 Prozent.
2024-05-23 18:24:36,944 - INFO - joeynmt.training - Example #1
2024-05-23 18:24:36,944 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:24:36,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:24:36,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schie@@', 'd@@', 'liche', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'den', 'E@@', 'is@@', 'sch@@', 'en.', '</s>']
2024-05-23 18:24:36,945 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:24:36,945 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:24:36,945 - INFO - joeynmt.training - 	Hypothesis: Aber das unterschiedliche Ernsthaft dieses speziellen Problem weil es den Dicksal den Eisschen.
2024-05-23 18:24:36,945 - INFO - joeynmt.training - Example #2
2024-05-23 18:24:36,945 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:24:36,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:24:36,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'ige', 'E@@', 'is@@', 'sch@@', ',', 'das', 'sch@@', 'ließ@@', 'lich', 'der', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', ',', 'das', 'Sch@@', 'l@@', 'us@@', 's', 'des', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', '</s>']
2024-05-23 18:24:36,945 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:24:36,945 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:24:36,945 - INFO - joeynmt.training - 	Hypothesis: Das künstige Eissch, das schließlich der Klimawandel, das Schluss des Klimawandels des Klimawandels
2024-05-23 18:24:36,945 - INFO - joeynmt.training - Example #3
2024-05-23 18:24:36,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:24:36,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:24:36,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'er@@', 'war@@', 't@@', 'et.', '</s>']
2024-05-23 18:24:36,946 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:24:36,946 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:24:36,946 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verträgt in Winter und Kontrakte in Winter und Kontrakte erwartet.
2024-05-23 18:24:36,946 - INFO - joeynmt.training - Example #4
2024-05-23 18:24:36,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:24:36,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:24:36,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'K@@', 'ri@@', 'eg@@', ',', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:24:36,946 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:24:36,946 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:24:36,946 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass Sie ein schnelles Krieg, was über die letzten 25 Jahren passiert ist.
2024-05-23 18:24:59,622 - INFO - joeynmt.training - Epoch   9, Step:    38100, Batch Loss:     1.535410, Batch Acc: 0.568202, Tokens per Sec:     3255, Lr: 0.000300
2024-05-23 18:25:23,100 - INFO - joeynmt.training - Epoch   9, Step:    38200, Batch Loss:     1.451821, Batch Acc: 0.564501, Tokens per Sec:     3236, Lr: 0.000300
2024-05-23 18:25:45,755 - INFO - joeynmt.training - Epoch   9, Step:    38300, Batch Loss:     1.497857, Batch Acc: 0.565585, Tokens per Sec:     3373, Lr: 0.000300
2024-05-23 18:26:07,954 - INFO - joeynmt.training - Epoch   9, Step:    38400, Batch Loss:     1.814058, Batch Acc: 0.569646, Tokens per Sec:     3497, Lr: 0.000300
2024-05-23 18:26:30,632 - INFO - joeynmt.training - Epoch   9, Step:    38500, Batch Loss:     1.420139, Batch Acc: 0.565042, Tokens per Sec:     3426, Lr: 0.000300
2024-05-23 18:26:30,633 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:26:30,633 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:28:07,113 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.72, acc:   0.54, generation: 96.4668[sec], evaluation: 0.0000[sec]
2024-05-23 18:28:07,116 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:28:07,578 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/36500.ckpt
2024-05-23 18:28:07,754 - INFO - joeynmt.training - Example #0
2024-05-23 18:28:07,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:28:07,754 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:28:07,754 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'F@@', 'ol@@', 'ie', 'ge@@', 'zeig@@', 't,', 'dass', 'das', 'T@@', 'at', 'der', 'ent@@', 'schei@@', 'den@@', 'de', 'P@@', 'unk@@', 't', 'ver@@', 'st@@', 'an@@', 'den', 'ver@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'L@@', 'u@@', 'ft', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:28:07,755 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:28:07,755 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:28:07,755 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigt, dass das Tat der entscheidende Punkt verstanden versten drei Millionen Jahre die meisten drei Millionen Jahre die Größe der Luft von 40 Prozent.
2024-05-23 18:28:07,755 - INFO - joeynmt.training - Example #1
2024-05-23 18:28:07,755 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:28:07,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:28:07,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieser', 'unter@@', 'schie@@', 'd@@', 'liche', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'das', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is', 'zeig@@', 't', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'ist.', '</s>']
2024-05-23 18:28:07,755 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:28:07,755 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:28:07,755 - INFO - joeynmt.training - 	Hypothesis: Aber dieser unterschiedliche Ernsthaft dieses besonders Problem, weil es das nicht die Dicksal der Eis zeigt nicht das Dicksal ist.
2024-05-23 18:28:07,755 - INFO - joeynmt.training - Example #2
2024-05-23 18:28:07,755 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:28:07,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:28:07,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'ungs@@', 'z@@', 'ah@@', 'l', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'K@@', 'lim@@', 'a', 'des', 'K@@', 'li@@', 'ma@@', 'w@@', 'at@@', 'ischen', 'E@@', 'is@@', 'e.', '</s>']
2024-05-23 18:28:07,756 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:28:07,756 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:28:07,756 - INFO - joeynmt.training - 	Hypothesis: Das künstlerungszahl ist in einem Sinne des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des Klima des Klimawatischen Eise.
2024-05-23 18:28:07,756 - INFO - joeynmt.training - Example #3
2024-05-23 18:28:07,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:28:07,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:28:07,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ac@@', 'ten', 'im', 'S@@', 'omm@@', 'er', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'er@@', 'war@@', 'ten.', '</s>']
2024-05-23 18:28:07,756 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:28:07,756 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:28:07,756 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Vertracten im Sommer in Sommer und Verträgung erwarten.
2024-05-23 18:28:07,756 - INFO - joeynmt.training - Example #4
2024-05-23 18:28:07,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:28:07,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:28:07,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'wird', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'vor@@', 'wä@@', 'r@@', 'ts', 'von', 'dem', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:28:07,757 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:28:07,757 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:28:07,757 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie wird Ihnen ein schnelles Fast-vorwärts von dem was in den letzten 25 Jahren passiert ist.
2024-05-23 18:28:30,885 - INFO - joeynmt.training - Epoch   9, Step:    38600, Batch Loss:     1.481829, Batch Acc: 0.570495, Tokens per Sec:     3252, Lr: 0.000300
2024-05-23 18:28:52,757 - INFO - joeynmt.training - Epoch   9, Step:    38700, Batch Loss:     1.405851, Batch Acc: 0.566911, Tokens per Sec:     3593, Lr: 0.000300
2024-05-23 18:29:15,785 - INFO - joeynmt.training - Epoch   9, Step:    38800, Batch Loss:     1.457491, Batch Acc: 0.567753, Tokens per Sec:     3355, Lr: 0.000300
2024-05-23 18:29:38,318 - INFO - joeynmt.training - Epoch   9, Step:    38900, Batch Loss:     1.351231, Batch Acc: 0.568147, Tokens per Sec:     3416, Lr: 0.000300
2024-05-23 18:30:01,325 - INFO - joeynmt.training - Epoch   9, Step:    39000, Batch Loss:     1.423755, Batch Acc: 0.566274, Tokens per Sec:     3325, Lr: 0.000300
2024-05-23 18:30:01,325 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:30:01,325 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:31:41,334 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.71, acc:   0.53, generation: 99.9948[sec], evaluation: 0.0000[sec]
2024-05-23 18:31:41,337 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:31:41,781 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/35500.ckpt
2024-05-23 18:31:41,916 - INFO - joeynmt.training - Example #0
2024-05-23 18:31:41,916 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:31:41,916 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:31:41,916 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 't@@', 'z', 'ge@@', 'zeig@@', 't', 'habe', 'so', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'ische', 'E@@', 'is@@', 'sch@@', 'ap@@', ',', 'das', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:31:41,917 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:31:41,917 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:31:41,917 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich diese beiden Schlitz gezeigt habe so dass die künstlerische Eisschap, das für die meisten drei Millionen Jahre die Größe 40 Prozent.
2024-05-23 18:31:41,917 - INFO - joeynmt.training - Example #1
2024-05-23 18:31:41,917 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:31:41,917 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:31:41,917 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is', 'zeig@@', 't.', '</s>']
2024-05-23 18:31:41,917 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:31:41,917 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:31:41,917 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Ernsthaft dieses speziellen Problem, weil es nicht die Dicksal der Eis zeigt.
2024-05-23 18:31:41,918 - INFO - joeynmt.training - Example #2
2024-05-23 18:31:41,918 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:31:41,918 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:31:41,918 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', 'e', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'sch@@', 'ließ@@', 'lich', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 18:31:41,918 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:31:41,918 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:31:41,918 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eissche ist in einem Sinn, das schließlich des globalen Klimawandels.
2024-05-23 18:31:41,918 - INFO - joeynmt.training - Example #3
2024-05-23 18:31:41,918 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:31:41,918 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:31:41,918 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tra@@', 'h@@', 'ung', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tra@@', 'f@@', 'en.', '</s>']
2024-05-23 18:31:41,918 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:31:41,918 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:31:41,918 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Vertrahung in Winter und Vertrafen.
2024-05-23 18:31:41,919 - INFO - joeynmt.training - Example #4
2024-05-23 18:31:41,919 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:31:41,919 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:31:41,919 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'R@@', 'ei@@', 'f@@', 'f', 'von', 'dem', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:31:41,919 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:31:41,919 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:31:41,919 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein Reiff von dem was in den letzten 25 Jahren passiert ist.
2024-05-23 18:32:04,939 - INFO - joeynmt.training - Epoch   9, Step:    39100, Batch Loss:     1.329599, Batch Acc: 0.567001, Tokens per Sec:     3189, Lr: 0.000300
2024-05-23 18:32:29,674 - INFO - joeynmt.training - Epoch   9, Step:    39200, Batch Loss:     1.422933, Batch Acc: 0.568865, Tokens per Sec:     3098, Lr: 0.000300
2024-05-23 18:32:51,861 - INFO - joeynmt.training - Epoch   9, Step:    39300, Batch Loss:     1.560860, Batch Acc: 0.566745, Tokens per Sec:     3527, Lr: 0.000300
2024-05-23 18:33:13,483 - INFO - joeynmt.training - Epoch   9, Step:    39400, Batch Loss:     1.483483, Batch Acc: 0.566512, Tokens per Sec:     3588, Lr: 0.000300
2024-05-23 18:33:35,962 - INFO - joeynmt.training - Epoch   9, Step:    39500, Batch Loss:     1.626638, Batch Acc: 0.566450, Tokens per Sec:     3435, Lr: 0.000300
2024-05-23 18:33:35,963 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:33:35,963 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:35:14,665 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.69, acc:   0.54, generation: 98.6868[sec], evaluation: 0.0000[sec]
2024-05-23 18:35:14,668 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:35:15,049 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/36000.ckpt
2024-05-23 18:35:15,153 - INFO - joeynmt.training - Example #0
2024-05-23 18:35:15,153 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:35:15,153 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:35:15,153 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Di@@', 'as@@', 's', 'ge@@', 'zeig@@', 't', 'habe', 'ich', 'diese', 'bei@@', 'den', 'F@@', 'ol@@', 'gen', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'in', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:35:15,153 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:35:15,153 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:35:15,154 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Diass gezeigt habe ich diese beiden Folgen der letzten drei Millionen Jahre in der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Größe 40 Prozent.
2024-05-23 18:35:15,154 - INFO - joeynmt.training - Example #1
2024-05-23 18:35:15,154 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:35:15,154 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:35:15,154 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 'en', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't@@', 'n@@', 'is', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'das', 'D@@', 'ick@@', 'ar@@', 'tig@@', 'keit', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'zeig@@', 't.', '</s>']
2024-05-23 18:35:15,154 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:35:15,154 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:35:15,154 - INFO - joeynmt.training - 	Hypothesis: Aber das verstehen die Ernsthaftnis dieses besonders Problem, weil es das Dickartigkeit nicht die Dicksal zeigt.
2024-05-23 18:35:15,154 - INFO - joeynmt.training - Example #2
2024-05-23 18:35:15,154 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:35:15,154 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:35:15,154 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'sch@@', 'e', 'ist', 'in', 'einem', 'S@@', 'inn@@', ',', 'das', 'Sch@@', 'l@@', 'us@@', 's@@', 'r@@', 'eich@@', 'e', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 18:35:15,154 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:35:15,155 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:35:15,155 - INFO - joeynmt.training - 	Hypothesis: Das arktische Eissche ist in einem Sinn, das Schlussreiche des globalen Klimawandels.
2024-05-23 18:35:15,155 - INFO - joeynmt.training - Example #3
2024-05-23 18:35:15,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:35:15,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:35:15,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er@@', 'm@@', 'al.', '</s>']
2024-05-23 18:35:15,155 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:35:15,155 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:35:15,155 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Vertrakte im Sommermal.
2024-05-23 18:35:15,155 - INFO - joeynmt.training - Example #4
2024-05-23 18:35:15,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:35:15,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:35:15,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 't,', 'dass', 'man', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'Vor@@', 'wä@@', 'r@@', 'd', 'von', 'de@@', 'm,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 18:35:15,155 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:35:15,156 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:35:15,156 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeigt, dass man ein schnelles Fast-Vorwärd von dem, was in den letzten 25 Jahren passiert.
2024-05-23 18:35:36,698 - INFO - joeynmt.training - Epoch   9, Step:    39600, Batch Loss:     1.444933, Batch Acc: 0.562284, Tokens per Sec:     3461, Lr: 0.000300
2024-05-23 18:35:58,393 - INFO - joeynmt.training - Epoch   9, Step:    39700, Batch Loss:     1.551764, Batch Acc: 0.565852, Tokens per Sec:     3617, Lr: 0.000300
2024-05-23 18:36:19,915 - INFO - joeynmt.training - Epoch   9, Step:    39800, Batch Loss:     1.566430, Batch Acc: 0.561582, Tokens per Sec:     3652, Lr: 0.000300
2024-05-23 18:36:40,922 - INFO - joeynmt.training - Epoch   9, Step:    39900, Batch Loss:     1.517090, Batch Acc: 0.565049, Tokens per Sec:     3682, Lr: 0.000300
2024-05-23 18:37:02,659 - INFO - joeynmt.training - Epoch   9, Step:    40000, Batch Loss:     1.374358, Batch Acc: 0.562521, Tokens per Sec:     3525, Lr: 0.000300
2024-05-23 18:37:02,660 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:37:02,660 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:38:27,758 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.67, acc:   0.54, generation: 85.0827[sec], evaluation: 0.0000[sec]
2024-05-23 18:38:27,761 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 18:38:28,137 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/37500.ckpt
2024-05-23 18:38:28,266 - INFO - joeynmt.training - Example #0
2024-05-23 18:38:28,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:38:28,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:38:28,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'ge@@', 'zeig@@', 't', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'is@@', 'sch@@', 'ap@@', ',', 'das', 'für', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'waren', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:38:28,267 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:38:28,267 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:38:28,267 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schluss gezeigt dass die arktische Eisisschap, das für die meisten drei Millionen Jahre die meisten drei Millionen Jahre die waren die Größe 40 Prozent.
2024-05-23 18:38:28,267 - INFO - joeynmt.training - Example #1
2024-05-23 18:38:28,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:38:28,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:38:28,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schei@@', 'det', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'nicht', 'zeig@@', 't.', '</s>']
2024-05-23 18:38:28,267 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:38:28,267 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:38:28,267 - INFO - joeynmt.training - 	Hypothesis: Aber das unterscheidet die Ernsthaft dieses speziellen Problem weil es den Dicksal nicht zeigt.
2024-05-23 18:38:28,267 - INFO - joeynmt.training - Example #2
2024-05-23 18:38:28,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:38:28,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:38:28,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'sch@@', 'e', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', '-@@', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 18:38:28,268 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:38:28,268 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:38:28,268 - INFO - joeynmt.training - 	Hypothesis: Das ktische Eissche ist in einem Sinn des globalen Klimaklima-System.
2024-05-23 18:38:28,268 - INFO - joeynmt.training - Example #3
2024-05-23 18:38:28,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:38:28,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:38:28,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'ver@@', 'tr@@', 'au@@', 'en.', '</s>']
2024-05-23 18:38:28,268 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:38:28,268 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:38:28,268 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und vertrakte in Winter und vertrauen.
2024-05-23 18:38:28,268 - INFO - joeynmt.training - Example #4
2024-05-23 18:38:28,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:38:28,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:38:28,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'man', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'vor@@', 'wä@@', 'r@@', 'ts', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:38:28,269 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:38:28,269 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:38:28,269 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass man ein schneller Fast-Fast-vorwärts von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 18:38:50,076 - INFO - joeynmt.training - Epoch   9, Step:    40100, Batch Loss:     1.443107, Batch Acc: 0.564753, Tokens per Sec:     3359, Lr: 0.000300
2024-05-23 18:39:11,385 - INFO - joeynmt.training - Epoch   9, Step:    40200, Batch Loss:     1.570854, Batch Acc: 0.568811, Tokens per Sec:     3636, Lr: 0.000300
2024-05-23 18:39:34,851 - INFO - joeynmt.training - Epoch   9, Step:    40300, Batch Loss:     1.573157, Batch Acc: 0.561878, Tokens per Sec:     3252, Lr: 0.000300
2024-05-23 18:39:57,020 - INFO - joeynmt.training - Epoch   9, Step:    40400, Batch Loss:     1.297471, Batch Acc: 0.565637, Tokens per Sec:     3503, Lr: 0.000300
2024-05-23 18:40:20,650 - INFO - joeynmt.training - Epoch   9, Step:    40500, Batch Loss:     1.469744, Batch Acc: 0.559045, Tokens per Sec:     3323, Lr: 0.000300
2024-05-23 18:40:20,651 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:40:20,652 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:41:53,397 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.53, generation: 92.7308[sec], evaluation: 0.0000[sec]
2024-05-23 18:41:53,772 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/38000.ckpt
2024-05-23 18:41:53,928 - INFO - joeynmt.training - Example #0
2024-05-23 18:41:53,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:41:53,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:41:53,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'li@@', 't@@', 'ten', 'so', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'en@@', 'b@@', 'ah@@', 'n@@', 'b@@', 'ar', 'ist,', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'es', 'der', 'L@@', 'ag@@', 'e,', 'die', 'man', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:41:53,929 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:41:53,929 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:41:53,929 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schlitten so dass die arktischen Eisenbahnbar ist, die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Größe der niedriges der Lage, die man von 40 Prozent.
2024-05-23 18:41:53,929 - INFO - joeynmt.training - Example #1
2024-05-23 18:41:53,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:41:53,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:41:53,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is', 'zeig@@', 't', 'nich@@', 'ts', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'zeig@@', 't', 'wird.', '</s>']
2024-05-23 18:41:53,930 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:41:53,930 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:41:53,930 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Ernsthaft dieses speziellen Problem weil es den Dicksal des Eis zeigt nichts des Eises zeigt nicht das Dicksal zeigt wird.
2024-05-23 18:41:53,930 - INFO - joeynmt.training - Example #2
2024-05-23 18:41:53,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:41:53,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:41:53,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ti@@', 'c', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'in@@', 'ne', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'at@@', 'es', 'Sy@@', 'st@@', 'em@@', '.', '</s>']
2024-05-23 18:41:53,930 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:41:53,930 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:41:53,930 - INFO - joeynmt.training - 	Hypothesis: Das ktic Eis ist in einem Sinne des globalen Klimawandels des globalen Klimaates System.
2024-05-23 18:41:53,930 - INFO - joeynmt.training - Example #3
2024-05-23 18:41:53,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:41:53,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:41:53,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'ver@@', 'w@@', 'andel@@', 't.', '</s>']
2024-05-23 18:41:53,931 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:41:53,931 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:41:53,931 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontrakte in Sommer verwandelt.
2024-05-23 18:41:53,931 - INFO - joeynmt.training - Example #4
2024-05-23 18:41:53,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:41:53,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:41:53,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'man', 'ein', 'schn@@', 'ell@@', 'es', 'F@@', 'ast@@', '-@@', 'vor@@', 'stell@@', 'ung', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-23 18:41:53,931 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:41:53,931 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:41:53,931 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass man ein schnelles Fast-vorstellung von dem, was über die letzten 25 Jahren geschehen ist.
2024-05-23 18:42:20,890 - INFO - joeynmt.training - Epoch   9, Step:    40600, Batch Loss:     1.351342, Batch Acc: 0.561891, Tokens per Sec:     2846, Lr: 0.000300
2024-05-23 18:42:25,961 - INFO - joeynmt.training - Epoch   9: total training loss 6469.44
2024-05-23 18:42:25,962 - INFO - joeynmt.training - EPOCH 10
2024-05-23 18:42:42,391 - INFO - joeynmt.training - Epoch  10, Step:    40700, Batch Loss:     1.346950, Batch Acc: 0.586783, Tokens per Sec:     3476, Lr: 0.000300
2024-05-23 18:43:04,246 - INFO - joeynmt.training - Epoch  10, Step:    40800, Batch Loss:     1.387713, Batch Acc: 0.586532, Tokens per Sec:     3447, Lr: 0.000300
2024-05-23 18:43:25,262 - INFO - joeynmt.training - Epoch  10, Step:    40900, Batch Loss:     1.274669, Batch Acc: 0.583839, Tokens per Sec:     3678, Lr: 0.000300
2024-05-23 18:43:45,425 - INFO - joeynmt.training - Epoch  10, Step:    41000, Batch Loss:     1.234171, Batch Acc: 0.584470, Tokens per Sec:     3794, Lr: 0.000300
2024-05-23 18:43:45,426 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:43:45,426 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:45:11,803 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.54, generation: 86.3617[sec], evaluation: 0.0000[sec]
2024-05-23 18:45:12,182 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/38500.ckpt
2024-05-23 18:45:12,407 - INFO - joeynmt.training - Example #0
2024-05-23 18:45:12,408 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:45:12,408 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:45:12,408 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'ge@@', 'zeig@@', 't,', 'dass', 'das', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'wur@@', 'de,', 'die', 'Gr@@', 'öß@@', 'e', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 18:45:12,408 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:45:12,408 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:45:12,408 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schluss gezeigt, dass das arktische Eis der letzten drei Millionen Jahre die meisten drei Millionen Jahre die meisten drei Millionen Jahre die Größe der niedriger wurde, die Größe 40 Prozent.
2024-05-23 18:45:12,408 - INFO - joeynmt.training - Example #1
2024-05-23 18:45:12,408 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:45:12,408 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:45:12,408 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'das', 'Pro@@', 'blem', 'nicht', 'zeig@@', 't', 'das', 'D@@', 'ick@@', 'te', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'zeig@@', 't.', '</s>']
2024-05-23 18:45:12,409 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:45:12,409 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:45:12,409 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht die ernsthaft dieses besonders Problem, weil es das Problem nicht zeigt das Dickte nicht das Dicksal zeigt.
2024-05-23 18:45:12,409 - INFO - joeynmt.training - Example #2
2024-05-23 18:45:12,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:45:12,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:45:12,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ti@@', 'k', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 18:45:12,409 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:45:12,409 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:45:12,409 - INFO - joeynmt.training - 	Hypothesis: Das ktik ist in einem Sinn in einem Sinn des globalen Klimawandels.
2024-05-23 18:45:12,409 - INFO - joeynmt.training - Example #3
2024-05-23 18:45:12,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:45:12,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:45:12,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'W@@', 'in@@', 'ter', 'und', 'kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'kon@@', 'tra@@', 'h@@', '.', '</s>']
2024-05-23 18:45:12,410 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:45:12,410 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:45:12,410 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte in Winter und kontrakte im Sommer kontrah.
2024-05-23 18:45:12,410 - INFO - joeynmt.training - Example #4
2024-05-23 18:45:12,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:45:12,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:45:12,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'F@@', 'ast@@', '-@@', 'vor@@', 'wä@@', 'r@@', 'ts', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 18:45:12,410 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:45:12,410 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:45:12,410 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein rapid fast-Fast-vorwärts passiert.
2024-05-23 18:45:32,101 - INFO - joeynmt.training - Epoch  10, Step:    41100, Batch Loss:     1.330240, Batch Acc: 0.577154, Tokens per Sec:     3707, Lr: 0.000300
2024-05-23 18:45:52,606 - INFO - joeynmt.training - Epoch  10, Step:    41200, Batch Loss:     1.522212, Batch Acc: 0.580898, Tokens per Sec:     3727, Lr: 0.000300
2024-05-23 18:46:12,115 - INFO - joeynmt.training - Epoch  10, Step:    41300, Batch Loss:     1.512790, Batch Acc: 0.583533, Tokens per Sec:     3910, Lr: 0.000300
2024-05-23 18:46:32,678 - INFO - joeynmt.training - Epoch  10, Step:    41400, Batch Loss:     1.305474, Batch Acc: 0.580952, Tokens per Sec:     3781, Lr: 0.000300
2024-05-23 18:46:55,028 - INFO - joeynmt.training - Epoch  10, Step:    41500, Batch Loss:     1.362157, Batch Acc: 0.580201, Tokens per Sec:     3494, Lr: 0.000300
2024-05-23 18:46:55,029 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:46:55,029 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:48:29,037 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.70, acc:   0.54, generation: 93.9938[sec], evaluation: 0.0000[sec]
2024-05-23 18:48:29,471 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/39000.ckpt
2024-05-23 18:48:29,645 - INFO - joeynmt.training - Example #0
2024-05-23 18:48:29,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:48:29,646 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:48:29,646 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'F@@', 'ol@@', 'gen', 'ge@@', 'zeig@@', 't,', 'so', 'dass', 'das', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'en@@', 'f@@', 'rei@@', 'z@@', 'eit', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'größ@@', 'te', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'wur@@', 'de,', 'die', 'für', 'die', '4@@', '0', 'Proz@@', 'ent', 'gew@@', 'es@@', 'en', 'wur@@', 'de.', '</s>']
2024-05-23 18:48:29,646 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:48:29,646 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:48:29,646 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folgen gezeigt, so dass das arktische Eisenfreizeit der letzten drei Millionen Jahre die größte der niedriger wurde, die für die 40 Prozent gewesen wurde.
2024-05-23 18:48:29,646 - INFO - joeynmt.training - Example #1
2024-05-23 18:48:29,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:48:29,646 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:48:29,646 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'unter@@', 'schie@@', 'd@@', 'liche', 'Zu@@', 'schau@@', 'er', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 'ert', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'zeig@@', 't.', '</s>']
2024-05-23 18:48:29,647 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:48:29,647 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:48:29,647 - INFO - joeynmt.training - 	Hypothesis: Aber diese unterschiedliche Zuschauer dieses speziellen Problem weil es den Dickert nicht die Dicksal des Eises zeigt nicht das Dicksal zeigt.
2024-05-23 18:48:29,647 - INFO - joeynmt.training - Example #2
2024-05-23 18:48:29,647 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:48:29,647 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:48:29,647 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'en@@', '-@@', 'E@@', 'is@@', 'en@@', '-@@', 'K@@', 'l@@', 'ein@@', 's@@', 'at@@', 'z@@', ',', 'der', 'das', 'glob@@', 'ale', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', '.', '</s>']
2024-05-23 18:48:29,647 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:48:29,647 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:48:29,647 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eisen-Eisen-Kleinsatz, der das globale Klimawandel.
2024-05-23 18:48:29,647 - INFO - joeynmt.training - Example #3
2024-05-23 18:48:29,647 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:48:29,647 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:48:29,647 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tra@@', 'k@@', 'ungen', 'in', 'S@@', 'omm@@', 'er', 'ver@@', 'fol@@', 'g@@', 't', '</s>']
2024-05-23 18:48:29,648 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:48:29,648 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:48:29,648 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Vertrakungen in Sommer verfolgt
2024-05-23 18:48:29,648 - INFO - joeynmt.training - Example #4
2024-05-23 18:48:29,648 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:48:29,648 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:48:29,648 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'Z@@', 'iel', 'sein,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:48:29,648 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:48:29,648 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:48:29,648 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich Ihnen ein schnelles Ziel sein, was in den letzten 25 Jahren passiert ist, was in den letzten 25 Jahren passiert ist.
2024-05-23 18:48:51,536 - INFO - joeynmt.training - Epoch  10, Step:    41600, Batch Loss:     1.419793, Batch Acc: 0.577154, Tokens per Sec:     3381, Lr: 0.000300
2024-05-23 18:49:17,702 - INFO - joeynmt.training - Epoch  10, Step:    41700, Batch Loss:     1.435892, Batch Acc: 0.577332, Tokens per Sec:     2891, Lr: 0.000300
2024-05-23 18:49:39,977 - INFO - joeynmt.training - Epoch  10, Step:    41800, Batch Loss:     1.478816, Batch Acc: 0.573964, Tokens per Sec:     3551, Lr: 0.000300
2024-05-23 18:50:02,067 - INFO - joeynmt.training - Epoch  10, Step:    41900, Batch Loss:     1.401855, Batch Acc: 0.574669, Tokens per Sec:     3466, Lr: 0.000300
2024-05-23 18:50:23,815 - INFO - joeynmt.training - Epoch  10, Step:    42000, Batch Loss:     1.221346, Batch Acc: 0.576410, Tokens per Sec:     3539, Lr: 0.000300
2024-05-23 18:50:23,817 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:50:23,817 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:51:48,262 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.69, acc:   0.54, generation: 84.4305[sec], evaluation: 0.0000[sec]
2024-05-23 18:51:48,685 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/41000.ckpt
2024-05-23 18:51:48,817 - INFO - joeynmt.training - Example #0
2024-05-23 18:51:48,818 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:51:48,818 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:51:48,818 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Di@@', 'as@@', 'sen@@', 'en', 'zeig@@', 't,', 'dass', 'das', 'Ar@@', 'k@@', 't@@', 'sch@@', 'at@@', ',', 'dass', 'das', 'die', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'B@@', 'eh@@', 'and@@', 'l@@', 'ung', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'hat.', '</s>']
2024-05-23 18:51:48,818 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:51:48,818 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:51:48,818 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Diassenen zeigt, dass das Arktschat, dass das die künstlersten drei Millionen Jahre die meisten drei Millionen Jahre die Behandlung der niedriger hat.
2024-05-23 18:51:48,818 - INFO - joeynmt.training - Example #1
2024-05-23 18:51:48,818 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:51:48,818 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:51:48,818 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'such@@', 't', 'das', 'Er@@', 'n@@', 'st', 'dieses', 'bes@@', 'onder@@', 's', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'k@@', 'el', 'zeig@@', 't', 'nicht', 'das', 'D@@', 'ick@@', 'ick@@', 's@@', 'al', 'zeig@@', 't.', '</s>']
2024-05-23 18:51:48,819 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:51:48,819 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:51:48,819 - INFO - joeynmt.training - 	Hypothesis: Aber das untersucht das Ernst dieses besonders dieses besonders Problem, weil es den Dicksal der Eiskel zeigt nicht das Dickicksal zeigt.
2024-05-23 18:51:48,819 - INFO - joeynmt.training - Example #2
2024-05-23 18:51:48,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:51:48,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:51:48,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'l@@', 'er@@', '-@@', 'K@@', 'a@@', 'p', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 18:51:48,819 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:51:48,819 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:51:48,819 - INFO - joeynmt.training - 	Hypothesis: Der künstler-Kap ist in einem Sinn des globalen Klimaklimaklimawandels.
2024-05-23 18:51:48,819 - INFO - joeynmt.training - Example #3
2024-05-23 18:51:48,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:51:48,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:51:48,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'in', 'W@@', 'in@@', 'ter', 'S@@', 'omm@@', 'er', 'zu', 'ver@@', 'tr@@', 'au@@', 'en.', '</s>']
2024-05-23 18:51:48,820 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:51:48,820 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:51:48,820 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verträgung in Winter Sommer zu vertrauen.
2024-05-23 18:51:48,820 - INFO - joeynmt.training - Example #4
2024-05-23 18:51:48,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:51:48,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:51:48,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 't', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'Z@@', 'w@@', 'ec@@', 'k', 'sein,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 18:51:48,820 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:51:48,820 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:51:48,820 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeigt Ihnen ein schnelles Zweck sein, was in den letzten 25 Jahre passiert ist, was in den letzten 25 Jahre passiert.
2024-05-23 18:52:10,531 - INFO - joeynmt.training - Epoch  10, Step:    42100, Batch Loss:     1.269473, Batch Acc: 0.578118, Tokens per Sec:     3441, Lr: 0.000300
2024-05-23 18:52:30,558 - INFO - joeynmt.training - Epoch  10, Step:    42200, Batch Loss:     1.487670, Batch Acc: 0.575737, Tokens per Sec:     3737, Lr: 0.000300
2024-05-23 18:52:51,644 - INFO - joeynmt.training - Epoch  10, Step:    42300, Batch Loss:     1.507395, Batch Acc: 0.571822, Tokens per Sec:     3723, Lr: 0.000300
2024-05-23 18:53:12,043 - INFO - joeynmt.training - Epoch  10, Step:    42400, Batch Loss:     1.347431, Batch Acc: 0.574630, Tokens per Sec:     3794, Lr: 0.000300
2024-05-23 18:53:32,882 - INFO - joeynmt.training - Epoch  10, Step:    42500, Batch Loss:     1.385294, Batch Acc: 0.574752, Tokens per Sec:     3694, Lr: 0.000300
2024-05-23 18:53:32,882 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:53:32,882 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:54:58,697 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.69, acc:   0.54, generation: 85.7993[sec], evaluation: 0.0000[sec]
2024-05-23 18:54:59,060 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/41500.ckpt
2024-05-23 18:54:59,258 - INFO - joeynmt.training - Example #0
2024-05-23 18:54:59,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:54:59,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:54:59,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'so', 'dass', 'die', 'bei@@', 'den', 'F@@', 'ol@@', 'gen', 'der', 'ar@@', 'k@@', 'ti@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', 'z@@', 'eit', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'B@@', 'es@@', 'ch@@', 'r@@', 'ing@@', 'en', 'von', '4@@', '0', 'Proz@@', 'ent', 'war.', '</s>']
2024-05-23 18:54:59,259 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:54:59,259 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:54:59,259 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien so dass die beiden Folgen der arktiketische Eiszeit der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Beschringen von 40 Prozent war.
2024-05-23 18:54:59,259 - INFO - joeynmt.training - Example #1
2024-05-23 18:54:59,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:54:59,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:54:59,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'versteh@@', 't', 'das', 'das', 'Er@@', 'n@@', 'st', 'dieses', 'bes@@', 'onder@@', 's', 'Problem@@', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'nicht', 'die', 'D@@', 'ick@@', 'wer@@', 'k', 'zeig@@', 't', 'wird.', '</s>']
2024-05-23 18:54:59,260 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:54:59,260 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:54:59,260 - INFO - joeynmt.training - 	Hypothesis: Aber das versteht das das Ernst dieses besonders Problem, weil es den Dicksal nicht die Dickwerk zeigt wird.
2024-05-23 18:54:59,260 - INFO - joeynmt.training - Example #2
2024-05-23 18:54:59,260 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:54:59,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:54:59,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ar@@', 'k@@', 'ti@@', 'ere', 'E@@', 'is@@', 'is@@', 'e,', 'der', 'Sch@@', 'l@@', 'us@@', 's', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'Sy@@', 'st@@', 'em@@', 's.', '</s>']
2024-05-23 18:54:59,260 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:54:59,260 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:54:59,260 - INFO - joeynmt.training - 	Hypothesis: Das arktiere Eisise, der Schluss ist in einem Sinn des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima des globalen Klima Systems.
2024-05-23 18:54:59,260 - INFO - joeynmt.training - Example #3
2024-05-23 18:54:59,260 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:54:59,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:54:59,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'im', 'S@@', 'omm@@', 'er', 'in', 'W@@', 'in@@', 'ter', 'S@@', 'omm@@', 'er', 'zu', 'ver@@', 'tr@@', 'au@@', 'en.', '</s>']
2024-05-23 18:54:59,261 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:54:59,261 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:54:59,261 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Verträgung im Sommer in Winter Sommer zu vertrauen.
2024-05-23 18:54:59,261 - INFO - joeynmt.training - Example #4
2024-05-23 18:54:59,261 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:54:59,261 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:54:59,261 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'wird', 'es', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'd@@', 'au@@', 'er@@', 'wä@@', 'r@@', 'ts', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahre', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:54:59,261 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:54:59,261 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:54:59,261 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie wird es in den letzten 25 Jahre dauerwärts von dem, was über die letzten 25 Jahre passiert ist.
2024-05-23 18:55:19,384 - INFO - joeynmt.training - Epoch  10, Step:    42600, Batch Loss:     1.416918, Batch Acc: 0.575813, Tokens per Sec:     3784, Lr: 0.000300
2024-05-23 18:55:40,166 - INFO - joeynmt.training - Epoch  10, Step:    42700, Batch Loss:     1.290844, Batch Acc: 0.572709, Tokens per Sec:     3754, Lr: 0.000300
2024-05-23 18:56:01,402 - INFO - joeynmt.training - Epoch  10, Step:    42800, Batch Loss:     1.376398, Batch Acc: 0.577879, Tokens per Sec:     3612, Lr: 0.000300
2024-05-23 18:56:22,024 - INFO - joeynmt.training - Epoch  10, Step:    42900, Batch Loss:     1.478380, Batch Acc: 0.574059, Tokens per Sec:     3805, Lr: 0.000300
2024-05-23 18:56:43,518 - INFO - joeynmt.training - Epoch  10, Step:    43000, Batch Loss:     1.303425, Batch Acc: 0.576461, Tokens per Sec:     3587, Lr: 0.000300
2024-05-23 18:56:43,520 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:56:43,520 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 18:57:57,963 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.67, acc:   0.54, generation: 74.4295[sec], evaluation: 0.0000[sec]
2024-05-23 18:57:58,403 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/40500.ckpt
2024-05-23 18:57:58,548 - INFO - joeynmt.training - Example #0
2024-05-23 18:57:58,548 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 18:57:58,548 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 18:57:58,548 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'n@@', 'it@@', 't', 'ge@@', 'zeig@@', 't', 'hab@@', 'e,', 'so', 'dass', 'das', 'k@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'größ@@', 'te', '4@@', '0', 'Proz@@', 'ent', 'der', 'L@@', 'ei@@', 'st@@', 'ad@@', 'en,', 'hat', 'sich', 'von', '4@@', '0', 'Proz@@', 'ent', 'ver@@', 'r@@', 'ück@@', 't.', '</s>']
2024-05-23 18:57:58,549 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 18:57:58,549 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 18:57:58,549 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schnitt gezeigt habe, so dass das künstler der letzten drei Millionen Jahre die meisten drei Millionen Jahre die größte 40 Prozent der Leistaden, hat sich von 40 Prozent verrückt.
2024-05-23 18:57:58,549 - INFO - joeynmt.training - Example #1
2024-05-23 18:57:58,549 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 18:57:58,549 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 18:57:58,549 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schie@@', 'd@@', 'liche', 'Pro@@', 'blem', 'des', 'bes@@', 'onder@@', 's', 'Problem@@', 's', 'zu', 'diesem', 'spe@@', 'zi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'nicht', 'zeig@@', 't.', '</s>']
2024-05-23 18:57:58,549 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 18:57:58,549 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 18:57:58,549 - INFO - joeynmt.training - 	Hypothesis: Aber das unterschiedliche Problem des besonders Problems zu diesem speziellen Problem, weil es den Dicksal nicht zeigt.
2024-05-23 18:57:58,549 - INFO - joeynmt.training - Example #2
2024-05-23 18:57:58,549 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 18:57:58,549 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 18:57:58,549 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', 'ä@@', 'tz@@', 't', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'glob@@', 'alen', 'K@@', 'lim@@', 'a', 'des', 'K@@', 'lim@@', 'a', 'des', 'K@@', 'lim@@', 'a', 'des', 'K@@', 'lim@@', 'a', 'ist.', '</s>']
2024-05-23 18:57:58,550 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 18:57:58,550 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 18:57:58,550 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eisschätzt in einem Sinn des globalen Klima des globalen Klima des Klima des Klima des Klima ist.
2024-05-23 18:57:58,550 - INFO - joeynmt.training - Example #3
2024-05-23 18:57:58,550 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 18:57:58,550 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 18:57:58,550 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'zu', 'kon@@', 'tra@@', 'f@@', 'en.', '</s>']
2024-05-23 18:57:58,550 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 18:57:58,550 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 18:57:58,550 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verträgung in Sommer Sommer zu kontrafen.
2024-05-23 18:57:58,550 - INFO - joeynmt.training - Example #4
2024-05-23 18:57:58,550 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 18:57:58,550 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 18:57:58,550 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'ra@@', 'pi@@', 'de', 'Sch@@', 'n@@', 'it@@', 't', 'von', 'de@@', 'm,', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 18:57:58,551 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 18:57:58,551 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 18:57:58,551 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass ich Ihnen ein rapide Schnitt von dem, was in den letzten 25 Jahren passiert ist.
2024-05-23 18:58:19,223 - INFO - joeynmt.training - Epoch  10, Step:    43100, Batch Loss:     1.356733, Batch Acc: 0.579857, Tokens per Sec:     3719, Lr: 0.000300
2024-05-23 18:58:39,781 - INFO - joeynmt.training - Epoch  10, Step:    43200, Batch Loss:     1.395407, Batch Acc: 0.574729, Tokens per Sec:     3712, Lr: 0.000300
2024-05-23 18:59:00,594 - INFO - joeynmt.training - Epoch  10, Step:    43300, Batch Loss:     1.399805, Batch Acc: 0.573325, Tokens per Sec:     3630, Lr: 0.000300
2024-05-23 18:59:20,962 - INFO - joeynmt.training - Epoch  10, Step:    43400, Batch Loss:     1.473509, Batch Acc: 0.570092, Tokens per Sec:     3857, Lr: 0.000300
2024-05-23 18:59:41,370 - INFO - joeynmt.training - Epoch  10, Step:    43500, Batch Loss:     1.469012, Batch Acc: 0.571459, Tokens per Sec:     3737, Lr: 0.000300
2024-05-23 18:59:41,371 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 18:59:41,371 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 19:01:33,951 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.65, acc:   0.54, generation: 112.5651[sec], evaluation: 0.0000[sec]
2024-05-23 19:01:33,955 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 19:01:34,293 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/42000.ckpt
2024-05-23 19:01:34,451 - INFO - joeynmt.training - Example #0
2024-05-23 19:01:34,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 19:01:34,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 19:01:34,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Sch@@', 'l@@', 'us@@', 's', 'ge@@', 'zeig@@', 't,', 'dass', 'das', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'sch@@', 'ä@@', 'tz@@', 'ung', 'des', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'ä@@', 'tz@@', 'ung', 'der', 'L@@', 'age', 'der', 'ver@@', 'w@@', 'andel@@', 't', 'hat,', 'die', 'sich', 'um', '4@@', '0', 'Proz@@', 'ent', 'ver@@', 'r@@', 'ück@@', 't', 'wur@@', 'de,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 19:01:34,451 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 19:01:34,451 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 19:01:34,451 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Schluss gezeigt, dass das arktische Eisschätzung des arktischen Eisschätzung der Lage der verwandelt hat, die sich um 40 Prozent verrückt wurde, Shrunk von 40 Prozent.
2024-05-23 19:01:34,451 - INFO - joeynmt.training - Example #1
2024-05-23 19:01:34,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 19:01:34,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 19:01:34,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'dieses', 'unter@@', 'hal@@', 'ten', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'tim@@', 'm@@', 'tes', 'Problem@@', ',', 'weil', 'es', 'den', 'D@@', 'ick@@', 'te', 'nicht', 'die', 'D@@', 'ick@@', 'ung', 'des', 'E@@', 'is@@', 'k@@', 'el@@', 's', 'zeig@@', 't.', '</s>']
2024-05-23 19:01:34,452 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 19:01:34,452 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 19:01:34,452 - INFO - joeynmt.training - 	Hypothesis: Aber dieses unterhalten die Ernsthaft dieses bestimmtes Problem, weil es den Dickte nicht die Dickung des Eiskels zeigt.
2024-05-23 19:01:34,452 - INFO - joeynmt.training - Example #2
2024-05-23 19:01:34,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 19:01:34,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 19:01:34,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'sch@@', 'ä@@', 'tz@@', 't', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 19:01:34,452 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 19:01:34,453 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 19:01:34,453 - INFO - joeynmt.training - 	Hypothesis: Das künstliche Eisschätzt in einem Sinn des globalen Klimawandels.
2024-05-23 19:01:34,453 - INFO - joeynmt.training - Example #3
2024-05-23 19:01:34,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 19:01:34,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 19:01:34,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'zu', 'ver@@', 'tr@@', 'ag@@', 'en.', '</s>']
2024-05-23 19:01:34,453 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 19:01:34,453 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 19:01:34,453 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakten in Sommer Sommer zu vertragen.
2024-05-23 19:01:34,453 - INFO - joeynmt.training - Example #4
2024-05-23 19:01:34,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 19:01:34,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 19:01:34,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'er', 'F@@', 'ast@@', '-@@', 'vor@@', 'wä@@', 'r@@', 'ts', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 't.', '</s>']
2024-05-23 19:01:34,453 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 19:01:34,454 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 19:01:34,454 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein schneller Fast-vorwärts in den letzten 25 Jahren passiert.
2024-05-23 19:02:50,095 - INFO - joeynmt.training - Epoch  10, Step:    43600, Batch Loss:     1.360397, Batch Acc: 0.573356, Tokens per Sec:     1018, Lr: 0.000300
2024-05-23 19:03:24,725 - INFO - joeynmt.training - Epoch  10, Step:    43700, Batch Loss:     1.335730, Batch Acc: 0.578962, Tokens per Sec:     2291, Lr: 0.000300
2024-05-23 19:04:10,194 - INFO - joeynmt.training - Epoch  10, Step:    43800, Batch Loss:     1.440524, Batch Acc: 0.571714, Tokens per Sec:     1676, Lr: 0.000300
2024-05-23 19:35:03,170 - INFO - joeynmt.training - Epoch  10, Step:    43900, Batch Loss:     1.283274, Batch Acc: 0.568680, Tokens per Sec:       41, Lr: 0.000300
2024-05-23 19:35:25,707 - INFO - joeynmt.training - Epoch  10, Step:    44000, Batch Loss:     1.164323, Batch Acc: 0.573981, Tokens per Sec:     3378, Lr: 0.000300
2024-05-23 19:35:25,708 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 19:35:25,708 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 19:39:03,861 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.64, acc:   0.54, generation: 218.1379[sec], evaluation: 0.0000[sec]
2024-05-23 19:39:03,863 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 19:39:04,280 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/39500.ckpt
2024-05-23 19:39:04,400 - INFO - joeynmt.training - Example #0
2024-05-23 19:39:04,400 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 19:39:04,400 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 19:39:04,400 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'F@@', 'ol@@', 'ie', 'zeig@@', 't,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'z@@', 'ep@@', 'ä@@', 'ge', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'mei@@', 'sten', 'drei', 'Millionen', 'Jahre', 'die', 'B@@', 'es@@', 'ch@@', 'ei@@', 'le', 'der', 'n@@', 'ie@@', 'dri@@', 'g@@', 'en', 'wur@@', 'de,', 'hat', 'sich', 'von', '4@@', '0', 'Proz@@', 'ent', 'ges@@', 'ag@@', 't', 'hat,', 'hat', 'sich', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 19:39:04,401 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 19:39:04,401 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 19:39:04,401 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folie zeigt, dass die arktischen Eiszepäge der letzten drei Millionen Jahre die meisten drei Millionen Jahre die Bescheile der niedrigen wurde, hat sich von 40 Prozent gesagt hat, hat sich von 40 Prozent.
2024-05-23 19:39:04,401 - INFO - joeynmt.training - Example #1
2024-05-23 19:39:04,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 19:39:04,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 19:39:04,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schei@@', 'det', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'bes@@', 'tim@@', 'm@@', 'ten', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 'te', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.', '</s>']
2024-05-23 19:39:04,401 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 19:39:04,401 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 19:39:04,401 - INFO - joeynmt.training - 	Hypothesis: Aber das unterscheidet die ernsthaft dieses bestimmten Problem weil es den Dickte nicht die Dicke des Eises zeigt.
2024-05-23 19:39:04,401 - INFO - joeynmt.training - Example #2
2024-05-23 19:39:04,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 19:39:04,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 19:39:04,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'liche', 'E@@', 'is@@', 'is@@', 'is@@', 'sch@@', 'en,', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'k@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 19:39:04,402 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 19:39:04,402 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 19:39:04,402 - INFO - joeynmt.training - 	Hypothesis: Der künstliche Eisisisschen, in einem Sinn des globalen Klimaklimawandels.
2024-05-23 19:39:04,402 - INFO - joeynmt.training - Example #3
2024-05-23 19:39:04,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 19:39:04,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 19:39:04,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'sich', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung.', '</s>']
2024-05-23 19:39:04,402 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 19:39:04,402 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 19:39:04,402 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Verträgung in Winter und Verträgung.
2024-05-23 19:39:04,402 - INFO - joeynmt.training - Example #4
2024-05-23 19:39:04,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 19:39:04,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 19:39:04,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'ra@@', 'pi@@', 'de', 'F@@', 'ast@@', 'st@@', 'ra@@', 'hl@@', 'ung', 'von', 'de@@', 'm,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.', '</s>']
2024-05-23 19:39:04,403 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 19:39:04,403 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 19:39:04,403 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige, dass Sie ein rapide Faststrahlung von dem, was über die letzten 25 Jahren passiert ist.
2024-05-23 19:39:24,063 - INFO - joeynmt.training - Epoch  10, Step:    44100, Batch Loss:     1.304729, Batch Acc: 0.576795, Tokens per Sec:     3709, Lr: 0.000300
2024-05-23 19:39:43,617 - INFO - joeynmt.training - Epoch  10, Step:    44200, Batch Loss:     1.301380, Batch Acc: 0.571811, Tokens per Sec:     3881, Lr: 0.000300
2024-05-23 19:40:03,744 - INFO - joeynmt.training - Epoch  10, Step:    44300, Batch Loss:     1.520730, Batch Acc: 0.572583, Tokens per Sec:     3899, Lr: 0.000300
2024-05-23 19:40:23,970 - INFO - joeynmt.training - Epoch  10, Step:    44400, Batch Loss:     1.504464, Batch Acc: 0.570940, Tokens per Sec:     3777, Lr: 0.000300
2024-05-23 19:40:44,697 - INFO - joeynmt.training - Epoch  10, Step:    44500, Batch Loss:     1.481119, Batch Acc: 0.568123, Tokens per Sec:     3774, Lr: 0.000300
2024-05-23 19:40:44,698 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 19:40:44,698 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 19:42:11,533 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.64, acc:   0.54, generation: 86.8202[sec], evaluation: 0.0000[sec]
2024-05-23 19:42:11,537 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 19:42:11,888 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/42500.ckpt
2024-05-23 19:42:11,989 - INFO - joeynmt.training - Example #0
2024-05-23 19:42:11,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 19:42:11,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 19:42:11,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Sch@@', 'li@@', 't@@', 'z', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'sch@@', ',', 'die', 'die', 'K@@', 'ün@@', 'st@@', 'l@@', 'er', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'öß@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'von', '4@@', '0', 'Proz@@', 'ent.', '</s>']
2024-05-23 19:42:11,990 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 19:42:11,990 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 19:42:11,990 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Schlitz gezeigt, dass die arktische Eissch, die die Künstler der letzten drei Millionen Jahre die Größe der niedriger von 40 Prozent.
2024-05-23 19:42:11,990 - INFO - joeynmt.training - Example #1
2024-05-23 19:42:11,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 19:42:11,990 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 19:42:11,990 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schie@@', 'd@@', 'liche', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spe@@', 'zi@@', 'elle', 'Pro@@', 'blem', 'weil', 'es', 'den', 'D@@', 'ick@@', 's@@', 'al', 'nicht', 'die', 'D@@', 'ick@@', 'e.', '</s>']
2024-05-23 19:42:11,990 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 19:42:11,990 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 19:42:11,990 - INFO - joeynmt.training - 	Hypothesis: Aber das unterschiedliche Ernsthaft dieses spezielle Problem weil es den Dicksal nicht die Dicke.
2024-05-23 19:42:11,990 - INFO - joeynmt.training - Example #2
2024-05-23 19:42:11,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 19:42:11,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 19:42:11,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'K@@', 'li@@', 'ma@@', '-@@', 'K@@', 'a@@', 'p', 'ist', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'ach@@', 'sen@@', 's,', 'das', 'Sch@@', 'l@@', 'ach@@', 't@@', 'ungs@@', 'system@@', '.', '</s>']
2024-05-23 19:42:11,991 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 19:42:11,991 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 19:42:11,991 - INFO - joeynmt.training - 	Hypothesis: Das Klima-Kap ist in einem Sinn des globalen Klimawachsens, das Schlachtungssystem.
2024-05-23 19:42:11,991 - INFO - joeynmt.training - Example #3
2024-05-23 19:42:11,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 19:42:11,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 19:42:11,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'war@@', 'tet', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'en.', '</s>']
2024-05-23 19:42:11,991 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 19:42:11,991 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 19:42:11,991 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Verträgung in Winter und Verträgen.
2024-05-23 19:42:11,991 - INFO - joeynmt.training - Example #4
2024-05-23 19:42:11,992 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 19:42:11,992 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 19:42:11,992 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'ich', 'Ihnen', 'ein', 'ra@@', 'pi@@', 'd', 'von', 'dem', 'F@@', 'ahr@@', 'ra@@', 'd', 'da@@', 'v@@', 'on,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 'te', 'ist.', '</s>']
2024-05-23 19:42:11,992 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 19:42:11,992 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 19:42:11,992 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass ich Ihnen ein rapid von dem Fahrrad davon, was über die letzten 25 Jahren passierte ist.
2024-05-23 19:42:32,486 - INFO - joeynmt.training - Epoch  10, Step:    44600, Batch Loss:     1.370923, Batch Acc: 0.565717, Tokens per Sec:     3591, Lr: 0.000300
2024-05-23 19:42:52,930 - INFO - joeynmt.training - Epoch  10, Step:    44700, Batch Loss:     1.386028, Batch Acc: 0.575091, Tokens per Sec:     3820, Lr: 0.000300
2024-05-23 19:43:13,611 - INFO - joeynmt.training - Epoch  10, Step:    44800, Batch Loss:     1.436105, Batch Acc: 0.571773, Tokens per Sec:     3747, Lr: 0.000300
2024-05-23 19:43:33,989 - INFO - joeynmt.training - Epoch  10, Step:    44900, Batch Loss:     1.336060, Batch Acc: 0.571410, Tokens per Sec:     3849, Lr: 0.000300
2024-05-23 19:43:54,902 - INFO - joeynmt.training - Epoch  10, Step:    45000, Batch Loss:     1.587120, Batch Acc: 0.566506, Tokens per Sec:     3683, Lr: 0.000300
2024-05-23 19:43:54,904 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 19:43:54,904 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 19:45:25,876 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.63, acc:   0.54, generation: 90.9581[sec], evaluation: 0.0000[sec]
2024-05-23 19:45:25,878 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 19:45:26,257 - INFO - joeynmt.helpers - delete ../models/model_bpe_2000/43000.ckpt
2024-05-23 19:45:26,458 - INFO - joeynmt.training - Example #0
2024-05-23 19:45:26,459 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'ye@@', 'ar', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 's@@', 'li@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '4@@', '0', 'perc@@', 'ent.']
2024-05-23 19:45:26,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'bei@@', 'den', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 't,', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ö@@', 's@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hat@@', 'te,', 'um', '4@@', '0', 'Proz@@', 'ent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist.']
2024-05-23 19:45:26,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'F@@', 'ol@@', 'ie', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'ar@@', 'k@@', 't@@', 'ischen', 'E@@', 'is@@', 'sch@@', 'rei@@', 'en,', 'dass', 'die', 'k@@', 'ün@@', 'st@@', 'igen', 'der', 'letz@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'größ@@', 'te', '4@@', '0', 'Proz@@', 'ent', 'war.', '</s>']
2024-05-23 19:45:26,459 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 19:45:26,459 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 19:45:26,459 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigt, dass die arktischen Eisschreien, dass die künstigen der letzten drei Millionen Jahre die größte 40 Prozent war.
2024-05-23 19:45:26,459 - INFO - joeynmt.training - Example #1
2024-05-23 19:45:26,459 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'ser@@', 'i@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'par@@', 'tic@@', 'ul@@', 'ar', 'pro@@', 'blem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2024-05-23 19:45:26,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'dr@@', 'ück@@', 't', 'nicht', 'star@@', 'k', 'gen@@', 'u@@', 'g', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spe@@', 'zi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 19:45:26,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schei@@', 'det', 'die', 'Er@@', 'n@@', 's@@', 'th@@', 'ik@@', 'an@@', 'ischen', 'Pro@@', 'blem', 'dieser', 'bes@@', 'onder@@', 'e', 'Pro@@', 'blem', 'zu', 'zeig@@', 'en,', 'weil', 'es', 'den', 'D@@', 'ick@@', 'st', 'nicht', 'zeig@@', 't.', '</s>']
2024-05-23 19:45:26,460 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 19:45:26,460 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 19:45:26,460 - INFO - joeynmt.training - 	Hypothesis: Aber das unterscheidet die Ernsthikanischen Problem dieser besondere Problem zu zeigen, weil es den Dickst nicht zeigt.
2024-05-23 19:45:26,460 - INFO - joeynmt.training - Example #2
2024-05-23 19:45:26,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2024-05-23 19:45:26,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'ge@@', 'wis@@', 'se@@', 'm', 'S@@', 'in@@', 'ne', 'ist', 'die', 'ar@@', 'k@@', 't@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unser@@', 'es', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 19:45:26,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'k@@', 'ün@@', 'st@@', 'ig@@', 'es', 'E@@', 'is@@', 'sch@@', ',', 'in', 'einem', 'S@@', 'in@@', 'n', 'des', 'glob@@', 'alen', 'K@@', 'li@@', 'ma@@', 'w@@', 'andel@@', 's.', '</s>']
2024-05-23 19:45:26,460 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 19:45:26,460 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 19:45:26,460 - INFO - joeynmt.training - 	Hypothesis: Der künstiges Eissch, in einem Sinn des globalen Klimawandels.
2024-05-23 19:45:26,460 - INFO - joeynmt.training - Example #3
2024-05-23 19:45:26,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 19:45:26,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'W@@', 'in@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 19:45:26,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'in', 'W@@', 'in@@', 'ter', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'in', 'und', 'Ver@@', 'tr@@', 'ä@@', 'g@@', 'ung', 'zu', 'ver@@', 'tr@@', 'ä@@', 'g@@', 't.', '</s>']
2024-05-23 19:45:26,461 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 19:45:26,461 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 19:45:26,461 - INFO - joeynmt.training - 	Hypothesis: Es erweitert in Winter und Verträgung in und Verträgung zu verträgt.
2024-05-23 19:45:26,461 - INFO - joeynmt.training - Example #4
2024-05-23 19:45:26,461 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', 'wh@@', "at's", 'happen@@', 'ed', 'over', 'the', 'last', '2@@', '5', 'year@@', 's.']
2024-05-23 19:45:26,461 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'näch@@', 'ste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Z@@', 'eit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'ert', 'ist.']
2024-05-23 19:45:26,461 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'näch@@', 'ste', 'F@@', 'ol@@', 'ie', 'zeig@@', 'e,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'Z@@', 'iel', 'sein,', 'was', 'über', 'die', 'letz@@', 'ten', '2@@', '5', 'Jahren', 'passi@@', 'er@@', 'te', 'passi@@', 'er@@', 'te.', '</s>']
2024-05-23 19:45:26,461 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 19:45:26,461 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 19:45:26,461 - INFO - joeynmt.training - 	Hypothesis: Das nächste Folie zeige, dass Sie ein schnelles Ziel sein, was über die letzten 25 Jahren passierte passierte.
2024-05-23 19:45:47,260 - INFO - joeynmt.training - Epoch  10, Step:    45100, Batch Loss:     1.287157, Batch Acc: 0.569853, Tokens per Sec:     3565, Lr: 0.000300
2024-05-23 19:45:56,284 - INFO - joeynmt.training - Epoch  10: total training loss 6366.75
2024-05-23 19:45:56,284 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-23 19:45:56,284 - INFO - joeynmt.training - Best validation result (greedy) at step    45000:   4.63 ppl.
2024-05-23 19:45:56,300 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-23 19:45:56,352 - INFO - joeynmt.model - Enc-dec model built.
2024-05-23 19:45:56,635 - INFO - joeynmt.helpers - Load model from /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_bpe_2000/45000.ckpt.
2024-05-23 19:45:56,639 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2000),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2000),
	loss_function=None)
2024-05-23 19:45:56,642 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-23 19:45:56,642 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 19:45:56,642 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 19:48:27,761 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 151.1056[sec], evaluation: 0.0000[sec]
2024-05-23 19:48:27,765 - INFO - joeynmt.prediction - Translations saved to: /Users/janinehindermann/Documents/workarea/mt-exercise-5/scripts/../models/model_bpe_2000/00045000.hyps.dev.
2024-05-23 19:48:27,765 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-23 19:48:27,765 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 19:48:27,765 - INFO - joeynmt.prediction - Predicting 1568 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 19:51:17,821 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 170.0356[sec], evaluation: 0.0000[sec]
2024-05-23 19:51:17,827 - INFO - joeynmt.prediction - Translations saved to: /Users/janinehindermann/Documents/workarea/mt-exercise-5/scripts/../models/model_bpe_2000/00045000.hyps.test.
