2024-05-23 21:21:21,736 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-23 21:21:21,736 - INFO - joeynmt.helpers -                           cfg.name : bpe_5000
2024-05-23 21:21:21,736 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-23 21:21:21,736 - INFO - joeynmt.helpers -                     cfg.data.train : data_sampled/sampled_train.en-de
2024-05-23 21:21:21,736 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev.en-de
2024-05-23 21:21:21,736 - INFO - joeynmt.helpers -                      cfg.data.test : data/test.en-de
2024-05-23 21:21:21,736 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-23 21:21:21,736 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data_bpe_5000/joint_vocab_clean.txt
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 5000
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data_bpe_5000/bpe_code_5000.bpe
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : de
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data_bpe_5000/joint_vocab_clean.txt
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 5000
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data_bpe_5000/bpe_code_5000.bpe
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-23 21:21:21,737 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -             cfg.training.model_dir : ../models/model_bpe_5000
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-23 21:21:21,738 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-23 21:21:21,739 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-23 21:21:21,741 - INFO - joeynmt.data - Building tokenizer...
2024-05-23 21:21:21,752 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-23 21:21:21,752 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-23 21:21:21,752 - INFO - joeynmt.data - Loading train set...
2024-05-23 21:21:21,963 - INFO - joeynmt.data - Building vocabulary...
2024-05-23 21:21:22,241 - INFO - joeynmt.data - Loading dev set...
2024-05-23 21:21:22,246 - INFO - joeynmt.data - Loading test set...
2024-05-23 21:21:22,249 - INFO - joeynmt.data - Data loaded.
2024-05-23 21:21:22,249 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 21:21:22,250 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=888, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 21:21:22,250 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1568, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 21:21:22,250 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ or@@ e: A@@ ver@@ ting the climate c@@ ris@@ is
	[TRG] A@@ l G@@ or@@ e: Die Ab@@ wen@@ dung der Klima@@ k@@ at@@ ast@@ rop@@ he
2024-05-23 21:21:22,250 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) !" (6) !@@ (7) " (8) ", (9) ".
2024-05-23 21:21:22,250 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) !" (6) !@@ (7) " (8) ", (9) ".
2024-05-23 21:21:22,250 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4998
2024-05-23 21:21:22,250 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4998
2024-05-23 21:21:22,253 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-23 21:21:22,342 - INFO - joeynmt.model - Enc-dec model built.
2024-05-23 21:21:22,345 - INFO - joeynmt.model - Total params: 4178688
2024-05-23 21:21:22,345 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2024-05-23 21:21:22,345 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4998),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4998),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-23 21:21:22,346 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-23 21:21:22,346 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-23 21:21:22,346 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-23 21:21:22,346 - INFO - joeynmt.training - EPOCH 1
2024-05-23 21:21:49,043 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.485443, Batch Acc: 0.039817, Tokens per Sec:     2786, Lr: 0.000300
2024-05-23 21:22:13,870 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.404895, Batch Acc: 0.052426, Tokens per Sec:     2912, Lr: 0.000300
2024-05-23 21:22:38,775 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.238067, Batch Acc: 0.059973, Tokens per Sec:     2979, Lr: 0.000300
2024-05-23 21:23:03,552 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     4.114520, Batch Acc: 0.063427, Tokens per Sec:     2877, Lr: 0.000300
2024-05-23 21:23:23,413 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.054371, Batch Acc: 0.068460, Tokens per Sec:     3752, Lr: 0.000300
2024-05-23 21:23:23,414 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:23:23,414 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:25:04,071 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   4.09, ppl:  59.93, acc:   0.06, generation: 100.6444[sec], evaluation: 0.0000[sec]
2024-05-23 21:25:04,073 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:25:04,475 - INFO - joeynmt.training - Example #0
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'die', 'T@@', 'z@@', ',', 'die', 'T@@', ',', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en', 'und', 'die', 'T@@', 'z@@', 'en.', '</s>']
2024-05-23 21:25:04,475 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:25:04,475 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:25:04,475 - INFO - joeynmt.training - 	Hypothesis: Ich die Tz, die T, und die Tzen und die Tzen und die Tzen und die Tzen und die Ten und die Tzen und die Tzen und die Tzen und die Ten und die Tzen und die Tzen und die Tzen.
2024-05-23 21:25:04,475 - INFO - joeynmt.training - Example #1
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'ich', 'die', 'W@@', '-@@', 'W@@', '-@@', 'W@@', '-@@', 'W@@', '-@@', 'W@@', '-@@', 'W@@', '-@@', 'T@@', '-@@', 'T@@', '-@@', 'T@@', '-@@', 'T@@', 'st@@', 'z@@', 'en', 'und', 'die', 'wir', 'und', 'die', 'wir', 'und', 'und', 'und', 'und', 'die', 'W@@', 'z@@', 'en.', '</s>']
2024-05-23 21:25:04,475 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:25:04,475 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:25:04,475 - INFO - joeynmt.training - 	Hypothesis: Ich ich die W-W-W-W-W-W-T-T-T-Tstzen und die wir und die wir und und und und die Wzen.
2024-05-23 21:25:04,475 - INFO - joeynmt.training - Example #2
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:25:04,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'ich', 'die', 'W@@', '-@@', 'Sie', 'die', 'wir', 'die', 'wir', 'die', 'wir', 'die', 'ich', 'die', 'ich', 'die', 'ich', 'die', 'wir', 'die', 'W@@', '.', '</s>']
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Hypothesis: Ich ich die W-Sie die wir die wir die wir die ich die ich die ich die wir die W.
2024-05-23 21:25:04,476 - INFO - joeynmt.training - Example #3
2024-05-23 21:25:04,476 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:25:04,476 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:25:04,476 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Sie', 'die', 'W@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'st@@', 'en', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu']
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Hypothesis: Sie die Wststststststststststststststststststststststststststststststststststststststen zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu
2024-05-23 21:25:04,476 - INFO - joeynmt.training - Example #4
2024-05-23 21:25:04,476 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:25:04,476 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:25:04,476 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'ich', 'die', 'W@@', '-@@', 'Sie', 'die', 'wir', 'die', 'wir', 'die', 'wir', 'die', 'wir', 'die', 'wir', 'die', 'wir', 'die', 'W@@', 'z@@', 'en', 'und', 'die', 'W@@', 'z@@', 'en', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', '.', '</s>']
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:25:04,476 - INFO - joeynmt.training - 	Hypothesis: Ich ich die W-Sie die wir die wir die wir die wir die wir die wir die Wzen und die Wzen zu zu zu zu zu zu zu zu zu zu zu zu zu zu .
2024-05-23 21:25:24,972 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.108520, Batch Acc: 0.068888, Tokens per Sec:     3590, Lr: 0.000300
2024-05-23 21:25:46,150 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.981727, Batch Acc: 0.076572, Tokens per Sec:     3445, Lr: 0.000300
2024-05-23 21:26:08,115 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.922865, Batch Acc: 0.079631, Tokens per Sec:     3348, Lr: 0.000300
2024-05-23 21:26:28,823 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.826747, Batch Acc: 0.085607, Tokens per Sec:     3586, Lr: 0.000300
2024-05-23 21:26:49,384 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.916611, Batch Acc: 0.088298, Tokens per Sec:     3628, Lr: 0.000300
2024-05-23 21:26:49,387 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:26:49,387 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:28:30,011 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.95, ppl:  51.84, acc:   0.09, generation: 100.6129[sec], evaluation: 0.0000[sec]
2024-05-23 21:28:30,012 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:28:30,412 - INFO - joeynmt.training - Example #0
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ü@@', ',', 'ich', 'es', 'ist', 'es', 'ist', 'ich', 'es', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'er@@', '.', '</s>']
2024-05-23 21:28:30,412 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:28:30,412 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:28:30,412 - INFO - joeynmt.training - 	Hypothesis: Die Kü, ich es ist es ist ich es zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu er.
2024-05-23 21:28:30,412 - INFO - joeynmt.training - Example #1
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'es', 'ist', 'es', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'das', 'ist', 'die', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der']
2024-05-23 21:28:30,412 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:28:30,412 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:28:30,412 - INFO - joeynmt.training - 	Hypothesis: Aber es ist es ist ist ist ist ist ist das ist die der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der der
2024-05-23 21:28:30,412 - INFO - joeynmt.training - Example #2
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:28:30,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:28:30,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'die', 'K@@', 'ü@@', 'f@@', 'ü@@', 'f@@', 'ü@@', ',', 'die', 'der', 'K@@', 'ü@@', 'ü@@', '.', '</s>']
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Hypothesis: Das ist ist ist ist ist ist ist ist ist ist die Küfüfü, die der Küü.
2024-05-23 21:28:30,413 - INFO - joeynmt.training - Example #3
2024-05-23 21:28:30,413 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:28:30,413 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:28:30,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'ist', 'ist', 'ist', 'und', 'und', 'und', 'das', 'ist', 'und', 'und', 'das', 'ist', 'und', 'und', 'und', 'und', 'und', 'das', 'ist', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'Sie', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und']
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Hypothesis: Es ist ist ist ist und und und das ist und und das ist und und und und und das ist und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und Sie und und und und und und und und und
2024-05-23 21:28:30,413 - INFO - joeynmt.training - Example #4
2024-05-23 21:28:30,413 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:28:30,413 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:28:30,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'ist', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', '.', '</s>']
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:28:30,413 - INFO - joeynmt.training - 	Hypothesis: Das ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist ist zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu .
2024-05-23 21:28:51,855 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.811175, Batch Acc: 0.095324, Tokens per Sec:     3408, Lr: 0.000300
2024-05-23 21:29:13,016 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.851500, Batch Acc: 0.095403, Tokens per Sec:     3560, Lr: 0.000300
2024-05-23 21:29:34,560 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.853162, Batch Acc: 0.099717, Tokens per Sec:     3446, Lr: 0.000300
2024-05-23 21:29:56,255 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.954448, Batch Acc: 0.102397, Tokens per Sec:     3387, Lr: 0.000300
2024-05-23 21:30:18,556 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.854402, Batch Acc: 0.108639, Tokens per Sec:     3324, Lr: 0.000300
2024-05-23 21:30:18,557 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:30:18,557 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:32:11,045 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.82, ppl:  45.61, acc:   0.10, generation: 112.4741[sec], evaluation: 0.0000[sec]
2024-05-23 21:32:11,046 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:32:11,369 - INFO - joeynmt.training - Example #0
2024-05-23 21:32:11,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:32:11,369 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:32:11,369 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'habe', 'ich', 'in', 'der', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'g', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu']
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Hypothesis: Ich habe ich in der Küden Küden Küden Küden Küden Küden Küden Küden Küden Küden Küden Küg zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu
2024-05-23 21:32:11,370 - INFO - joeynmt.training - Example #1
2024-05-23 21:32:11,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:32:11,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:32:11,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'man', 'die', 'K@@', 'ü@@', 'g', 'der', 'K@@', 'ü@@', 'g', 'der', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'den', 'K@@', 'ü@@', 'g', 'der', 'K@@', 'ü@@', 'g', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu', 'zu']
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Hypothesis: Aber man die Küg der Küg der Küden Küden Küg der Küg zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu zu
2024-05-23 21:32:11,370 - INFO - joeynmt.training - Example #2
2024-05-23 21:32:11,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:32:11,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:32:11,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ü@@', 'g', 'in', 'der', 'K@@', 'ü@@', 'g', 'in', 'der', 'K@@', 'ü@@', 'g', 'der', 'K@@', 'ü@@', 'g', 'der', 'K@@', 'ü@@', 'g', 'von', 'der', 'K@@', 'ü@@', 'g', 'in', 'der', 'K@@', 'ü@@', 'g', 'zu', 'zu', 'zu', 'ver@@', '.', '</s>']
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:32:11,370 - INFO - joeynmt.training - 	Hypothesis: Die Küg in der Küg in der Küg der Küg der Küg von der Küg in der Küg zu zu zu ver.
2024-05-23 21:32:11,370 - INFO - joeynmt.training - Example #3
2024-05-23 21:32:11,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:32:11,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:32:11,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'ein', 'K@@', 'ü@@', '.', '</s>']
2024-05-23 21:32:11,371 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:32:11,371 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:32:11,371 - INFO - joeynmt.training - 	Hypothesis: Es ist ein Kü.
2024-05-23 21:32:11,371 - INFO - joeynmt.training - Example #4
2024-05-23 21:32:11,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:32:11,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:32:11,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'K@@', 'ü@@', 'g', 'in', 'ich', 'in', 'ich', 'in', 'ich', 'in', 'ich', 'in', 'ich', 'in', 'ich', 'in', 'man', 'zu', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', 'zu@@', '.', '</s>']
2024-05-23 21:32:11,371 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:32:11,371 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:32:11,371 - INFO - joeynmt.training - 	Hypothesis: Die Küg in ich in ich in ich in ich in ich in ich in man zu zuzuzuzuzuzuzuzuzuzuzuzuzuzuzu.
2024-05-23 21:32:34,770 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.754744, Batch Acc: 0.108986, Tokens per Sec:     3074, Lr: 0.000300
2024-05-23 21:32:55,925 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.763972, Batch Acc: 0.115290, Tokens per Sec:     3387, Lr: 0.000300
2024-05-23 21:33:17,329 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.748148, Batch Acc: 0.117125, Tokens per Sec:     3431, Lr: 0.000300
2024-05-23 21:33:38,860 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.661206, Batch Acc: 0.121709, Tokens per Sec:     3458, Lr: 0.000300
2024-05-23 21:34:00,260 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.615211, Batch Acc: 0.128243, Tokens per Sec:     3507, Lr: 0.000300
2024-05-23 21:34:00,261 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:34:00,261 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:35:54,907 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.66, ppl:  38.68, acc:   0.12, generation: 114.6351[sec], evaluation: 0.0000[sec]
2024-05-23 21:35:54,908 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:35:55,237 - INFO - joeynmt.training - Example #0
2024-05-23 21:35:55,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:35:55,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:35:55,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'habe', 'ich', 'habe', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in', 'der', 'ich', 'in']
2024-05-23 21:35:55,237 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:35:55,237 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:35:55,237 - INFO - joeynmt.training - 	Hypothesis: Ich habe ich habe ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in der ich in
2024-05-23 21:35:55,237 - INFO - joeynmt.training - Example #1
2024-05-23 21:35:55,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:35:55,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:35:55,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'der', 'W@@', 'ü@@', 'ch@@', ',', 'dass', 'es', 'ist', 'die', 'der', 'W@@', 'ü@@', 'd@@', 'ischen', 'W@@', 'au@@', 'k@@', 'ische', 'W@@', 'au@@', 'k@@', 'ische', 'W@@', 'au@@', 'k@@', 'ische', 'W@@', 'au@@', 'ch@@', '.', '</s>']
2024-05-23 21:35:55,237 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:35:55,237 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:35:55,237 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die der Wüch, dass es ist die der Wüdischen Waukische Waukische Waukische Wauch.
2024-05-23 21:35:55,237 - INFO - joeynmt.training - Example #2
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'W@@', 'an@@', 'an@@', 'ische', 'S@@', 'al@@', 'k@@', 'a', 'in', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'W@@', 'al@@', '.', '</s>']
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Hypothesis: Die Wananische Salka in der der der der der der der der der der der der der der der der der der der der der der der der der Wal.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - Example #3
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'in', 'der', 'S@@', 'on@@', 'a', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'und', 'sie', 'sind', 'in', 'der', 'B@@', 'ü@@', 'ß@@', '.', '</s>']
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Hypothesis: Es gibt es in der Sona und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und und sie sind in der Büß.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - Example #4
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:35:55,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'ich', 'habe', 'ich', 'habe', 'ich', 'mit', 'der', 'ich', 'mit', 'dem', 'ich', 'mit', 'dem', 'ich', 'mit', 'dem', 'ich', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'zu', 'zu', 'zu', 'zu', 'zu', 'haben.', '</s>']
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:35:55,238 - INFO - joeynmt.training - 	Hypothesis: Die ich habe ich habe ich mit der ich mit dem ich mit dem ich mit dem ich nicht nicht nicht nicht nicht zu zu zu zu zu haben.
2024-05-23 21:36:17,444 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.653515, Batch Acc: 0.135416, Tokens per Sec:     3314, Lr: 0.000300
2024-05-23 21:36:40,214 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.597436, Batch Acc: 0.141625, Tokens per Sec:     3244, Lr: 0.000300
2024-05-23 21:37:01,217 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.574579, Batch Acc: 0.146086, Tokens per Sec:     3465, Lr: 0.000300
2024-05-23 21:37:23,343 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.386022, Batch Acc: 0.149284, Tokens per Sec:     3235, Lr: 0.000300
2024-05-23 21:37:44,981 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.331380, Batch Acc: 0.156755, Tokens per Sec:     3356, Lr: 0.000300
2024-05-23 21:37:44,983 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:37:44,983 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:39:41,372 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.44, ppl:  31.34, acc:   0.16, generation: 116.3794[sec], evaluation: 0.0000[sec]
2024-05-23 21:39:41,373 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:39:41,745 - INFO - joeynmt.training - Example #0
2024-05-23 21:39:41,745 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:39:41,745 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:39:41,745 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ich', 'habe', 'ich', 'diese', 'Welt', 'von', 'dem', 'ich', 'die', 'Welt', 'von', 'dem', 'Welt', 'der', 'Welt', 'der', 'Welt', 'der', 'Welt', 'der', 'Welt', 'der', 'der', 'der', 'Welt', 'der', 'Welt', 'der', 'Welt', 'der', 'der', 'der', 'Welt', 'von', 'dem', 'Welt', 'von', 'dem', 'Ver@@', 'füg@@', 'ung', 'der', 'der', 'der', 'der', 'der', 'der', 'Welt', 'von', 'der', 'der', 'der', 'der', 'Welt', 'von', 'der', 'der', 'Welt', 'von', 'dem', 'Welt', 'von', 'dem', 'Welt', 'von', 'dem', 'Welt', 'der', 'Welt', 'der', 'Welt', 'von', 'den', 'von', 'dem', 'Welt', 'von', 'der', 'Welt', 'von', 'dem', 'Welt', 'von', 'dem', 'Ver@@', 'füg@@', 'ten.', '</s>']
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Hypothesis: Ich habe ich diese Welt von dem ich die Welt von dem Welt der Welt der Welt der Welt der Welt der der der Welt der Welt der Welt der der der Welt von dem Welt von dem Verfügung der der der der der der Welt von der der der der Welt von der der Welt von dem Welt von dem Welt von dem Welt der Welt der Welt von den von dem Welt von der Welt von dem Welt von dem Verfügten.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - Example #1
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'Welt', 'der', 'Welt', 'der', 'Welt', 'ist', 'nicht', 'nicht', 'nicht', 'nicht', 'nicht', 'die', 'Welt', 'ist', 'nicht', 'die', 'Welt', 'der', 'der', 'der', 'Welt', 'der', 'Welt', 'der', 'Welt', 'der', 'Welt', 'zu', 'tun', 'kann.', '</s>']
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Welt der Welt der Welt ist nicht nicht nicht nicht nicht die Welt ist nicht die Welt der der der Welt der Welt der Welt der Welt zu tun kann.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - Example #2
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ver@@', 'k@@', 'ä@@', 'ten', 'in', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'der', 'Welt', 'der', 'Welt', 'der', 'Welt', 'der', 'Welt', 'der', 'der', 'Welt', 'ver@@', 'öffent@@', 'lichen', 'Ver@@', 'füg@@', 'ung', 'der', 'Welt', 'ist.', '</s>']
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - 	Hypothesis: Die Verkäten in der der der der der der der der der Welt der Welt der Welt der Welt der der Welt veröffentlichen Verfügung der Welt ist.
2024-05-23 21:39:41,746 - INFO - joeynmt.training - Example #3
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:39:41,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'der', 'und', 'in', 'der', 'und', 'die', 'Ver@@', 'füg@@', 'ung', 'und', 'in', 'der', 'K@@', 'ü@@', 'ch@@', 'e.', '</s>']
2024-05-23 21:39:41,747 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:39:41,747 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:39:41,747 - INFO - joeynmt.training - 	Hypothesis: Es gibt in der und in der und die Verfügung und in der Küche.
2024-05-23 21:39:41,747 - INFO - joeynmt.training - Example #4
2024-05-23 21:39:41,747 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:39:41,747 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:39:41,747 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'erste', 'erste', 'erste', 'erste', 'erste', 'Leben', 'auf', 'der', 'Welt', 'zu', 'tun', 'können,', 'die', 'Welt', 'ist,', 'dass', 'die', 'Welt', 'zu', 'tun', 'können.', '</s>']
2024-05-23 21:39:41,747 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:39:41,747 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:39:41,747 - INFO - joeynmt.training - 	Hypothesis: Die erste erste erste erste erste Leben auf der Welt zu tun können, die Welt ist, dass die Welt zu tun können.
2024-05-23 21:40:04,319 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.525874, Batch Acc: 0.165986, Tokens per Sec:     3147, Lr: 0.000300
2024-05-23 21:40:26,100 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.269133, Batch Acc: 0.176268, Tokens per Sec:     3422, Lr: 0.000300
2024-05-23 21:40:47,750 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.441644, Batch Acc: 0.183536, Tokens per Sec:     3383, Lr: 0.000300
2024-05-23 21:41:09,777 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.225694, Batch Acc: 0.187381, Tokens per Sec:     3303, Lr: 0.000300
2024-05-23 21:41:31,594 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.182294, Batch Acc: 0.198841, Tokens per Sec:     3345, Lr: 0.000300
2024-05-23 21:41:31,596 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:41:31,596 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:43:15,406 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.23, ppl:  25.22, acc:   0.19, generation: 103.8015[sec], evaluation: 0.0000[sec]
2024-05-23 21:43:15,410 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:43:15,737 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/500.ckpt
2024-05-23 21:43:15,946 - INFO - joeynmt.training - Example #0
2024-05-23 21:43:15,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:43:15,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:43:15,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['N@@', 'ach', 'der', 'ich', 'zwei', 'zwei', 'zwei', 'zwei', 'zwei', 'zwei', 'zwei', 'von', 'zwei', 'von', 'den', 'M@@', 'ikro@@', 's@@', 's@@', 's@@', 's@@', 'etzt', 'die', 'die', 'drei', 'Jahren', 'von', 'drei', 'Jahren', 'von', 'drei', 'Jahren', 'von', 'drei', 'Jahren', 'von', '1@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@', '6@@']
2024-05-23 21:43:15,946 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:43:15,946 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:43:15,946 - INFO - joeynmt.training - 	Hypothesis: Nach der ich zwei zwei zwei zwei zwei zwei zwei von zwei von den Mikrossssetzt die die drei Jahren von drei Jahren von drei Jahren von drei Jahren von 166666666666666666666666666666666666666666666666666666
2024-05-23 21:43:15,946 - INFO - joeynmt.training - Example #1
2024-05-23 21:43:15,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:43:15,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:43:15,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'die', 'die', 'die', 'die', 'Ver@@', 'gangen@@', 'heit', 'der', 'der', 'nicht', 'die', 'die', 'die', 'die', 'Ver@@', 'gangen@@', 'heit', 'der', 'der', 'der', 'der', 'Ver@@', 'gangen@@', 'heit', 'des', 'des', 'des', 'K@@', 'rieg@@', '.', '</s>']
2024-05-23 21:43:15,946 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:43:15,946 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die die die die die Vergangenheit der der nicht die die die die Vergangenheit der der der der Vergangenheit des des des Krieg.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - Example #2
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ver@@', 'gangen@@', 'heit', 'ist', 'ein', 'S@@', 'at@@', 'at@@', 'at@@', 'at@@', 'at@@', 'at@@', 'at@@', 'at@@', 'ische', 'Ver@@', 'gangen@@', 'heit', 'der', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'füg@@', 'ung.', '</s>']
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Hypothesis: Die Vergangenheit ist ein Satatatatatatatatische Vergangenheit der der Vergangenheit der Vergangenheit der Vergangenheit der Verfügung.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - Example #3
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'den', 'B@@', 'est@@', 'in', 'und', 'in', 'den', 'B@@', 'est@@', 'est@@', 'en.', '</s>']
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Hypothesis: Es ist in den Bestin und in den Bestesten.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - Example #4
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:43:15,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'erste', 'erste', 'erste', 'Mal', 'habe', 'ich', 'Ihnen', 'ein', 'paar', 'paar', 'paar', 'paar', 'paar', 'Jahre', 'für', 'die', 'Welt', 'ist,', 'was', 'das', 'das', 'erste', 'erste', 'Jahre', 'für', 'die', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'Jahr@@', 'en.', '</s>']
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:43:15,947 - INFO - joeynmt.training - 	Hypothesis: Die erste erste erste Mal habe ich Ihnen ein paar paar paar paar paar Jahre für die Welt ist, was das das erste erste Jahre für die letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten Jahren.
2024-05-23 21:43:37,941 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.181841, Batch Acc: 0.205522, Tokens per Sec:     3257, Lr: 0.000300
2024-05-23 21:43:59,424 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.097438, Batch Acc: 0.208716, Tokens per Sec:     3424, Lr: 0.000300
2024-05-23 21:44:20,612 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     3.064111, Batch Acc: 0.212636, Tokens per Sec:     3473, Lr: 0.000300
2024-05-23 21:44:42,692 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.063602, Batch Acc: 0.218753, Tokens per Sec:     3334, Lr: 0.000300
2024-05-23 21:45:04,687 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     3.076553, Batch Acc: 0.220975, Tokens per Sec:     3473, Lr: 0.000300
2024-05-23 21:45:04,688 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:45:04,688 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:46:52,454 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.05, ppl:  21.11, acc:   0.22, generation: 107.7577[sec], evaluation: 0.0000[sec]
2024-05-23 21:46:52,457 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:46:52,810 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/1000.ckpt
2024-05-23 21:46:52,970 - INFO - joeynmt.training - Example #0
2024-05-23 21:46:52,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:46:52,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:46:52,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['N@@', 'ach', 'drei', 'Jahre', 'habe', 'ich', 'zwei', 'Millionen', 'von', 'zwei', 'Millionen', 'von', 'zwei', 'Millionen', 'von', 'drei', 'Jahren', 'die', 'letzten', 'drei', 'Millionen', 'von', 'drei', 'Millionen', 'Jahren', 'für', 'drei', 'Millionen', 'Jahren', 'für', 'drei', 'Millionen', 'Jahren', 'von', 'drei', 'Millionen', 'von', '1@@', '50', 'Millionen', 'von', '1@@', '50', 'Millionen', 'von', '1@@', '50', 'Prozent', 'Prozent', 'der', '1@@', '50', 'Prozent', 'Prozent', 'der', 'letzten', '1@@', '50', 'Prozent', 'Prozent', 'der', 'letzten', '1@@', '50', 'Prozent', 'der', 'letzten', '1@@', '50', 'Prozent', 'Prozent', 'der', 'letzten', '1@@', '50', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'Millionen', 'von', '1@@', '50', 'Millionen', 'Millionen', 'Millionen']
2024-05-23 21:46:52,970 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:46:52,970 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:46:52,970 - INFO - joeynmt.training - 	Hypothesis: Nach drei Jahre habe ich zwei Millionen von zwei Millionen von zwei Millionen von drei Jahren die letzten drei Millionen von drei Millionen Jahren für drei Millionen Jahren für drei Millionen Jahren von drei Millionen von 150 Millionen von 150 Millionen von 150 Prozent Prozent der 150 Prozent Prozent der letzten 150 Prozent Prozent der letzten 150 Prozent der letzten 150 Prozent Prozent der letzten 150 Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen Millionen von 150 Millionen Millionen Millionen
2024-05-23 21:46:52,971 - INFO - joeynmt.training - Example #1
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'An@@', 'satz', 'der', 'S@@', 'of@@', 't@@', 'eil', 'der', 'Welt', 'nicht', 'nur', 'die', 'Frage', 'der', 'nicht', 'nicht', 'die', 'W@@', 'ach@@', 'e.', '</s>']
2024-05-23 21:46:52,971 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:46:52,971 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:46:52,971 - INFO - joeynmt.training - 	Hypothesis: Aber diese Ansatz der Softeil der Welt nicht nur die Frage der nicht nicht die Wache.
2024-05-23 21:46:52,971 - INFO - joeynmt.training - Example #2
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'R@@', 'ep@@', 'ep@@', 'ep@@', 'ep@@', 'tem@@', 'tem@@', 'ber', 'ist', 'ein', 'W@@', 'ach@@', 'st@@', 'ens', 'in', 'der', 'Welt.', '</s>']
2024-05-23 21:46:52,971 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:46:52,971 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:46:52,971 - INFO - joeynmt.training - 	Hypothesis: Die Repepepeptemtember ist ein Wachstens in der Welt.
2024-05-23 21:46:52,971 - INFO - joeynmt.training - Example #3
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:46:52,971 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'sind', 'in', 'und', 'in', 'und', 'in', 'der', 'K@@', 'raft', 'und', 'in', 'der', 'K@@', 'raft', 'in', 'der', 'K@@', 'raft', 'in', 'der', 'K@@', 'raft', 'und', 'in', 'der', 'K@@', 'raft', 'und', 'in', 'der', 'K@@', 'raft', 'und', 'in', 'der', 'K@@', 'raft', 'und', 'in', 'und', 'in', 'der', 'K@@', 'raft', 'und', 'in', 'und', 'in', 'und', 'in', 'und', 'in', 'und', 'in', 'und', 'in', 'der', 'K@@', 'raft', 'und', 'in', 'und', 'in', 'und', 'in', 'und', 'ver@@', 'wandel@@', 'n.', '</s>']
2024-05-23 21:46:52,972 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:46:52,972 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:46:52,972 - INFO - joeynmt.training - 	Hypothesis: Es sind in und in und in der Kraft und in der Kraft in der Kraft in der Kraft und in der Kraft und in der Kraft und in der Kraft und in und in der Kraft und in und in und in und in und in und in der Kraft und in und in und in und verwandeln.
2024-05-23 21:46:52,972 - INFO - joeynmt.training - Example #4
2024-05-23 21:46:52,972 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:46:52,972 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:46:52,972 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'nächste', 'L@@', 'etz@@', 'e', 'ich', 'Ihnen', 'ein', 'bisschen', 'ein', 'paar', 'Jahren', 'haben', 'ein', 'paar', 'Jahren', 'für', 'die', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'Jahr@@', '.', '</s>']
2024-05-23 21:46:52,972 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:46:52,972 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:46:52,972 - INFO - joeynmt.training - 	Hypothesis: Die nächste nächste Letze ich Ihnen ein bisschen ein paar Jahren haben ein paar Jahren für die letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten Jahr.
2024-05-23 21:47:15,697 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.958739, Batch Acc: 0.224509, Tokens per Sec:     3169, Lr: 0.000300
2024-05-23 21:47:37,892 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     3.089531, Batch Acc: 0.231669, Tokens per Sec:     3318, Lr: 0.000300
2024-05-23 21:47:59,981 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.935866, Batch Acc: 0.238719, Tokens per Sec:     3351, Lr: 0.000300
2024-05-23 21:48:17,354 - INFO - joeynmt.training - Epoch   1: total training loss 14072.33
2024-05-23 21:48:17,355 - INFO - joeynmt.training - EPOCH 2
2024-05-23 21:48:21,948 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     2.745709, Batch Acc: 0.256151, Tokens per Sec:     3274, Lr: 0.000300
2024-05-23 21:48:43,802 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     2.874840, Batch Acc: 0.251231, Tokens per Sec:     3393, Lr: 0.000300
2024-05-23 21:48:43,803 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:48:43,803 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:50:33,238 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.92, ppl:  18.47, acc:   0.24, generation: 109.4248[sec], evaluation: 0.0000[sec]
2024-05-23 21:50:33,240 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:50:33,616 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/1500.ckpt
2024-05-23 21:50:33,726 - INFO - joeynmt.training - Example #0
2024-05-23 21:50:33,726 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:50:33,726 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:50:33,726 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Jahr', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Jahre', 'alt', 'und', 'so', 'dass', 'die', 'meisten', 'von', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'für', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', 'war', 'das', 'letzten', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'letzten', '1@@', '6@@', '5', 'Jahren', 'für', 'den', '1@@', '50', 'Jahren', 'hat', 'sich', '100', 'Millionen', 'von', '2@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', '-@@', 'M@@', 'ä@@', 'ä@@', 'he', 'von', 'dem', 'letzten', 'letzten', 'Millionen', 'Millionen', 'Millionen', 'Jahren', 'ver@@', 'öffent@@', 'lich@@', 't.', '</s>']
2024-05-23 21:50:33,726 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:50:33,726 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:50:33,726 - INFO - joeynmt.training - 	Hypothesis: Jahr Jahr habe ich diese zwei Jahre alt und so dass die meisten von den letzten drei Millionen Jahren die meisten der letzten drei Millionen Jahren für den letzten drei Millionen Jahren war das letzten drei Millionen Jahren hat die letzten 165 Jahren für den 150 Jahren hat sich 100 Millionen von 2--------------Määhe von dem letzten letzten Millionen Millionen Millionen Jahren veröffentlicht.
2024-05-23 21:50:33,726 - INFO - joeynmt.training - Example #1
2024-05-23 21:50:33,726 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:50:33,726 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:50:33,726 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'einig@@', 'ung', 'der', 'Problem', 'der', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'Ver@@', 'einig@@', 'ung', 'des', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'einig@@', 'ung', 'des', 'Ver@@', 'einig@@', 't.', '</s>']
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Hypothesis: Aber diese Vergangenheit der Vereinigung der Problem der Problem ist, weil es nicht die Vereinigung des Vergangenheit der Vergangenheit der Vergangenheit der Vereinigung des Vereinigt.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - Example #2
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ver@@', 't@@', 't@@', 't@@', 'eil', 'ist', 'ein', 'ein', 'W@@', 'ieder@@', 'sch@@', 'sch@@', 'sch@@', 'wer@@', 'k@@', 'k@@', 't.', '</s>']
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Hypothesis: Die Verttteil ist ein ein Wiederschschschwerkkt.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - Example #3
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ist', 'in', 'der', 'B@@', 'ü@@', 'cke', 'und', 'ver@@', 'öffent@@', 'lich@@', 't.', '</s>']
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Hypothesis: Es ist in der Bücke und veröffentlicht.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - Example #4
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:50:33,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'R@@', 'oll@@', 'en', 'ich', 'Ihnen', 'eine', 'D@@', 'ok@@', 'us@@', 's@@', 's@@', 's@@', 'eit@@', 'ung', 'von', 'dem', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'Jahr@@', 'en.', '</s>']
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:50:33,727 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:50:33,728 - INFO - joeynmt.training - 	Hypothesis: Die nächste Rollen ich Ihnen eine Dokusssseitung von dem letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten Jahren.
2024-05-23 21:50:56,199 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     2.756332, Batch Acc: 0.254090, Tokens per Sec:     3208, Lr: 0.000300
2024-05-23 21:51:17,515 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     2.863598, Batch Acc: 0.263572, Tokens per Sec:     3541, Lr: 0.000300
2024-05-23 21:51:39,547 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     2.840734, Batch Acc: 0.268020, Tokens per Sec:     3488, Lr: 0.000300
2024-05-23 21:52:01,580 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     2.692977, Batch Acc: 0.272165, Tokens per Sec:     3458, Lr: 0.000300
2024-05-23 21:52:24,130 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     2.720792, Batch Acc: 0.278914, Tokens per Sec:     3293, Lr: 0.000300
2024-05-23 21:52:24,131 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:52:24,131 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:53:59,324 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.80, ppl:  16.42, acc:   0.27, generation: 95.1851[sec], evaluation: 0.0000[sec]
2024-05-23 21:53:59,325 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:53:59,621 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/2000.ckpt
2024-05-23 21:53:59,750 - INFO - joeynmt.training - Example #0
2024-05-23 21:53:59,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:53:59,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:53:59,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'M@@', 'üt@@', 'ter', 'der', 'Ver@@', 'br@@', 'eit@@', 'ung', 'der', 'Ver@@', 'br@@', 'eit@@', 'ung', 'für', 'drei', 'Millionen', 'Jahre', 'alt', 'und', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'alt', 'hat', 'die', 'meisten', 'der', 'H@@', 'äl@@', 'fte', 'der', 'der', 'H@@', 'äl@@', 'fte', 'der', 'der', 'Ver@@', 'br@@', 'eit@@', 'ung', 'der', 'Ver@@', 'einig@@', 'ten', 'Sta@@', 'aten', 'ver@@', 'wandel@@', 't.', '</s>']
2024-05-23 21:53:59,750 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:53:59,750 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:53:59,750 - INFO - joeynmt.training - 	Hypothesis: Letzte Jahr habe ich diese zwei Mütter der Verbreitung der Verbreitung für drei Millionen Jahre alt und die meisten der letzten drei Millionen Jahre alt hat die meisten der Hälfte der der Hälfte der der Verbreitung der Vereinigten Staaten verwandelt.
2024-05-23 21:53:59,750 - INFO - joeynmt.training - Example #1
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'Sch@@', 'wi@@', 'es@@', 'se', 'des', 'Ver@@', 'gangen@@', 'heit', 'dieser', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ok@@', 'rat@@', 'ie', 'des', 'D@@', 'ok@@', 'ok@@', 'a@@', 'ik@@', '.', '</s>']
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Schwiesse des Vergangenheit dieser Problem ist, weil es nicht die Dokratie des Dokokaik.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - Example #2
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'R@@', 'olle', 'ist', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'Ver@@', 'gangen@@', 'heit', 'der', 'glob@@', 'alen', 'glob@@', 'alen', 'Ver@@', 'br@@', 'it@@', 'ung.', '</s>']
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Hypothesis: Die Rolle ist der Vergangenheit der Vergangenheit der Vergangenheit der Vergangenheit der globalen globalen Verbritung.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - Example #3
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'wird', 'in', 'den', 'W@@', 'ett@@', 'be@@', 'be@@', 'woh@@', 'n@@', 'ern', 'in', 'den', 'K@@', 'öpf@@', 'e', 'in', 'den', 'K@@', 'öpf@@', 'e', 'zu', 'ver@@', 'wandel@@', 'n.', '</s>']
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - 	Hypothesis: Es wird in den Wettbebewohnern in den Köpfe in den Köpfe zu verwandeln.
2024-05-23 21:53:59,751 - INFO - joeynmt.training - Example #4
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:53:59,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'M@@', 'üt@@', 'te', 'ich', 'Ihnen', 'Ihnen', 'Ihnen', 'eine', 'D@@', 'ok@@', 'ok@@', 'a', 'zu', 'einem', 'D@@', 'ru@@', 'ck', 'der', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'letzten', 'Jahr@@', 'en.', '</s>']
2024-05-23 21:53:59,752 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:53:59,752 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:53:59,752 - INFO - joeynmt.training - 	Hypothesis: Die nächste Mütte ich Ihnen Ihnen Ihnen eine Dokoka zu einem Druck der letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten letzten Jahren.
2024-05-23 21:54:22,682 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.581580, Batch Acc: 0.281391, Tokens per Sec:     3173, Lr: 0.000300
2024-05-23 21:54:43,826 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     2.787964, Batch Acc: 0.292455, Tokens per Sec:     3438, Lr: 0.000300
2024-05-23 21:55:05,803 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.589108, Batch Acc: 0.292120, Tokens per Sec:     3200, Lr: 0.000300
2024-05-23 21:55:27,032 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.607759, Batch Acc: 0.301669, Tokens per Sec:     3493, Lr: 0.000300
2024-05-23 21:55:49,260 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     2.735327, Batch Acc: 0.304622, Tokens per Sec:     3256, Lr: 0.000300
2024-05-23 21:55:49,261 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:55:49,261 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 21:57:20,920 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.68, ppl:  14.55, acc:   0.30, generation: 91.6507[sec], evaluation: 0.0000[sec]
2024-05-23 21:57:20,921 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 21:57:21,203 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/2500.ckpt
2024-05-23 21:57:21,372 - INFO - joeynmt.training - Example #0
2024-05-23 21:57:21,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 21:57:21,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 21:57:21,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Millionen', 'Jahre', 'alt', 'ist,', 'dass', 'die', 'meisten', 'von', 'drei', 'Millionen', 'Jahren', 'die', 'meisten', 'von', 'drei', 'Millionen', 'Jahren', 'hat', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'alt', 'hat', 'hat', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'alt', 'hat', 'hat', 'die', '4@@', '8', 'Prozent', 'der', 'Erde', 'hat', 'sich', 'um', '40', 'Prozent', 'der', '4@@', '8', '%', 'der', 'Proz@@', 'ent.', '</s>']
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Millionen Jahre alt ist, dass die meisten von drei Millionen Jahren die meisten von drei Millionen Jahren hat die meisten drei Millionen Jahre alt hat hat die meisten drei Millionen Jahre alt hat hat die 48 Prozent der Erde hat sich um 40 Prozent der 48 % der Prozent.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - Example #1
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'D@@', 'rit@@', 'tel', 'der', 'Ver@@', 'einig@@', 'ten', 'dieser', 'bestimm@@', 'ten', 'Problem', 'ist,', 'weil', 'es', 'das', 'D@@', 'rit@@', 'tel', 'der', 'D@@', 'ic@@', 'kt', 'des', 'D@@', 'ic@@', 's.', '</s>']
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Drittel der Vereinigten dieser bestimmten Problem ist, weil es das Drittel der Dickt des Dics.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - Example #2
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'm', 'ist', 'ist', 'ein', 'S@@', 'ehr', 'ist', 'in', 'einem', 'S@@', 'en@@', 'en@@', 'k@@', 'rit@@', 'ischen', 'Ver@@', 'füg@@', 'ung', 'des', 'des', 'glob@@', 'ale', '</s>']
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 21:57:21,373 - INFO - joeynmt.training - 	Hypothesis: Die Arm ist ist ein Sehr ist in einem Senenkritischen Verfügung des des globale
2024-05-23 21:57:21,373 - INFO - joeynmt.training - Example #3
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 21:57:21,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ver@@', 'öffent@@', 'lich@@', 'erweise', 'in', 'den', 'W@@', 'ieder@@', 'sch@@', 'r@@', 'än@@', 'z@@', 'en.', '</s>']
2024-05-23 21:57:21,374 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 21:57:21,374 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 21:57:21,374 - INFO - joeynmt.training - 	Hypothesis: Es veröffentlicherweise in den Wiederschränzen.
2024-05-23 21:57:21,374 - INFO - joeynmt.training - Example #4
2024-05-23 21:57:21,374 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 21:57:21,374 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 21:57:21,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'D@@', 'rit@@', 'te', 'ich', 'Ihnen', 'werden', 'ein', 'B@@', 'ru@@', 'ck', 'ein', 'B@@', 'erg@@', 'wer@@', 'k@@', 'el', 'von', 'der', 'letzten', '25', 'Jahren', 'ist', 'das', 'im', 'letzten', '25', 'Jahren', 'in', 'der', 'letzten', '25', 'Jahren', 'ist', 'das', 'im', 'letzten', '25', 'Jahren', 'werden', 'zu', 'sein.', '</s>']
2024-05-23 21:57:21,374 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 21:57:21,374 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 21:57:21,374 - INFO - joeynmt.training - 	Hypothesis: Die nächste Dritte ich Ihnen werden ein Bruck ein Bergwerkel von der letzten 25 Jahren ist das im letzten 25 Jahren in der letzten 25 Jahren ist das im letzten 25 Jahren werden zu sein.
2024-05-23 21:57:43,759 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.671623, Batch Acc: 0.305859, Tokens per Sec:     3324, Lr: 0.000300
2024-05-23 21:58:04,810 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.648286, Batch Acc: 0.312772, Tokens per Sec:     3573, Lr: 0.000300
2024-05-23 21:58:27,141 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.655720, Batch Acc: 0.321375, Tokens per Sec:     3244, Lr: 0.000300
2024-05-23 21:58:48,899 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.624417, Batch Acc: 0.327136, Tokens per Sec:     3377, Lr: 0.000300
2024-05-23 21:59:11,764 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     2.546394, Batch Acc: 0.332574, Tokens per Sec:     3263, Lr: 0.000300
2024-05-23 21:59:11,765 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:59:11,766 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:00:31,146 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.58, ppl:  13.26, acc:   0.32, generation: 79.3723[sec], evaluation: 0.0000[sec]
2024-05-23 22:00:31,146 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:00:31,456 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/3000.ckpt
2024-05-23 22:00:31,613 - INFO - joeynmt.training - Example #0
2024-05-23 22:00:31,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:00:31,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:00:31,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'L@@', 'etz@@', 'e', 'der', 'K@@', 'ünst@@', 'ler', 'der', 'K@@', 'ünst@@', 'ler', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'Jahre', 'lang', 'lang', 'lang', 'wurde', 'für', 'den', 'letzten', 'drei', 'Millionen', 'Jahre', 'Jahre', 'alt', 'hat', 'hat', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Ver@@', 'einig@@', 'ten', 'Sta@@', 'at@@', 'en,', 'die', 'von', '40', 'Prozent', 'von', '40', 'Prozent', 'von', '40', 'Prozent', 'zu', 'sein.', '</s>']
2024-05-23 22:00:31,613 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:00:31,613 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:00:31,613 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Letze der Künstler der Künstler der letzten drei Millionen Jahre Jahre lang lang lang wurde für den letzten drei Millionen Jahre Jahre alt hat hat die Größe der Größe der Größe der Vereinigten Staaten, die von 40 Prozent von 40 Prozent von 40 Prozent zu sein.
2024-05-23 22:00:31,613 - INFO - joeynmt.training - Example #1
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'S@@', 'icht@@', 'weise', 'der', 'W@@', 'ach@@', 'st@@', 'st@@', 'off', 'nicht', 'die', 'D@@', 'ru@@', 'ck', 'der', 'der', 'N@@', 'amen', 'des', 'D@@', 'ok@@', 'ok@@', 'ok@@', 'rat@@', '.', '</s>']
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die Sichtweise der Wachststoff nicht die Druck der der Namen des Dokokokrat.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - Example #2
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'tik@@', 'el', 'ist', 'ist', 'ein', 'S@@', 'oh@@', 'n@@', 'ung,', 'in', 'einem', 'S@@', 'oh@@', 'n@@', 'ungs@@', 'k@@', 'raft', 'der', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'der', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'der', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'der', 'Klima@@', 'wandel@@', '.', '</s>']
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Hypothesis: Die Artikel ist ist ein Sohnung, in einem Sohnungskraft der globalen Klimawandel der globalen Klimawandel der globalen Klimawandel der Klimawandel.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - Example #3
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'wird', 'auch', 'in', 'den', 'W@@', 'ör@@', 'ter', 'und', 'Kon@@', 'z@@', 'ept', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'z@@', 'ept', 'zu', 'sein.', '</s>']
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - 	Hypothesis: Es wird auch in den Wörter und Konzept in Sommer und Konzept zu sein.
2024-05-23 22:00:31,614 - INFO - joeynmt.training - Example #4
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:00:31,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'L@@', 'etz@@', 'e', 'werde', 'ich', 'Ihnen', 'ein', 'R@@', 'oh@@', 'oh@@', 'stoff@@', 'e', 'zu', 'sein,', 'was', 'was', 'passiert', 'ist,', 'was', 'was', 'passiert', 'ist.', '</s>']
2024-05-23 22:00:31,615 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:00:31,615 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:00:31,615 - INFO - joeynmt.training - 	Hypothesis: Die nächste Letze werde ich Ihnen ein Rohohstoffe zu sein, was was passiert ist, was was passiert ist.
2024-05-23 22:00:53,871 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.529077, Batch Acc: 0.332258, Tokens per Sec:     3274, Lr: 0.000300
2024-05-23 22:01:15,894 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     2.471045, Batch Acc: 0.337939, Tokens per Sec:     3323, Lr: 0.000300
2024-05-23 22:01:37,879 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     2.483571, Batch Acc: 0.348331, Tokens per Sec:     3341, Lr: 0.000300
2024-05-23 22:01:59,927 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     2.469535, Batch Acc: 0.341262, Tokens per Sec:     3299, Lr: 0.000300
2024-05-23 22:02:22,883 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.414792, Batch Acc: 0.353234, Tokens per Sec:     3360, Lr: 0.000300
2024-05-23 22:02:22,884 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:02:22,884 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:03:46,909 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.48, ppl:  11.92, acc:   0.35, generation: 84.0166[sec], evaluation: 0.0000[sec]
2024-05-23 22:03:46,910 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:03:47,180 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/3500.ckpt
2024-05-23 22:03:47,366 - INFO - joeynmt.training - Example #0
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'to@@', 's', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'kt@@', 'ische', 'Fähig@@', 'keiten', 'zu', 'dieser', 'Ar@@', 'kt@@', 'kt@@', 'ische', 'Fähig@@', 'keiten', 'in', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'wurde', 'die', 'Größ@@', 'e', 'der', 'der', 'ganzen', 'ganzen', 'ganzen', 'ganzen', 'ganzen', 'ganzen', 'ganzen', 'ganzen', 'ganzen', 'ganzen', 'Tag', 'zu', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 22:03:47,366 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:03:47,366 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:03:47,366 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Fotos so dass die Arktktische Fähigkeiten zu dieser Arktktische Fähigkeiten in der letzten drei Millionen Jahren wurde die Größe der der ganzen ganzen ganzen ganzen ganzen ganzen ganzen ganzen ganzen ganzen Tag zu 40 Prozent.
2024-05-23 22:03:47,366 - INFO - joeynmt.training - Example #1
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'W@@', 'ahl', 'der', 'W@@', 'ahl', 'dieser', 'spezi@@', 'ell', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'enk@@', 'enk@@', 'weise', 'der', 'D@@', 'enk@@', 'enk@@', 'enk@@', 'en.', '</s>']
2024-05-23 22:03:47,366 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:03:47,366 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:03:47,366 - INFO - joeynmt.training - 	Hypothesis: Aber das Wahl der Wahl dieser speziell dieses Problem ist, weil es nicht die Denkenkweise der Denkenkenken.
2024-05-23 22:03:47,366 - INFO - joeynmt.training - Example #2
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:03:47,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:03:47,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'kt@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'tern@@', 'e,', 'die', 'die', 'Ver@@', 'gangen@@', 'heit', 'der', 'glob@@', 'alen', 'Klima@@', 'wandel@@', '.', '</s>']
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Hypothesis: Die Arktktktische Eis ist in einem Sterne, die die Vergangenheit der globalen Klimawandel.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - Example #3
2024-05-23 22:03:47,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:03:47,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:03:47,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ver@@', 'wandel@@', 't', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'sum@@', 'enten', 'in', 'S@@', 'omm@@', 'er', 'zu', 'ver@@', 'wandel@@', 'n.', '</s>']
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Hypothesis: Es verwandelt in den Sommer und Konsumenten in Sommer zu verwandeln.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - Example #4
2024-05-23 22:03:47,367 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:03:47,367 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:03:47,367 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'to', 'zei@@', 'ge', 'ich', 'Ihnen', 'werden', 'ein', 'R@@', 'ek@@', 't@@', 'eil', 'der', 'letzten', '25', 'Jahre', 'lang', 'ist,', 'was', 'passier@@', 'te', 'in', 'der', 'letzten', '25', 'Jahre', 'passier@@', 't.', '</s>']
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:03:47,367 - INFO - joeynmt.training - 	Hypothesis: Die nächste Foto zeige ich Ihnen werden ein Rekteil der letzten 25 Jahre lang ist, was passierte in der letzten 25 Jahre passiert.
2024-05-23 22:04:10,999 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     2.414465, Batch Acc: 0.346471, Tokens per Sec:     3024, Lr: 0.000300
2024-05-23 22:04:32,857 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     2.462843, Batch Acc: 0.354385, Tokens per Sec:     3429, Lr: 0.000300
2024-05-23 22:04:54,877 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     2.447276, Batch Acc: 0.360013, Tokens per Sec:     3391, Lr: 0.000300
2024-05-23 22:05:16,560 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     2.477836, Batch Acc: 0.358345, Tokens per Sec:     3389, Lr: 0.000300
2024-05-23 22:05:38,434 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.454154, Batch Acc: 0.363055, Tokens per Sec:     3269, Lr: 0.000300
2024-05-23 22:05:38,435 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:05:38,435 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:07:03,308 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.40, ppl:  11.08, acc:   0.36, generation: 84.8648[sec], evaluation: 0.0000[sec]
2024-05-23 22:07:03,310 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:07:03,708 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/4000.ckpt
2024-05-23 22:07:03,884 - INFO - joeynmt.training - Example #0
2024-05-23 22:07:03,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'togra@@', 'f@@', 'ie', 'so', 'dass', 'die', 'Ar@@', 'tik@@', 'tik@@', 'ale', 'ist', 'die', 'Ar@@', 'tik@@', 'tik@@', 'ale', 'Kap@@', ',', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'alt', 'war', 'die', 'Größ@@', 'e', 'der', 'größ@@', 'ten', 'Mon@@', 'ate', 'hat', '4@@', '8', 'Prozent', 'der', '40', 'Prozent', 'der', '8@@', '%', 'zu', 'sehen.', '</s>']
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Fotografie so dass die Artiktikale ist die Artiktikale Kap, die meisten drei Millionen Jahre lang der letzten drei Millionen Jahre alt war die Größe der größten Monate hat 48 Prozent der 40 Prozent der 8% zu sehen.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - Example #1
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ern@@', 'st', 'die', 'ern@@', 's@@', 's@@', 's@@', 'eit@@', 'ung', 'dieses', 'spezi@@', 'ell', 'Problem', 'ist,', 'weil', 'es', 'die', 'D@@', 'ic@@', 'ken', 'des', 'E@@', 'is@@', 'mus@@', 's.', '</s>']
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Hypothesis: Aber das ernst die ernssseitung dieses speziell Problem ist, weil es die Dicken des Eismuss.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - Example #2
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'k@@', 't@@', 't@@', 't@@', 'ische', 'K@@', 'a@@', 'p', 'ist,', 'dass', 'die', 'Ver@@', 'gangen@@', 'heit', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', '</s>']
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:07:03,885 - INFO - joeynmt.training - 	Hypothesis: Die Arktttische Kap ist, dass die Vergangenheit des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen Klimasystem des globalen Klimawandel des globalen Klimawandel des globalen Klimawandel des globalen
2024-05-23 22:07:03,885 - INFO - joeynmt.training - Example #3
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:07:03,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:07:03,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ver@@', 'r@@', 'än@@', 'glich@@', 'er', 'und', 'Kon@@', 'z@@', 'ept', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'sum@@', 'enten', 'in', 'S@@', 'omm@@', 'en.', '</s>']
2024-05-23 22:07:03,886 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:07:03,886 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:07:03,886 - INFO - joeynmt.training - 	Hypothesis: Es verränglicher und Konzept in Sommer und Konsumenten in Sommen.
2024-05-23 22:07:03,886 - INFO - joeynmt.training - Example #4
2024-05-23 22:07:03,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:07:03,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:07:03,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'togra@@', 'f', 'ich', 'Ihnen', 'Ihnen', 'zeigen', 'ein', 'R@@', 'oll@@', 't@@', 'eil', 'von', 'einem', 'R@@', 'oll@@', 'en', 'von', 'der', 'letzten', '25', 'Jahre', 'alt', 'ist.', '</s>']
2024-05-23 22:07:03,886 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:07:03,886 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:07:03,886 - INFO - joeynmt.training - 	Hypothesis: Die nächste Fotograf ich Ihnen Ihnen zeigen ein Rollteil von einem Rollen von der letzten 25 Jahre alt ist.
2024-05-23 22:07:27,064 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     2.446834, Batch Acc: 0.364026, Tokens per Sec:     3103, Lr: 0.000300
2024-05-23 22:07:49,121 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.324551, Batch Acc: 0.365525, Tokens per Sec:     3290, Lr: 0.000300
2024-05-23 22:08:11,714 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     2.402009, Batch Acc: 0.368122, Tokens per Sec:     3281, Lr: 0.000300
2024-05-23 22:08:33,516 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     2.289589, Batch Acc: 0.366715, Tokens per Sec:     3388, Lr: 0.000300
2024-05-23 22:08:56,262 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     2.404005, Batch Acc: 0.376882, Tokens per Sec:     3214, Lr: 0.000300
2024-05-23 22:08:56,264 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:08:56,264 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:10:20,411 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.35, ppl:  10.53, acc:   0.37, generation: 84.1394[sec], evaluation: 0.0000[sec]
2024-05-23 22:10:20,412 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:10:20,714 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/4500.ckpt
2024-05-23 22:10:20,859 - INFO - joeynmt.training - Example #0
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'to', 'hat', 'diese', 'zwei', 'Fo@@', 'to', 'von', 'der', 'Ar@@', 'kt@@', 'ischen', 'Kap@@', ',', 'die', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'lo@@', 'k@@', 'alen', 'Sta@@', 'aten', 'hat', 'die', 'Größ@@', 'e', 'der', 'ganzen', 'ganzen', 'drei', 'Prozent', 'zu', 'sehen.', '</s>']
2024-05-23 22:10:20,859 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:10:20,859 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:10:20,859 - INFO - joeynmt.training - 	Hypothesis: Letzte ich diese zwei Foto hat diese zwei Foto von der Arktischen Kap, die die Arktische Kap, die die meisten der letzten drei Millionen Jahre lang der Größe der Größe der lokalen Staaten hat die Größe der ganzen ganzen drei Prozent zu sehen.
2024-05-23 22:10:20,859 - INFO - joeynmt.training - Example #1
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Unter@@', 'such@@', 'ung', 'der', 'S@@', 'en@@', 'heit', 'dieser', 'bestimm@@', 'te', 'Problem', 'der', 'D@@', 'enk@@', 'ungs@@', 'k@@', 'raft', 'der', 'D@@', 'enk@@', 'ungs@@', 'k@@', 'raft', 'der', 'D@@', 'enk@@', 'ungs@@', 'ver@@', 'urs@@', 'ach@@', 't.', '</s>']
2024-05-23 22:10:20,859 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:10:20,859 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:10:20,859 - INFO - joeynmt.training - 	Hypothesis: Aber das Untersuchung der Senheit dieser bestimmte Problem der Denkungskraft der Denkungskraft der Denkungsverursacht.
2024-05-23 22:10:20,859 - INFO - joeynmt.training - Example #2
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:10:20,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'kt@@', 'ische', 'E@@', 't@@', 't@@', 't@@', 't@@', 't@@', 'ik@@', ',', 'die', 'S@@', 'en@@', 'heit', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'Klima@@', '-@@', 'System@@', 's.', '</s>']
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Hypothesis: Die Arktktische Etttttik, die Senheit des globalen Klimawandel des Klima-Systems.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - Example #3
2024-05-23 22:10:20,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:10:20,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:10:20,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'ver@@', 'rück@@', 't', 'in', 'W@@', 'en@@', 'f@@', 'ör@@', 'm@@', 'ig@@', '.', '</s>']
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Hypothesis: Es verrückt in Wenförmig.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - Example #4
2024-05-23 22:10:20,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:10:20,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:10:20,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'to', 'werde', 'ich', 'Ihnen', 'Ihnen', 'ein', 'R@@', 'is@@', 'ik@@', 'o', 'be@@', 'gr@@', 'enz@@', 't', 'ist.', 'Das', 'passier@@', 'te', 'in', 'der', 'letzten', '25', 'Jah@@', 're.', '</s>']
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:10:20,860 - INFO - joeynmt.training - 	Hypothesis: Die nächste Foto werde ich Ihnen Ihnen ein Risiko begrenzt ist. Das passierte in der letzten 25 Jahre.
2024-05-23 22:10:44,363 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     2.401898, Batch Acc: 0.376230, Tokens per Sec:     3081, Lr: 0.000300
2024-05-23 22:11:06,049 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     2.367821, Batch Acc: 0.379916, Tokens per Sec:     3439, Lr: 0.000300
2024-05-23 22:11:27,533 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     2.217493, Batch Acc: 0.380987, Tokens per Sec:     3428, Lr: 0.000300
2024-05-23 22:11:50,374 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     2.270876, Batch Acc: 0.384562, Tokens per Sec:     3212, Lr: 0.000300
2024-05-23 22:12:12,447 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     2.167455, Batch Acc: 0.383465, Tokens per Sec:     3357, Lr: 0.000300
2024-05-23 22:12:12,448 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:12:12,448 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:13:43,072 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.31, ppl:  10.06, acc:   0.38, generation: 90.6162[sec], evaluation: 0.0000[sec]
2024-05-23 22:13:43,074 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:13:43,351 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/5000.ckpt
2024-05-23 22:13:43,467 - INFO - joeynmt.training - Example #0
2024-05-23 22:13:43,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:13:43,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:13:43,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'togra@@', 'f@@', 'iert', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'der', 'die', 'meisten', 'von', 'den', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Prozent', 'zu', 'machen.', '</s>']
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Fotografiert so dass die Arktische Kap, die Arktische Kap, der die meisten von den letzten drei Millionen Jahren die Größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der letzten drei Prozent zu machen.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - Example #1
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'ellen', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'D@@', 'ic@@', 'kt', 'ist.', '</s>']
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Hypothesis: Aber das ist die ernsthaft dieses speziellen Problem ist, weil es nicht das Dicksal des Dickt ist.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - Example #2
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'S@@', 'en@@', 's,', 'der', 'der', 'der', 'glob@@', 'alen', 'Klima@@', 'wandel@@', '.', '</s>']
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eis ist in einem Sens, der der der globalen Klimawandel.
2024-05-23 22:13:43,468 - INFO - joeynmt.training - Example #3
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:13:43,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'hat', 'in', 'W@@', 'ör@@', 'ter', 'und', 'Kon@@', 'z@@', 'ept', 'in', 'S@@', 'omm@@', 'er', 'und', 'ver@@', 'fol@@', 'g@@', 't.', '</s>']
2024-05-23 22:13:43,469 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:13:43,469 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:13:43,469 - INFO - joeynmt.training - 	Hypothesis: Es hat in Wörter und Konzept in Sommer und verfolgt.
2024-05-23 22:13:43,469 - INFO - joeynmt.training - Example #4
2024-05-23 22:13:43,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:13:43,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:13:43,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'li@@', 'en', 'werde', 'ich', 'Ihnen', 'ein', 'R@@', 'ön@@', 't@@', 'gen@@', '-@@', 'St@@', 'ell@@', 'ung', 'von', 'dem', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 22:13:43,469 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:13:43,469 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:13:43,469 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folien werde ich Ihnen ein Röntgen-Stellung von dem letzten 25 Jahren passiert ist.
2024-05-23 22:14:05,805 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     2.291095, Batch Acc: 0.387674, Tokens per Sec:     3232, Lr: 0.000300
2024-05-23 22:14:27,642 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     2.360740, Batch Acc: 0.385890, Tokens per Sec:     3368, Lr: 0.000300
2024-05-23 22:14:38,605 - INFO - joeynmt.training - Epoch   2: total training loss 9777.97
2024-05-23 22:14:38,606 - INFO - joeynmt.training - EPOCH 3
2024-05-23 22:14:49,625 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     2.199340, Batch Acc: 0.404560, Tokens per Sec:     3332, Lr: 0.000300
2024-05-23 22:15:11,836 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     2.267621, Batch Acc: 0.409276, Tokens per Sec:     3360, Lr: 0.000300
2024-05-23 22:15:33,284 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     2.145540, Batch Acc: 0.405587, Tokens per Sec:     3510, Lr: 0.000300
2024-05-23 22:15:33,285 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:15:33,285 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:16:55,014 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.27, ppl:   9.68, acc:   0.39, generation: 81.7205[sec], evaluation: 0.0000[sec]
2024-05-23 22:16:55,016 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:16:55,328 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/5500.ckpt
2024-05-23 22:16:55,489 - INFO - joeynmt.training - Example #0
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'togra@@', 'f@@', 'iert', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'Jahre', 'war', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'lo@@', 'k@@', 'ale', 'Sta@@', 'aten', 'hat', 'die', 'Größ@@', 'e', 'der', 'lo@@', 'k@@', 'alen', 'Sta@@', 'aten', 'ge@@', 'zeigt', 'hat.', '</s>']
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Fotografiert so dass die Arktische Kap, die die Arktische Kap, die die meisten der letzten drei Millionen Jahre Jahre war die Größe der Größe der lokale Staaten hat die Größe der lokalen Staaten gezeigt hat.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - Example #1
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'einig@@', 'ten', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem', 'zu', 'zeigen.', 'Denn', 'es', 'Ihnen', 'das', 'D@@', 'ic@@', 'kt', 'der', 'E@@', 'is@@', 'se', 'des', 'E@@', 'is@@', 'ik@@', '.', '</s>']
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Hypothesis: Aber das Vereinigten die ernsthaft dieses Problem zu zeigen. Denn es Ihnen das Dickt der Eisse des Eisik.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - Example #2
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:16:55,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ik@@', ',', 'ist', 'in', 'einem', 'S@@', 'to@@', 't@@', 'z', 'der', 'glob@@', 'alen', 'Klima@@', 'system', 'der', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:16:55,490 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisik, ist in einem Stotz der globalen Klimasystem der globalen Klimasystem des globalen Klimasystem des Klimasystem
2024-05-23 22:16:55,491 - INFO - joeynmt.training - Example #3
2024-05-23 22:16:55,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:16:55,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:16:55,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'schein@@', 't', 'in', 'W@@', 'aff@@', 'en', 'und', 'Kon@@', 'z@@', 'ept', 'in', 'S@@', 'omm@@', 'er', 'zu', 'ver@@', 'fol@@', 'gen.', '</s>']
2024-05-23 22:16:55,491 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:16:55,491 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:16:55,491 - INFO - joeynmt.training - 	Hypothesis: Es erscheint in Waffen und Konzept in Sommer zu verfolgen.
2024-05-23 22:16:55,491 - INFO - joeynmt.training - Example #4
2024-05-23 22:16:55,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:16:55,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:16:55,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'ich', 'Ihnen', 'ein', 'R@@', 'ah@@', 'n', 'zeigen,', 'was', 'über', 'das', 'letzte', '25', 'Jahre', 'in', 'der', 'letzten', '25', 'Jahr@@', 'en.', '</s>']
2024-05-23 22:16:55,491 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:16:55,491 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:16:55,491 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen ich Ihnen ein Rahn zeigen, was über das letzte 25 Jahre in der letzten 25 Jahren.
2024-05-23 22:17:18,775 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     2.236160, Batch Acc: 0.406970, Tokens per Sec:     3064, Lr: 0.000300
2024-05-23 22:17:40,450 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     2.102131, Batch Acc: 0.408284, Tokens per Sec:     3416, Lr: 0.000300
2024-05-23 22:18:01,998 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     2.038574, Batch Acc: 0.409295, Tokens per Sec:     3366, Lr: 0.000300
2024-05-23 22:18:24,787 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     2.279438, Batch Acc: 0.404592, Tokens per Sec:     3110, Lr: 0.000300
2024-05-23 22:18:47,186 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     2.310094, Batch Acc: 0.410475, Tokens per Sec:     3227, Lr: 0.000300
2024-05-23 22:18:47,187 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:18:47,187 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:20:05,566 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.23, ppl:   9.34, acc:   0.40, generation: 78.3705[sec], evaluation: 0.0000[sec]
2024-05-23 22:20:05,567 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:20:05,870 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/6000.ckpt
2024-05-23 22:20:05,983 - INFO - joeynmt.training - Example #0
2024-05-23 22:20:05,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:20:05,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:20:05,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Lo@@', 'ch', 'der', 'lo@@', 'k@@', 'ale', 'Sta@@', 'aten', 'ge@@', 'zeigt,', 'dass', 'die', 'ar@@', 'k@@', 'u@@', 'ck@@', 't', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigt habe, dass die Arktische Kap, die Arktische Kap, die meisten der letzten drei Millionen Jahre lang der Loch der lokale Staaten gezeigt, dass die arkuckt von 40 Prozent.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - Example #1
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'such@@', 'ungen', 'des', 'Aus@@', 'ma@@', 'ß', 'dieses', 'spezi@@', 'f@@', 'ischen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'mus@@', '.', '</s>']
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Hypothesis: Aber diese Untersuchungen des Ausmaß dieses spezifischen Problem, weil es nicht die Dicke des Eismus.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - Example #2
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', 'e,', 'die', 'S@@', 'ens@@', 'ch', 'des', 'glob@@', 'alen', 'Klima@@', 'wandel@@', 's', 'des', 'glob@@', 'alen', 'Klima@@', 'wandel@@', '-@@', 'System@@', 's', 'des', 'glob@@', 'alen', 'Klima@@', 'wandel@@', 's', 'des', 'glob@@', 'alen', 'Klima@@', 'wandel@@', 's', '</s>']
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:20:05,984 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisis ist in einem Sinne, die Sensch des globalen Klimawandels des globalen Klimawandel-Systems des globalen Klimawandels des globalen Klimawandels
2024-05-23 22:20:05,984 - INFO - joeynmt.training - Example #3
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:20:05,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'eine', 'W@@', 'än@@', 'ge', 'und', 'Kon@@', 'tra@@', 'fen', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'fen', 'in', 'S@@', 'omm@@', 'er@@', '-@@', 'und', 'Kon@@', 'tra@@', 'f', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'f', 'in', 'S@@', 'omm@@', 'er@@', 'sch@@', 'wer@@', '.', '</s>']
2024-05-23 22:20:05,985 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:20:05,985 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:20:05,985 - INFO - joeynmt.training - 	Hypothesis: Es gibt eine Wänge und Kontrafen in Sommer und Kontrafen in Sommer-und Kontraf in Sommer und Kontraf in Sommerschwer.
2024-05-23 22:20:05,985 - INFO - joeynmt.training - Example #4
2024-05-23 22:20:05,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:20:05,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:20:05,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'ich', 'Ihnen', 'Ihnen', 'zeigen', 'zeigen', 'werde', 'ich', 'Ihnen', 'schn@@', 'ell@@', '-@@', 'St@@', 'ro@@', 'm@@', 'utz@@', 'ung', 'von', 'dem', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 22:20:05,985 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:20:05,985 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:20:05,985 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen ich Ihnen Ihnen zeigen zeigen werde ich Ihnen schnell-Stromutzung von dem letzten 25 Jahren passiert.
2024-05-23 22:20:29,140 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     2.136064, Batch Acc: 0.415688, Tokens per Sec:     3221, Lr: 0.000300
2024-05-23 22:20:52,096 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     2.127058, Batch Acc: 0.415054, Tokens per Sec:     3322, Lr: 0.000300
2024-05-23 22:21:14,068 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     2.140934, Batch Acc: 0.414298, Tokens per Sec:     3386, Lr: 0.000300
2024-05-23 22:21:36,530 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     2.137341, Batch Acc: 0.411520, Tokens per Sec:     3313, Lr: 0.000300
2024-05-23 22:21:58,940 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     2.242931, Batch Acc: 0.416937, Tokens per Sec:     3251, Lr: 0.000300
2024-05-23 22:21:58,941 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:21:58,941 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:23:32,136 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.22, ppl:   9.19, acc:   0.40, generation: 93.1868[sec], evaluation: 0.0000[sec]
2024-05-23 22:23:32,136 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:23:32,424 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/6500.ckpt
2024-05-23 22:23:32,609 - INFO - joeynmt.training - Example #0
2024-05-23 22:23:32,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:23:32,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:23:32,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'die', 'Ar@@', 'kt@@', 'is', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'des', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'der', 'Z@@', 'ahl', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 22:23:32,609 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:23:32,609 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:23:32,609 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die Arktis die Arktis für die meisten letzten drei Millionen Jahre lang der letzten drei Millionen Jahre lang der Größe des letzten drei Millionen Jahre lang der Größe der Zahl von 40 Prozent.
2024-05-23 22:23:32,609 - INFO - joeynmt.training - Example #1
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'such@@', 'ungen', 'der', 'ern@@', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'ellen', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Hypothesis: Aber diese Untersuchungen der ernernsthaft dieses speziellen Problem ist, weil es nicht die Dicke des Eiss.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - Example #2
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'ist', 'eine', 'S@@', 'en@@', 'en@@', 'en@@', 's,', 'der', 'Ver@@', 'such@@', 'ung', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Hypothesis: Die Arktis ist eine Senenens, der Versuchung des Klimasystem.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - Example #3
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'de', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'zu', 'sein.', '</s>']
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - 	Hypothesis: Es expande und Kontrakten in Sommer und Kontrakten in Sommer zu sein.
2024-05-23 22:23:32,610 - INFO - joeynmt.training - Example #4
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:23:32,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'ich', 'Ihnen', 'eine', 'R@@', 'i@@', 'hr@@', 'f@@', 'ung', 'zeigen', 'wird,', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 22:23:32,611 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:23:32,611 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:23:32,611 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen ich Ihnen eine Rihrfung zeigen wird, was über die letzten 25 Jahren passiert.
2024-05-23 22:23:55,776 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     2.148263, Batch Acc: 0.415915, Tokens per Sec:     3190, Lr: 0.000300
2024-05-23 22:24:17,673 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     2.139944, Batch Acc: 0.416812, Tokens per Sec:     3292, Lr: 0.000300
2024-05-23 22:24:40,251 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     2.087185, Batch Acc: 0.414513, Tokens per Sec:     3313, Lr: 0.000300
2024-05-23 22:25:02,953 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     2.236913, Batch Acc: 0.415414, Tokens per Sec:     3324, Lr: 0.000300
2024-05-23 22:25:24,892 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     2.278405, Batch Acc: 0.418046, Tokens per Sec:     3253, Lr: 0.000300
2024-05-23 22:25:24,893 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:25:24,893 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:26:43,299 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.18, ppl:   8.88, acc:   0.41, generation: 78.3978[sec], evaluation: 0.0000[sec]
2024-05-23 22:26:43,300 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:26:43,678 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/7000.ckpt
2024-05-23 22:26:43,824 - INFO - joeynmt.training - Example #0
2024-05-23 22:26:43,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:26:43,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:26:43,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Vor', 'kur@@', 'z@@', 'em', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'togra@@', 'f@@', 'iert', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'lo@@', 'sen', 'Größ@@', 'e', 'der', 'lo@@', 'sen', 'Um@@', 'geb@@', 'ung', 'der', 'lo@@', 'sen', 'Um@@', 'geb@@', 'ung', 'der', 'lo@@', 'sen', 'Um@@', 'geb@@', 'ung', 'ist.', '</s>']
2024-05-23 22:26:43,824 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:26:43,824 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:26:43,824 - INFO - joeynmt.training - 	Hypothesis: Vor kurzem Jahr habe ich diese zwei Fotografiert so dass die Arktische Eis für die meisten der letzten drei Millionen Jahre der letzten drei Millionen Jahre die Größe der losen Größe der losen Umgebung der losen Umgebung der losen Umgebung ist.
2024-05-23 22:26:43,824 - INFO - joeynmt.training - Example #1
2024-05-23 22:26:43,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:26:43,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:26:43,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'An@@', 'l@@', 'ungs@@', 'f@@', 'ähig@@', 'keit', 'dieser', 'spezi@@', 'ellen', 'Problem', 'der', 'E@@', 'is@@', 'se', 'der', 'E@@', 'is@@', 'se', 'der', 'E@@', 'is@@', 'se.', '</s>']
2024-05-23 22:26:43,824 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:26:43,824 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:26:43,824 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser Anlungsfähigkeit dieser speziellen Problem der Eisse der Eisse der Eisse.
2024-05-23 22:26:43,824 - INFO - joeynmt.training - Example #2
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einer', 'Sinn@@', ',', 'der', 'Sch@@', 'en@@', 'ze', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einer Sinn, der Schenze des globalen Klimasystem.
2024-05-23 22:26:43,825 - INFO - joeynmt.training - Example #3
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'an@@', 'ges@@', 'etzt', 'und', 'die', 'Ver@@', 'tr@@', 'agen', 'in', 'S@@', 'omm@@', 'er', '</s>']
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Hypothesis: Es expanangesetzt und die Vertragen in Sommer
2024-05-23 22:26:43,825 - INFO - joeynmt.training - Example #4
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:26:43,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'ich', 'Ihnen', 'eine', 'R@@', 'i@@', 'i@@', 'd', 'schn@@', 'ell@@', '-@@', 'W@@', 'ör@@', 'ter', 'der', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:26:43,825 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen ich Ihnen eine Riid schnell-Wörter der letzten 25 Jahren passiert.
2024-05-23 22:27:05,603 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.970606, Batch Acc: 0.419036, Tokens per Sec:     3321, Lr: 0.000300
2024-05-23 22:27:27,407 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     2.126988, Batch Acc: 0.416225, Tokens per Sec:     3449, Lr: 0.000300
2024-05-23 22:27:49,433 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.996985, Batch Acc: 0.418883, Tokens per Sec:     3366, Lr: 0.000300
2024-05-23 22:28:11,800 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     2.146726, Batch Acc: 0.421518, Tokens per Sec:     3286, Lr: 0.000300
2024-05-23 22:28:33,961 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     2.087749, Batch Acc: 0.421218, Tokens per Sec:     3277, Lr: 0.000300
2024-05-23 22:28:33,962 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:28:33,962 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:30:01,750 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.16, ppl:   8.66, acc:   0.41, generation: 87.7803[sec], evaluation: 0.0000[sec]
2024-05-23 22:30:01,750 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:30:02,072 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/7500.ckpt
2024-05-23 22:30:02,234 - INFO - joeynmt.training - Example #0
2024-05-23 22:30:02,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:30:02,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:30:02,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'ä@@', 'te', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'die', 'Ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Prozent', 'der', 'Größ@@', 'e', 'der', 'Lo@@', 'gi@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 22:30:02,234 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:30:02,234 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:30:02,234 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Diäte gezeigt habe, dass die Arktische Kap, die die Arktische Kap, die für die meisten der letzten drei Millionen Jahre lang die Größe der Lower 48 Prozent der Größe der Logik von 40 Prozent.
2024-05-23 22:30:02,234 - INFO - joeynmt.training - Example #1
2024-05-23 22:30:02,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:30:02,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:30:02,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'Ver@@', 'stän@@', 'dn@@', 'is', 'dieses', 'Problem', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'se', 'des', 'E@@', 'is@@', 'se', 'des', 'E@@', 'is@@', 'is@@', 'mus@@', '.', '</s>']
2024-05-23 22:30:02,234 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:30:02,234 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:30:02,234 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses Verständnis dieses Problem dieses Problem ist, weil es nicht das Dicksal des Eisse des Eisse des Eisismus.
2024-05-23 22:30:02,234 - INFO - joeynmt.training - Example #2
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'ä@@', 'mm@@', 'er,', 'das', 'ist', 'ein', 'Sinn@@', ',', 'das', 'G@@', 'eist@@', 'es', 'Klima@@', 'system.', '</s>']
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiskämmer, das ist ein Sinn, das Geistes Klimasystem.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - Example #3
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'an@@', 'ges@@', 'tal@@', 'ten', 'und', 'Kon@@', 'tra@@', 'hl@@', 'en', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'hl@@', 'en.', '</s>']
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Hypothesis: Es expanangestalten und Kontrahlen in Sommer und Kontrag in Sommer und Kontrag in den Sommer und Kontrahlen.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - Example #4
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:30:02,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'werde', 'ich', 'Ihnen', 'ein', 'R@@', 'is@@', 'ik@@', 'o', 'ein', 'Re@@', 'gen@@', 'gen@@', 'utz@@', 't', 'ist,', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:30:02,235 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen werde ich Ihnen ein Risiko ein Regengenutzt ist, was über die letzten 25 Jahren passiert.
2024-05-23 22:30:23,973 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     2.319485, Batch Acc: 0.427471, Tokens per Sec:     3243, Lr: 0.000300
2024-05-23 22:30:46,386 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     2.294218, Batch Acc: 0.420928, Tokens per Sec:     3212, Lr: 0.000300
2024-05-23 22:31:09,032 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     2.176159, Batch Acc: 0.419746, Tokens per Sec:     3240, Lr: 0.000300
2024-05-23 22:31:31,834 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     2.053938, Batch Acc: 0.428956, Tokens per Sec:     3258, Lr: 0.000300
2024-05-23 22:31:54,220 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     2.247576, Batch Acc: 0.424703, Tokens per Sec:     3255, Lr: 0.000300
2024-05-23 22:31:54,221 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:31:54,221 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:33:23,984 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.14, ppl:   8.53, acc:   0.42, generation: 89.7557[sec], evaluation: 0.0000[sec]
2024-05-23 22:33:23,986 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:33:24,357 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/8000.ckpt
2024-05-23 22:33:24,497 - INFO - joeynmt.training - Example #0
2024-05-23 22:33:24,497 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:33:24,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:33:24,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'to@@', 's', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'is@@', 'is@@', 'se', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'die', 'meisten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'aten', 'wurde', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Lo@@', 'k@@', 'top@@', 'f', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 22:33:24,497 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:33:24,497 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:33:24,497 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Fotos so dass die Arktische Eisisisse für die meisten drei Millionen Jahren die meisten drei Millionen Jahren die Größe der Lower 48 Staaten wurde die Größe der Größe der Loktopf von 40 Prozent.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - Example #1
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'rich@@', 'ten', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'elle', 'Problem', 'ist,', 'weil', 'es', 'die', 'D@@', 'ick@@', 'ung', 'des', 'E@@', 'is@@', 'se', 'des', 'E@@', 'is@@', 'mus', 'der', 'E@@', 'is@@', 'mus', 'der', 'E@@', 'is@@', 'se', 'des', 'E@@', 'is@@', 'se', 'des', 'E@@', 'ing@@', 'es.', '</s>']
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Hypothesis: Aber das unterrichten die ernsthaft dieses spezielle Problem ist, weil es die Dickung des Eisse des Eismus der Eismus der Eisse des Eisse des Einges.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - Example #2
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'z@@', 't@@', 't@@', 'ische', 'E@@', 'is@@', 'ik@@', ',', 'der', 'eine', 'Sinn@@', 'e,', 'die', 'Her@@', 'z', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Hypothesis: Die Arzttische Eisik, der eine Sinne, die Herz des globalen Klimasystem.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - Example #3
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:33:24,498 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'tion', 'in', 'S@@', 'omm@@', 'er', 'zu', 'er@@', 'en.', '</s>']
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:33:24,498 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:33:24,499 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Kontraktion in Sommer zu eren.
2024-05-23 22:33:24,499 - INFO - joeynmt.training - Example #4
2024-05-23 22:33:24,499 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:33:24,499 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:33:24,499 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'to@@', 's', 'zeigen', 'wird', 'ein', 'R@@', 'i@@', 'i@@', 'um', 'zu', 'zeigen,', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'ah', 'über', 'die', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 22:33:24,499 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:33:24,499 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:33:24,499 - INFO - joeynmt.training - 	Hypothesis: Die nächste Fotos zeigen wird ein Riium zu zeigen, was über die letzten 25 Jahren geschah über die letzten 25 Jahren passiert.
2024-05-23 22:33:47,322 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     2.149065, Batch Acc: 0.427013, Tokens per Sec:     3098, Lr: 0.000300
2024-05-23 22:34:09,537 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     2.132825, Batch Acc: 0.422626, Tokens per Sec:     3329, Lr: 0.000300
2024-05-23 22:34:31,781 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.949910, Batch Acc: 0.428690, Tokens per Sec:     3307, Lr: 0.000300
2024-05-23 22:34:53,545 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     2.102984, Batch Acc: 0.426993, Tokens per Sec:     3389, Lr: 0.000300
2024-05-23 22:35:15,558 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     2.054967, Batch Acc: 0.427454, Tokens per Sec:     3368, Lr: 0.000300
2024-05-23 22:35:15,559 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:35:15,560 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:36:33,952 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.11, ppl:   8.28, acc:   0.42, generation: 78.3849[sec], evaluation: 0.0000[sec]
2024-05-23 22:36:33,952 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:36:34,245 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/8500.ckpt
2024-05-23 22:36:34,376 - INFO - joeynmt.training - Example #0
2024-05-23 22:36:34,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:36:34,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:36:34,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'ä@@', 'me', 'ge@@', 'zeigt', 'habe,', 'die', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'ik@@', ',', 'der', 'für', 'die', 'meisten', 'von', 'den', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Z@@', 'w@@', 'ün@@', 'ge', 'der', 'Größ@@', 'e', 'der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ik@@', '.', '</s>']
2024-05-23 22:36:34,376 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:36:34,376 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:36:34,376 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Diäme gezeigt habe, die die arktischen Eisik, der für die meisten von den meisten drei Millionen Jahre lang der größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der Größe der Zwünge der Größe der Arktische Eisik.
2024-05-23 22:36:34,376 - INFO - joeynmt.training - Example #1
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'stän@@', 'dn@@', 'is', 'dieses', 'D@@', 'enk@@', 'weise', 'dieses', 'spezi@@', 'elle', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'se', 'der', 'E@@', 'is@@', 'e.', '</s>']
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Hypothesis: Aber das Verständnis dieses Denkweise dieses spezielle Problem ist, weil es nicht die Dicksal der Eisse der Eise.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - Example #2
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'c@@', 'et@@', 'ische', 'E@@', 'is@@', 'c@@', 'a@@', 'p', 'ist', 'in', 'einer', 'Sinn@@', 'e,', 'die', 'Herz@@', '-@@', 'Klima@@', 'wandel@@', '.', '</s>']
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Hypothesis: Die Arcetische Eiscap ist in einer Sinne, die Herz-Klimawandel.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - Example #3
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'eine', 'Er@@', 'kenn@@', 'tn@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'tion', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'tion', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'f@@', 'lik@@', 't', 'in', 'W@@', 'ill@@', 'en.', '</s>']
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - 	Hypothesis: Es gibt eine Erkenntner und Kontrakten in Sommer und Kontraktion in der Sommer und Kontrakten in der Sommer und Kontraktion in der Sommer und Kontraflikt in Willen.
2024-05-23 22:36:34,377 - INFO - joeynmt.training - Example #4
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:36:34,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Di@@', 'en@@', 'ste', 'zeigen', 'ich', 'Ihnen', 'eine', 'R@@', 'i@@', 'um', 'schn@@', 'elle', 'schn@@', 'ell@@', '-@@', 'St@@', 'elle', 'des', 'letzten', '25', 'Jahr@@', 'en.', '</s>']
2024-05-23 22:36:34,378 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:36:34,378 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:36:34,378 - INFO - joeynmt.training - 	Hypothesis: Der nächste Dienste zeigen ich Ihnen eine Rium schnelle schnell-Stelle des letzten 25 Jahren.
2024-05-23 22:36:56,717 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     2.258930, Batch Acc: 0.432272, Tokens per Sec:     3239, Lr: 0.000300
2024-05-23 22:37:18,880 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     2.000003, Batch Acc: 0.434825, Tokens per Sec:     3298, Lr: 0.000300
2024-05-23 22:37:40,899 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     2.301708, Batch Acc: 0.430702, Tokens per Sec:     3381, Lr: 0.000300
2024-05-23 22:38:03,355 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     2.035001, Batch Acc: 0.433044, Tokens per Sec:     3255, Lr: 0.000300
2024-05-23 22:38:25,963 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     2.111260, Batch Acc: 0.434758, Tokens per Sec:     3331, Lr: 0.000300
2024-05-23 22:38:25,964 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:38:25,964 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:39:52,677 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.10, ppl:   8.13, acc:   0.43, generation: 86.7057[sec], evaluation: 0.0000[sec]
2024-05-23 22:39:52,678 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:39:52,991 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/9000.ckpt
2024-05-23 22:39:53,140 - INFO - joeynmt.training - Example #0
2024-05-23 22:39:53,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:39:53,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:39:53,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'to', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'Jahre', 'lang', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'Jahre', 'die', 'größ@@', 'ten', '4@@', '8', 'Sta@@', 'aten', 'wurde', 'die', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Prozent', 'ist.', '</s>']
2024-05-23 22:39:53,140 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:39:53,140 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:39:53,140 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Foto gezeigt habe, dass die Arktische Eis der letzten drei Millionen Jahre Jahre lang die meisten der letzten drei Millionen Jahre Jahre die größten 48 Staaten wurde die Größe der Lower 48 Prozent ist.
2024-05-23 22:39:53,140 - INFO - joeynmt.training - Example #1
2024-05-23 22:39:53,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:39:53,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:39:53,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'bestimm@@', 'te', 'Probleme', 'dieser', 'spezi@@', 'elle', 'Problem', 'dieser', 'spezi@@', 'elle', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'ung', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser bestimmte Probleme dieser spezielle Problem dieser spezielle Problem ist, weil es nicht die Dickung des Eiss.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - Example #2
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'glob@@', 'alen', 'Klima@@', 'system', '</s>']
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einem Sinne des globalen Klimawandel des globalen Klimasystem des globalen Klimasystem
2024-05-23 22:39:53,141 - INFO - joeynmt.training - Example #3
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'F@@', 'en@@', 'ster', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'K@@', 'er@@', '-@@', '</s>']
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Fenster und Kontrakten in Sommer-Sommer-Ker-
2024-05-23 22:39:53,141 - INFO - joeynmt.training - Example #4
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:39:53,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'Ihnen', 'zeigen', 'werde', 'ich', 'Ihnen', 'ein', 'R@@', 'i@@', 'd', 'schn@@', 'ell@@', '-@@', 'St@@', 'elle', 'von', 'dem,', 'was', 'passiert', 'ist.', '</s>']
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:39:53,141 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie Ihnen zeigen werde ich Ihnen ein Rid schnell-Stelle von dem, was passiert ist.
2024-05-23 22:40:15,985 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     2.080781, Batch Acc: 0.430982, Tokens per Sec:     3152, Lr: 0.000300
2024-05-23 22:40:22,517 - INFO - joeynmt.training - Epoch   3: total training loss 8295.86
2024-05-23 22:40:22,517 - INFO - joeynmt.training - EPOCH 4
2024-05-23 22:40:38,332 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     2.053489, Batch Acc: 0.456037, Tokens per Sec:     3241, Lr: 0.000300
2024-05-23 22:41:00,617 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     1.843380, Batch Acc: 0.458997, Tokens per Sec:     3326, Lr: 0.000300
2024-05-23 22:41:22,825 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.929669, Batch Acc: 0.455142, Tokens per Sec:     3283, Lr: 0.000300
2024-05-23 22:41:45,107 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.776413, Batch Acc: 0.452869, Tokens per Sec:     3319, Lr: 0.000300
2024-05-23 22:41:45,108 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:41:45,108 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:43:02,315 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.09, ppl:   8.08, acc:   0.43, generation: 77.1982[sec], evaluation: 0.0000[sec]
2024-05-23 22:43:02,317 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:43:02,607 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/9500.ckpt
2024-05-23 22:43:02,780 - INFO - joeynmt.training - Example #0
2024-05-23 22:43:02,780 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:43:02,780 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:43:02,780 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'ä@@', 'te', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'größ@@', 'ten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'lo@@', 'wer', '4@@', '8', 'St@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 22:43:02,780 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:43:02,780 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Diäte so dass die Arktische Eis für die meisten der letzten drei Millionen Jahre der letzten drei Millionen Jahre die größten drei Millionen Jahre die Größe der lower 48 Stunk von 40 Prozent.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - Example #1
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'bestimm@@', 'te', 'Problem', 'dieser', 'spezi@@', 'elle', 'Problem@@', 'e,', 'denn', 'es', 'zeigt', 'nicht', 'die', 'D@@', 'ick@@', 'ung', 'des', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'is', 'ist.', '</s>']
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser bestimmte Problem dieser spezielle Probleme, denn es zeigt nicht die Dickung des Eis des Eis des Eis ist.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - Example #2
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'c@@', 't@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Hypothesis: Die Arctische Eis ist in einem Sinn des globalen Klimasystem.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - Example #3
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'de', 'W@@', 'en@@', 'der@@', 'sp@@', 'ann@@', 't', 'in', 'S@@', 'omm@@', 'er', 'und', 'die', 'Ver@@', 'kehr@@', 's@@', 'ä@@', 'tig@@', 'ung', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'die', 'Ver@@', 'kehr@@', 's@@', 'ver@@', 'fol@@', 'gen.', '</s>']
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - 	Hypothesis: Es expande Wenderspannt in Sommer und die Verkehrsätigung in der Sommer und die Verkehrsverfolgen.
2024-05-23 22:43:02,781 - INFO - joeynmt.training - Example #4
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:43:02,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:43:02,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', 'Di@@', 'e,', 'die', 'ich', 'Ihnen', 'zeigen', 'werden', 'eine', 'Re@@', 'gen@@', 'wal@@', 'd', 'schn@@', 'eller', 'in', 'der', 'letzten', '25', 'Jahre', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-23 22:43:02,782 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:43:02,782 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:43:02,782 - INFO - joeynmt.training - 	Hypothesis: Die nächste DiDie, die ich Ihnen zeigen werden eine Regenwald schneller in der letzten 25 Jahre geschehen ist.
2024-05-23 22:43:26,161 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     2.032952, Batch Acc: 0.455603, Tokens per Sec:     3155, Lr: 0.000300
2024-05-23 22:43:48,628 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.988285, Batch Acc: 0.454611, Tokens per Sec:     3328, Lr: 0.000300
2024-05-23 22:44:10,956 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.913568, Batch Acc: 0.452254, Tokens per Sec:     3260, Lr: 0.000300
2024-05-23 22:44:33,656 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     2.025616, Batch Acc: 0.459352, Tokens per Sec:     3270, Lr: 0.000300
2024-05-23 22:44:55,721 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     1.979770, Batch Acc: 0.454984, Tokens per Sec:     3448, Lr: 0.000300
2024-05-23 22:44:55,722 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:44:55,722 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:46:21,600 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.07, ppl:   7.92, acc:   0.43, generation: 85.8701[sec], evaluation: 0.0000[sec]
2024-05-23 22:46:21,602 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:46:21,964 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/10000.ckpt
2024-05-23 22:46:22,122 - INFO - joeynmt.training - Example #0
2024-05-23 22:46:22,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:46:22,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:46:22,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'di@@', 'en@@', 'er', 'sind', 'die', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Prozent', 'ver@@', 'st@@', 'är@@', 'ker', 'von', '40', 'Prozent', 'ver@@', 'gleich@@', 't.', '</s>']
2024-05-23 22:46:22,122 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:46:22,122 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:46:22,122 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die Arktische Eis für die meisten drei Millionen Jahre der letzten drei Millionen Jahren die meisten drei Millionen Jahre der Lower 48 Stadiener sind die Größe der Lower 48 Prozent verstärker von 40 Prozent vergleicht.
2024-05-23 22:46:22,122 - INFO - joeynmt.training - Example #1
2024-05-23 22:46:22,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:46:22,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:46:22,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'Unter@@', 'such@@', 'ung', 'des', 'E@@', 'is@@', 'se', 'dieses', 'Problem', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'is', 'des', 'E@@', 'is', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'zu', 'zeigen.', '</s>']
2024-05-23 22:46:22,122 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser Untersuchung des Eisse dieses Problem der Eis des Eis des Eis des Eis der Eis des Eis zu zeigen.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - Example #2
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'c@@', 'a@@', 'p', 'ist', 'in', 'einem', 'Sinn@@', ',', 'der', 'B@@', 'en@@', 'utz@@', 'ung', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiscap ist in einem Sinn, der Benutzung des globalen Klimasystem.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - Example #3
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'tra@@', 'f', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'f@@', 'ern', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'f@@', 'ern', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'f@@', 'ern', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'f@@', 'ern', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'f@@', 'ern', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'f@@', 'ern', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'g', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'f@@', 'ern', 'in']
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich in Winter und Vertraf in Sommer und Vertrafern in der Sommer und Vertrafern in der Sommer und Vertrafern in der Sommer und Vertrafern in der Sommer und Vertrafern in der Sommer und Vertrafern in der Sommer und Vertrag in der Sommer und Vertrafern in
2024-05-23 22:46:22,123 - INFO - joeynmt.training - Example #4
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:46:22,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'dass', 'Sie', 'ein', 'Re@@', 'gen@@', 'wal@@', 'd', 'wird', 'wird', 'in', 'der', 'letzten', '25', 'Jahr@@', 'en.', '</s>']
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:46:22,123 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen ich Ihnen zeigen werde, dass Sie ein Regenwald wird wird in der letzten 25 Jahren.
2024-05-23 22:46:45,265 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     1.959673, Batch Acc: 0.455753, Tokens per Sec:     3150, Lr: 0.000300
2024-05-23 22:47:07,137 - INFO - joeynmt.training - Epoch   4, Step:    12700, Batch Loss:     1.927724, Batch Acc: 0.453563, Tokens per Sec:     3332, Lr: 0.000300
2024-05-23 22:47:29,456 - INFO - joeynmt.training - Epoch   4, Step:    12800, Batch Loss:     1.803489, Batch Acc: 0.455051, Tokens per Sec:     3231, Lr: 0.000300
2024-05-23 22:47:51,620 - INFO - joeynmt.training - Epoch   4, Step:    12900, Batch Loss:     2.046336, Batch Acc: 0.456736, Tokens per Sec:     3259, Lr: 0.000300
2024-05-23 22:48:13,212 - INFO - joeynmt.training - Epoch   4, Step:    13000, Batch Loss:     2.132171, Batch Acc: 0.449218, Tokens per Sec:     3390, Lr: 0.000300
2024-05-23 22:48:13,213 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:48:13,213 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:49:30,834 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.06, ppl:   7.82, acc:   0.44, generation: 77.6086[sec], evaluation: 0.0000[sec]
2024-05-23 22:49:30,834 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:49:31,260 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/10500.ckpt
2024-05-23 22:49:31,446 - INFO - joeynmt.training - Example #0
2024-05-23 22:49:31,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:49:31,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:49:31,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'ä@@', 't', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'isch@@', 't', 'E@@', 'is', 'in', 'den', 'letzten', 'drei', 'Millionen', 'Jahre', 'Jahre', 'lang', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Stat@@', 'en', 'der', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Prozent', 'sind.', '</s>']
2024-05-23 22:49:31,446 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:49:31,446 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:49:31,446 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Diät so dass die Arktischt Eis in den letzten drei Millionen Jahre Jahre lang die meisten der letzten drei Millionen Jahre lang die Größe der Lower 48 Staten der Größe der Lower 48 Prozent sind.
2024-05-23 22:49:31,446 - INFO - joeynmt.training - Example #1
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'Bild', 'der', 'ern@@', 'sten', 'Problem@@', 'e,', 'weil', 'es', 'die', 'D@@', 'ick@@', 'ung', 'der', 'E@@', 'is@@', 'se', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses Bild der ernsten Probleme, weil es die Dickung der Eisse des Eiss.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - Example #2
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', 'e,', 'der', 'Sch@@', 'n@@', 'it@@', 't', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einem Sinne, der Schnitt des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem
2024-05-23 22:49:31,447 - INFO - joeynmt.training - Example #3
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'den', 'S@@', 'omm@@', 'er', 'zu', 'be@@', 'kommen.', '</s>']
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer und Kontrakte in den Sommer zu bekommen.
2024-05-23 22:49:31,447 - INFO - joeynmt.training - Example #4
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:49:31,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', 'a@@', 'i@@', 'de', 'zeigt', 'Ihnen', 'ein', 'R@@', 'i@@', 'den', 'schn@@', 'ell@@', '-@@', 'schn@@', 'ell@@', '-@@', 'schn@@', 'ell@@', '-@@', 'St@@', 're@@', 'm@@', 'ung', 'gesch@@', 'ehen', 'wird.', '</s>']
2024-05-23 22:49:31,448 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:49:31,448 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:49:31,448 - INFO - joeynmt.training - 	Hypothesis: Die nächste Diaide zeigt Ihnen ein Riden schnell-schnell-schnell-Stremung geschehen wird.
2024-05-23 22:49:54,721 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     1.982600, Batch Acc: 0.458870, Tokens per Sec:     3123, Lr: 0.000300
2024-05-23 22:50:17,818 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     1.998262, Batch Acc: 0.455966, Tokens per Sec:     3109, Lr: 0.000300
2024-05-23 22:50:39,578 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.990359, Batch Acc: 0.452964, Tokens per Sec:     3471, Lr: 0.000300
2024-05-23 22:51:02,509 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.995108, Batch Acc: 0.456252, Tokens per Sec:     3312, Lr: 0.000300
2024-05-23 22:51:24,937 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.756078, Batch Acc: 0.457883, Tokens per Sec:     3224, Lr: 0.000300
2024-05-23 22:51:24,938 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:51:24,938 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:52:42,575 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.05, ppl:   7.75, acc:   0.44, generation: 77.6293[sec], evaluation: 0.0000[sec]
2024-05-23 22:52:42,577 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:52:42,920 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/11000.ckpt
2024-05-23 22:52:43,078 - INFO - joeynmt.training - Example #0
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'ä@@', 'te', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'E@@', 'is', 'E@@', 'is', 'is', 'is', 'in', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'größ@@', 'ten', 'drei', 'Millionen', 'Jahre', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'ats@@', 'k@@', 'und@@', '-@@', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', 'der', 'lo@@', 'k@@', 'alen', '4@@', '8', 'Prozent', 'sind.', '</s>']
2024-05-23 22:52:43,078 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:52:43,078 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:52:43,078 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Diäte so dass die arktische Eis Eis Eis is is in der letzten drei Millionen Jahre der größten drei Millionen Jahre der Lower 48 Staatskund-Staaten, die Größe der lokalen 48 Prozent sind.
2024-05-23 22:52:43,078 - INFO - joeynmt.training - Example #1
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'bestimm@@', 'te', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'den', 'D@@', 'rei@@', 'z@@', 'ig@@', 'keit,', 'die', 'D@@', 'ick@@', 's@@', 'al', 'der', 'E@@', 'is@@', 'b@@', 'ah@@', 'n@@', 'b@@', 'ah@@', 'n', 'der', 'E@@', 'is@@', 'b@@', 'ah@@', 'n@@', 'b@@', 't.', '</s>']
2024-05-23 22:52:43,078 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:52:43,078 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:52:43,078 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser bestimmte Probleme, weil es nicht den Dreizigkeit, die Dicksal der Eisbahnbahn der Eisbahnbt.
2024-05-23 22:52:43,078 - INFO - joeynmt.training - Example #2
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:52:43,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'c@@', 'et@@', 'ische', 'E@@', 'is@@', 'z@@', 'ü@@', 'g@@', 'keit', 'ist', 'in', 'einem', 'Sinn@@', ',', 'der', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Hypothesis: Die Arcetische Eiszügkeit ist in einem Sinn, der globalen Klimasystem.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - Example #3
2024-05-23 22:52:43,079 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:52:43,079 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:52:43,079 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakten in Sommer.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - Example #4
2024-05-23 22:52:43,079 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:52:43,079 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:52:43,079 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'werde', 'ich', 'Ihnen', 'ein', 'ra@@', 'pi@@', 'd', 'schn@@', 'ell@@', '-@@', 'St@@', 'elle', 'zu', 'sein,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:52:43,079 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen werde ich Ihnen ein rapid schnell-Stelle zu sein, was in den letzten 25 Jahren passiert ist.
2024-05-23 22:53:05,747 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.940641, Batch Acc: 0.451774, Tokens per Sec:     3143, Lr: 0.000300
2024-05-23 22:53:27,365 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.980713, Batch Acc: 0.452571, Tokens per Sec:     3382, Lr: 0.000300
2024-05-23 22:53:49,950 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     2.032146, Batch Acc: 0.453205, Tokens per Sec:     3237, Lr: 0.000300
2024-05-23 22:54:12,302 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     2.134532, Batch Acc: 0.451240, Tokens per Sec:     3282, Lr: 0.000300
2024-05-23 22:54:34,798 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     2.115985, Batch Acc: 0.456717, Tokens per Sec:     3217, Lr: 0.000300
2024-05-23 22:54:34,799 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:54:34,799 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:55:56,900 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.03, ppl:   7.59, acc:   0.44, generation: 82.0926[sec], evaluation: 0.0000[sec]
2024-05-23 22:55:56,901 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:55:57,212 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/11500.ckpt
2024-05-23 22:55:57,357 - INFO - joeynmt.training - Example #0
2024-05-23 22:55:57,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:55:57,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:55:57,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'so', 'dass', 'die', 'Ar@@', 'kt@@', 'isch@@', 'e,', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Stat@@', 'en', 'der', 'Größ@@', 'e', 'der', 'lo@@', 'k@@', 'ale', 'Sta@@', 'aten', 'von', '40', '%@@', '.', '</s>']
2024-05-23 22:55:57,357 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:55:57,357 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:55:57,357 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich diese zwei Folie so dass die Arktische, die arktische Eis für die meisten letzten drei Millionen Jahre lang die Größe der Größe der Größe der Lower 48 Staten der Größe der lokale Staaten von 40 %.
2024-05-23 22:55:57,357 - INFO - joeynmt.training - Example #1
2024-05-23 22:55:57,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:55:57,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:55:57,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des Eiss des Eiss des Eiss des Eiss des Eiss.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - Example #2
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'im', 'Sinn@@', ',', 'das', 'ist', 'ein', 'Sinn@@', ',', 'das', 'Ver@@', 'k@@', 'ehr', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist im Sinn, das ist ein Sinn, das Verkehr des globalen Klimasystem.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - Example #3
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 'ung', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'k@@', 'ehr', 'zu', 'ver@@', 'fol@@', 'gen.', '</s>']
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verfolgung in Sommer und Verkehr zu verfolgen.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - Example #4
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:55:57,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'zeigen', 'werden,', 'dass', 'Sie', 'eine', 'R@@', 'ah@@', 'men', 'des', 'letzten', '25', 'Jahre', 'passier@@', 'te.', '</s>']
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:55:57,358 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen zeigen werden, dass Sie eine Rahmen des letzten 25 Jahre passierte.
2024-05-23 22:56:19,177 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.898135, Batch Acc: 0.457424, Tokens per Sec:     3251, Lr: 0.000300
2024-05-23 22:56:40,550 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.951389, Batch Acc: 0.457008, Tokens per Sec:     3458, Lr: 0.000300
2024-05-23 22:57:01,766 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     2.005986, Batch Acc: 0.458672, Tokens per Sec:     3523, Lr: 0.000300
2024-05-23 22:57:23,390 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.856597, Batch Acc: 0.459182, Tokens per Sec:     3331, Lr: 0.000300
2024-05-23 22:57:44,671 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.883930, Batch Acc: 0.457121, Tokens per Sec:     3568, Lr: 0.000300
2024-05-23 22:57:44,672 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 22:57:44,672 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 22:59:06,166 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.49, acc:   0.45, generation: 81.4869[sec], evaluation: 0.0000[sec]
2024-05-23 22:59:06,168 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 22:59:06,507 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/12000.ckpt
2024-05-23 22:59:06,645 - INFO - joeynmt.training - Example #0
2024-05-23 22:59:06,645 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 22:59:06,645 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 22:59:06,645 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'de', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'der', 'Ar@@', 'kt@@', 'is', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'aten', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 22:59:06,645 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 22:59:06,645 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folide gezeigt habe, dass die Arktis der Arktis für die meisten der letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe der Lower 48 Staaten von 40 Prozent.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - Example #1
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'stän@@', 'dn@@', 'is', 'dieses', 'Bild', 'des', 'Jahr@@', 'es', 'Problem', 'dieser', 'spezi@@', 'ellen', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ru@@', 'ck@@', 'er@@', 'ei', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is@@', 'ierung', 'des', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is']
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Hypothesis: Aber das Verständnis dieses Bild des Jahres Problem dieser speziellen Problem ist, weil es nicht die Druckerei der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis des Eis der Eis der Eis des Eis der Eis der Eis der Eisierung des Eis der Eis der Eis der Eis der Eis
2024-05-23 22:59:06,646 - INFO - joeynmt.training - Example #2
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', 'e,', 'das', 'Sch@@', 'ön@@', 'e', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eis ist in einem Sinne, das Schöne des globalen Klimasystem des globalen Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem
2024-05-23 22:59:06,646 - INFO - joeynmt.training - Example #3
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 22:59:06,646 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'die', 'W@@', 'en@@', 'des', 'und', 'Kon@@', 'tra@@', 'k@@', 'ti@@', 's', 'in', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', '</s>']
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 22:59:06,646 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich die Wendes und Kontraktis in Sommer Sommer
2024-05-23 22:59:06,646 - INFO - joeynmt.training - Example #4
2024-05-23 22:59:06,647 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 22:59:06,647 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 22:59:06,647 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', 'en@@', 'ste', 'zeigen,', 'dass', 'Sie', 'ein', 'R@@', 'i@@', 'elle', 'zeigen,', 'was', 'passiert', 'passiert', 'ist.', '</s>']
2024-05-23 22:59:06,647 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 22:59:06,647 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 22:59:06,647 - INFO - joeynmt.training - 	Hypothesis: Die nächste Dienste zeigen, dass Sie ein Rielle zeigen, was passiert passiert ist.
2024-05-23 22:59:29,374 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.983228, Batch Acc: 0.455074, Tokens per Sec:     3141, Lr: 0.000300
2024-05-23 22:59:51,510 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.845044, Batch Acc: 0.462297, Tokens per Sec:     3385, Lr: 0.000300
2024-05-23 23:00:14,088 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     2.003052, Batch Acc: 0.459627, Tokens per Sec:     3256, Lr: 0.000300
2024-05-23 23:00:35,822 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     2.076786, Batch Acc: 0.462682, Tokens per Sec:     3433, Lr: 0.000300
2024-05-23 23:00:57,354 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.900355, Batch Acc: 0.465389, Tokens per Sec:     3436, Lr: 0.000300
2024-05-23 23:00:57,355 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:00:57,355 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:02:10,660 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.43, acc:   0.45, generation: 73.2970[sec], evaluation: 0.0000[sec]
2024-05-23 23:02:10,661 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:02:10,969 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/12500.ckpt
2024-05-23 23:02:11,175 - INFO - joeynmt.training - Example #0
2024-05-23 23:02:11,175 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:02:11,175 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:02:11,175 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 'te', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'zu', 'dem', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'lo@@', 'wer', '4@@', '8', 'Sta@@', 'aten', 'ver@@', 'di@@', 'en@@', 'en.', '</s>']
2024-05-23 23:02:11,175 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigte so dass die arktische Eis zu dem die meisten der letzten drei Millionen Jahre lang die Größe der Größe der Größe der lower 48 Staaten verdienen.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - Example #1
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'bestimm@@', 'te', 'Problem', 'dieses', 'bestimm@@', 'te', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'weise', 'der', 'I@@', 'I@@', 'r@@', 'gend@@', 'liche', 'ist.', '</s>']
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses bestimmte Problem dieses bestimmte Problem, weil es nicht die Dicksweise der IIrgendliche ist.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - Example #2
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eis ist in einem Sinn des globalen Klimasystem.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - Example #3
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:02:11,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'der', 'W@@', 'en@@', 'dep@@', 'unk@@', 't,', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er', '</s>']
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:02:11,176 - INFO - joeynmt.training - 	Hypothesis: Es gibt in der Wendepunkt, in der Sommer und Kontrakte in der Sommer
2024-05-23 23:02:11,176 - INFO - joeynmt.training - Example #4
2024-05-23 23:02:11,177 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:02:11,177 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:02:11,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'dass', 'Ihnen', 'eine', 'Re@@', 'pi@@', 'd', 'schn@@', 'elle', 'Zeit', 'ist.', '</s>']
2024-05-23 23:02:11,177 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:02:11,177 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:02:11,177 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen zeigen werde, dass Ihnen eine Repid schnelle Zeit ist.
2024-05-23 23:02:32,925 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.818911, Batch Acc: 0.461739, Tokens per Sec:     3305, Lr: 0.000300
2024-05-23 23:02:54,447 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.941500, Batch Acc: 0.460102, Tokens per Sec:     3305, Lr: 0.000300
2024-05-23 23:03:16,232 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.878218, Batch Acc: 0.460811, Tokens per Sec:     3421, Lr: 0.000300
2024-05-23 23:03:38,368 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.949114, Batch Acc: 0.462183, Tokens per Sec:     3389, Lr: 0.000300
2024-05-23 23:03:59,876 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.876280, Batch Acc: 0.458400, Tokens per Sec:     3411, Lr: 0.000300
2024-05-23 23:03:59,877 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:03:59,877 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:05:21,519 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.99, ppl:   7.29, acc:   0.45, generation: 81.6348[sec], evaluation: 0.0000[sec]
2024-05-23 23:05:21,521 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:05:21,880 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/13000.ckpt
2024-05-23 23:05:22,003 - INFO - joeynmt.training - Example #0
2024-05-23 23:05:22,003 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:05:22,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:05:22,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ver@@', 'letz@@', 'tes', 'Jahr', 'ge@@', 'zeigt', 'hat,', 'die', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'L@@', 'lo@@', 'k@@', 'al@@', 'er', '4@@', '8', 'Sta@@', 'aten', 'ge@@', 'zeigt', 'hat.', '</s>']
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die arktische Eis verletztes Jahr gezeigt hat, die für die meisten der letzten drei Millionen Jahre der Llokaler 48 Staaten gezeigt hat.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - Example #1
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'spezi@@', 'ellen', 'Problem', 'dieser', 'spezi@@', 'ellen', 'Problem', 'dieser', 'spezi@@', 'ellen', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'mus', 'zeigen', 'kann.', '</s>']
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses speziellen Problem dieser speziellen Problem dieser speziellen Problem ist, weil es nicht die Dicksal des Eismus zeigen kann.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - Example #2
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Das', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'ä@@', 'ft', 'ist,', 'in', 'einem', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - 	Hypothesis: Das Arktische Eisschäft ist, in einem Sinne des globalen Klimasystem.
2024-05-23 23:05:22,004 - INFO - joeynmt.training - Example #3
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:05:22,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'g', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'g', 'in', 'S@@', 'omm@@', 'er', '</s>']
2024-05-23 23:05:22,005 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:05:22,005 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:05:22,005 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich in der Sommer und Vertrag in Sommer und Vertrag in Sommer
2024-05-23 23:05:22,005 - INFO - joeynmt.training - Example #4
2024-05-23 23:05:22,005 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:05:22,005 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:05:22,005 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'ich', 'Ihnen', 'eine', 'Re@@', 'p@@', 'p@@', 'fl@@', 'anz@@', 'en@@', 'ver@@', 'st@@', 'ör@@', 't', 'werden', 'können.', '</s>']
2024-05-23 23:05:22,005 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:05:22,005 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:05:22,005 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen ich Ihnen eine Reppflanzenverstört werden können.
2024-05-23 23:05:24,128 - INFO - joeynmt.training - Epoch   4: total training loss 7668.38
2024-05-23 23:05:24,129 - INFO - joeynmt.training - EPOCH 5
2024-05-23 23:05:43,882 - INFO - joeynmt.training - Epoch   5, Step:    15600, Batch Loss:     2.054376, Batch Acc: 0.487611, Tokens per Sec:     3312, Lr: 0.000300
2024-05-23 23:06:05,289 - INFO - joeynmt.training - Epoch   5, Step:    15700, Batch Loss:     1.851524, Batch Acc: 0.486720, Tokens per Sec:     3439, Lr: 0.000300
2024-05-23 23:06:28,244 - INFO - joeynmt.training - Epoch   5, Step:    15800, Batch Loss:     1.930817, Batch Acc: 0.487998, Tokens per Sec:     3285, Lr: 0.000300
2024-05-23 23:06:50,900 - INFO - joeynmt.training - Epoch   5, Step:    15900, Batch Loss:     1.782501, Batch Acc: 0.489452, Tokens per Sec:     3339, Lr: 0.000300
2024-05-23 23:07:12,529 - INFO - joeynmt.training - Epoch   5, Step:    16000, Batch Loss:     1.713981, Batch Acc: 0.483084, Tokens per Sec:     3346, Lr: 0.000300
2024-05-23 23:07:12,531 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:07:12,531 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:08:26,921 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.99, ppl:   7.28, acc:   0.45, generation: 74.3821[sec], evaluation: 0.0000[sec]
2024-05-23 23:08:26,922 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:08:27,225 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/13500.ckpt
2024-05-23 23:08:27,394 - INFO - joeynmt.training - Example #0
2024-05-23 23:08:27,394 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:08:27,394 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:08:27,394 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'dass', 'die', 'Ar@@', 'kt@@', 'isch', 'E@@', 'is', 'ge@@', 'zeigt', 'hat,', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'ver@@', 'mut@@', 'lich', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:08:27,394 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt dass die Arktisch Eis gezeigt hat, die meisten der letzten drei Millionen Jahre lang die meisten der letzten drei Millionen Jahre vermutlich die Größe 48 Staaten, die von 40 Prozent.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - Example #1
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'ern@@', 's', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'zu', 'zeigen,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des erns ernsthaft dieses speziellen Problems zu zeigen, weil es nicht die Dickheit der Eis des Eiss.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - Example #2
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einem Sinne des globalen Klimasystem des Klimasystem.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - Example #3
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:08:27,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'auch', 'W@@', 'en@@', 'der@@', 'sp@@', 'ur@@', 'en', 'und', 'den', 'den', 'K@@', 'er@@', 'l', 'und', 'den', 'K@@', 'er@@', 'l', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'den', 'Ber@@', 'eich@@', '.', '</s>']
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - 	Hypothesis: Es gibt auch Wenderspuren und den den Kerl und den Kerl in den Sommer und den Bereich.
2024-05-23 23:08:27,395 - INFO - joeynmt.training - Example #4
2024-05-23 23:08:27,396 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:08:27,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:08:27,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'Re@@', 'gen@@', 'wal@@', 'd', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', '-@@', 'Vor@@', 'aus@@', 'ch', 'sein', 'wird.', '</s>']
2024-05-23 23:08:27,396 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:08:27,396 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:08:27,396 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen ein Regenwald schnelles schnell-Vorausch sein wird.
2024-05-23 23:08:50,445 - INFO - joeynmt.training - Epoch   5, Step:    16100, Batch Loss:     2.032769, Batch Acc: 0.479309, Tokens per Sec:     3164, Lr: 0.000300
2024-05-23 23:09:12,283 - INFO - joeynmt.training - Epoch   5, Step:    16200, Batch Loss:     1.794717, Batch Acc: 0.483268, Tokens per Sec:     3293, Lr: 0.000300
2024-05-23 23:09:33,938 - INFO - joeynmt.training - Epoch   5, Step:    16300, Batch Loss:     2.021541, Batch Acc: 0.476942, Tokens per Sec:     3393, Lr: 0.000300
2024-05-23 23:09:55,843 - INFO - joeynmt.training - Epoch   5, Step:    16400, Batch Loss:     1.899104, Batch Acc: 0.479752, Tokens per Sec:     3447, Lr: 0.000300
2024-05-23 23:10:18,850 - INFO - joeynmt.training - Epoch   5, Step:    16500, Batch Loss:     1.903927, Batch Acc: 0.477950, Tokens per Sec:     3251, Lr: 0.000300
2024-05-23 23:10:18,851 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:10:18,851 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:11:49,301 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.21, acc:   0.46, generation: 90.4418[sec], evaluation: 0.0000[sec]
2024-05-23 23:11:49,303 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:11:49,641 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/14000.ckpt
2024-05-23 23:11:49,783 - INFO - joeynmt.training - Example #0
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'ist,', 'die', 'am', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Stat@@', 'en,', 'die', 'Größ@@', 'e', 'der', 'lang@@', 'sam@@', 'er', '4@@', '8', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die arktischen Eis ist, die am meisten der letzten drei Millionen Jahre lang die meisten drei Millionen Jahre lang die Größe der unteren 48 Staten, die Größe der langsamer 48 Prozent.
2024-05-23 23:11:49,784 - INFO - joeynmt.training - Example #1
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'Problem', 'dieser', 'bestimm@@', 'te', 'Problem', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'der', 'D@@', 'ich@@', 'te', 'des', 'E@@', 'is@@', 's', 'zeigt', 'sich', 'nicht', 'der', 'D@@', 'ich@@', 'te', 'zu', 'zeigen.', '</s>']
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses Problem dieser bestimmte Problem dieses Problem ist, weil es nicht der Dichte des Eiss zeigt sich nicht der Dichte zu zeigen.
2024-05-23 23:11:49,784 - INFO - joeynmt.training - Example #2
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:11:49,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:11:49,784 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:11:49,785 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einem Sinn des globalen Klimasystem des Klimasystem des Klimasystem des Klimasystem
2024-05-23 23:11:49,785 - INFO - joeynmt.training - Example #3
2024-05-23 23:11:49,785 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:11:49,785 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:11:49,785 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'W@@', 'en@@', 'der@@', 'sp@@', 'ann@@', 'e', 'und', 'den', 'S@@', 'omm@@', 'er', 'in', 'S@@', 'omm@@', 'er', 'zu', 'ver@@', 'fol@@', 'gen.', '</s>']
2024-05-23 23:11:49,785 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:11:49,785 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:11:49,785 - INFO - joeynmt.training - 	Hypothesis: Es gibt es Wenderspanne und den Sommer in Sommer zu verfolgen.
2024-05-23 23:11:49,785 - INFO - joeynmt.training - Example #4
2024-05-23 23:11:49,785 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:11:49,785 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:11:49,785 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'Re@@', 'pi@@', 'd', 'schn@@', 'eller', 'zu', 'ver@@', 'wandel@@', 'n', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 'te.', '</s>']
2024-05-23 23:11:49,785 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:11:49,785 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:11:49,785 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen ein Repid schneller zu verwandeln von dem, was in den letzten 25 Jahren passierte.
2024-05-23 23:12:12,983 - INFO - joeynmt.training - Epoch   5, Step:    16600, Batch Loss:     1.918474, Batch Acc: 0.478228, Tokens per Sec:     3168, Lr: 0.000300
2024-05-23 23:12:34,535 - INFO - joeynmt.training - Epoch   5, Step:    16700, Batch Loss:     1.762210, Batch Acc: 0.482380, Tokens per Sec:     3373, Lr: 0.000300
2024-05-23 23:12:56,618 - INFO - joeynmt.training - Epoch   5, Step:    16800, Batch Loss:     1.758833, Batch Acc: 0.485471, Tokens per Sec:     3316, Lr: 0.000300
2024-05-23 23:13:18,336 - INFO - joeynmt.training - Epoch   5, Step:    16900, Batch Loss:     1.999477, Batch Acc: 0.478451, Tokens per Sec:     3392, Lr: 0.000300
2024-05-23 23:13:40,580 - INFO - joeynmt.training - Epoch   5, Step:    17000, Batch Loss:     1.842190, Batch Acc: 0.482049, Tokens per Sec:     3304, Lr: 0.000300
2024-05-23 23:13:40,581 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:13:40,581 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:14:50,578 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.20, acc:   0.46, generation: 69.9899[sec], evaluation: 0.0000[sec]
2024-05-23 23:14:50,579 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:14:50,912 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/14500.ckpt
2024-05-23 23:14:51,067 - INFO - joeynmt.training - Example #0
2024-05-23 23:14:51,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:14:51,067 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:14:51,067 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeig@@', 't,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'is', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'such@@', 't', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'such@@', 't', 'hat.', '</s>']
2024-05-23 23:14:51,067 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:14:51,067 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:14:51,067 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigt, dass die arktische Eis is für die meisten letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe der untersucht der Größe der untersucht hat.
2024-05-23 23:14:51,067 - INFO - joeynmt.training - Example #1
2024-05-23 23:14:51,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:14:51,067 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:14:51,067 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'bestimm@@', 'te', 'Problem', 'dieser', 'bestimm@@', 'te', 'Problem', 'dieser', 'bestimm@@', 'te', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'en@@', 'heit', 'der', 'E@@', 'is@@', 'en@@', 'br@@', 'u@@', 'ch', 'ist.', '</s>']
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses bestimmte Problem dieser bestimmte Problem dieser bestimmte Probleme, weil es nicht die Dickenheit der Eisenbruch ist.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - Example #2
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eis ist in einem Sinne des globalen Klimasystem.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - Example #3
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'exp@@', 'an@@', 'an@@', 'an@@', 'zi@@', 'ehen', 'und', 'Kon@@', 'tra@@', 'k@@', 't', 'in', 'der', 'S@@', 'omm@@', 'er', 'er@@', 'h@@', 'eb@@', 'en.', '</s>']
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Hypothesis: Es expanananziehen und Kontrakt in der Sommer erheben.
2024-05-23 23:14:51,068 - INFO - joeynmt.training - Example #4
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:14:51,068 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'ich', 'Ihnen', 'eine', 'Re@@', 'pi@@', 'de', 'zeigen,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 23:14:51,068 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:14:51,069 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:14:51,069 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen ich Ihnen eine Repide zeigen, was in den letzten 25 Jahren in den letzten 25 Jahren passiert.
2024-05-23 23:15:13,833 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     1.906750, Batch Acc: 0.479748, Tokens per Sec:     3216, Lr: 0.000300
2024-05-23 23:15:35,709 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     1.884507, Batch Acc: 0.483724, Tokens per Sec:     3389, Lr: 0.000300
2024-05-23 23:15:57,218 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     1.748707, Batch Acc: 0.480997, Tokens per Sec:     3379, Lr: 0.000300
2024-05-23 23:16:19,447 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.881381, Batch Acc: 0.487716, Tokens per Sec:     3437, Lr: 0.000300
2024-05-23 23:16:41,153 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.848002, Batch Acc: 0.482073, Tokens per Sec:     3335, Lr: 0.000300
2024-05-23 23:16:41,154 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:16:41,154 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:17:59,006 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.13, acc:   0.46, generation: 77.8450[sec], evaluation: 0.0000[sec]
2024-05-23 23:17:59,009 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:17:59,317 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/15000.ckpt
2024-05-23 23:17:59,495 - INFO - joeynmt.training - Example #0
2024-05-23 23:17:59,495 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:17:59,495 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:17:59,495 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', ',', 'die', 'am', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'war', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'such@@', 'en,', 'die', 'meisten', 'unter@@', 'halb', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'halb', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'such@@', 't.', '</s>']
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die Arktische Eis, die am meisten drei Millionen Jahre lang der letzten drei Millionen Jahre war die Größe der Größe der untersuchen, die meisten unterhalb der Größe der unterhalb der Größe der untersucht.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - Example #1
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is@@', 'se', 'zeigt', 'sich', 'die', 'D@@', 'ick@@', 'en@@', 'heit', 'der', 'E@@', 'is@@', 'mus', 'zeigen.', '</s>']
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses speziellen Problems dieses speziellen Problems, weil es nicht die Dickheit der Eisse zeigt sich die Dickenheit der Eismus zeigen.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - Example #2
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'et@@', 'e,', 'in', 'einem', 'Sinn@@', ',', 'der', 'B@@', 'üh@@', 'ne', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiszete, in einem Sinn, der Bühne des Klimasystem.
2024-05-23 23:17:59,496 - INFO - joeynmt.training - Example #3
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:17:59,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'auch', 'auch', 'in', 'dem', 'F@@', 'enst@@', 'ern', 'und', 'Kon@@', 'tra@@', 'k@@', 'ts', 'in', 'S@@', 'omm@@', 'er@@', 'in.', '</s>']
2024-05-23 23:17:59,497 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:17:59,497 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:17:59,497 - INFO - joeynmt.training - 	Hypothesis: Es gibt auch auch in dem Fenstern und Kontrakts in Sommerin.
2024-05-23 23:17:59,497 - INFO - joeynmt.training - Example #4
2024-05-23 23:17:59,497 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:17:59,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:17:59,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'Re@@', 'chn@@', 'ung', 'von', 'der', 'letzten', '25', 'Jahr@@', 'en.', '</s>']
2024-05-23 23:17:59,497 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:17:59,497 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:17:59,497 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen eine Rechnung von der letzten 25 Jahren.
2024-05-23 23:18:21,803 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.875031, Batch Acc: 0.477715, Tokens per Sec:     3271, Lr: 0.000300
2024-05-23 23:18:43,715 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.831890, Batch Acc: 0.480867, Tokens per Sec:     3237, Lr: 0.000300
2024-05-23 23:19:05,236 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     2.074422, Batch Acc: 0.475894, Tokens per Sec:     3401, Lr: 0.000300
2024-05-23 23:19:26,951 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.832002, Batch Acc: 0.480883, Tokens per Sec:     3300, Lr: 0.000300
2024-05-23 23:19:48,903 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.787500, Batch Acc: 0.478668, Tokens per Sec:     3372, Lr: 0.000300
2024-05-23 23:19:48,904 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:19:48,904 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:20:56,783 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.04, acc:   0.46, generation: 67.8726[sec], evaluation: 0.0000[sec]
2024-05-23 23:20:56,785 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:20:57,093 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/15500.ckpt
2024-05-23 23:20:57,307 - INFO - joeynmt.training - Example #0
2024-05-23 23:20:57,307 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:20:57,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:20:57,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en,', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'auf@@', 'ge@@', 'zeigt,', 'dass', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'hat', 'sich', 'um', '40', 'Prozent', 'ver@@', 'lang@@', 't.', '</s>']
2024-05-23 23:20:57,307 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien, so dass die arktische Eis aufgezeigt, dass die meisten der letzten drei Millionen Jahre lang der letzten drei Millionen Jahre lang der Lower 48 Staaten, hat sich um 40 Prozent verlangt.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - Example #1
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'ern@@', 'st@@', 'es', 'ist', 'das', 'das', 'Problem', 'der', 'spezi@@', 'ellen', 'Problem@@', 'e,', 'denn', 'es', 'zeigt', 'nicht', 'die', 'd@@', 'ün@@', 'n@@', 't.', '</s>']
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des ernstes ist das das Problem der speziellen Probleme, denn es zeigt nicht die dünnt.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - Example #2
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', ',', 'das', 'G@@', 'eg@@', 'ent@@', 'eil@@', ',', 'das', 'das', 'G@@', 'lob@@', 'us', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einem Sinn, das Gegenteil, das das Globus des Klimasystem.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - Example #3
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'sich', 'die', 'W@@', 'en@@', 'dep@@', 'unk@@', 'te', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'dem', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - 	Hypothesis: Es erwartet sich die Wendepunkte und Kontrakte in der Sommer-Sommer-und Kontrakte in der Sommer-Sommer-Sommer-und Kontrakte in dem Sommer-Sommer-Sommer.
2024-05-23 23:20:57,308 - INFO - joeynmt.training - Example #4
2024-05-23 23:20:57,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:20:57,309 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:20:57,309 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'Re@@', 'p@@', 'fl@@', 'anz@@', 'k@@', 'ap@@', 'ut@@', 't', 'wird.', '</s>']
2024-05-23 23:20:57,309 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:20:57,309 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:20:57,309 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen ein Repflanzkaputt wird.
2024-05-23 23:21:20,771 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.894444, Batch Acc: 0.479566, Tokens per Sec:     3139, Lr: 0.000300
2024-05-23 23:21:41,932 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.898749, Batch Acc: 0.483940, Tokens per Sec:     3393, Lr: 0.000300
2024-05-23 23:22:03,524 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     2.011168, Batch Acc: 0.474654, Tokens per Sec:     3198, Lr: 0.000300
2024-05-23 23:22:25,946 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.999148, Batch Acc: 0.482332, Tokens per Sec:     3382, Lr: 0.000300
2024-05-23 23:22:48,363 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.973742, Batch Acc: 0.477051, Tokens per Sec:     3376, Lr: 0.000300
2024-05-23 23:22:48,364 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:22:48,365 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:24:03,298 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.93, acc:   0.47, generation: 74.9264[sec], evaluation: 0.0000[sec]
2024-05-23 23:24:03,300 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:24:03,664 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/16000.ckpt
2024-05-23 23:24:03,807 - INFO - joeynmt.training - Example #0
2024-05-23 23:24:03,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:24:03,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:24:03,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Größ@@', 'e', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'hat', 'es', 'S@@', 'hr@@', 'un@@', 'sch@@', '.', '</s>']
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien so dass die arktischen Eis der letzten drei Millionen Jahre lang der letzten drei Millionen Jahre lang der Größe der letzten drei Millionen Jahre lang der Größe der Größe 48 Staaten, hat es Shrunsch.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - Example #1
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'Ver@@', 'st@@', 'and,', 'das', 'das', 'Problem', 'dieser', 'spezi@@', 'elle', 'Problem@@', 'e,', 'denn', 'es', 'zeigt', 'die', 'D@@', 'ick@@', 's@@', 'ität', 'der', 'E@@', 'is@@', 'e.', '</s>']
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser Verstand, das das Problem dieser spezielle Probleme, denn es zeigt die Dicksität der Eise.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - Example #2
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', 'e,', 'der', 'Sch@@', 'ei@@', 'ß@@', 'en@@', 'm@@', 'ü@@', 'll@@', 'ung', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis Eis ist in einem Sinne, der Scheißenmüllung des Klimasystem.
2024-05-23 23:24:03,808 - INFO - joeynmt.training - Example #3
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:24:03,808 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'auch', 'W@@', 'ind', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er@@', '-@@', 'S@@', 'omm@@', 'er@@', '-@@', 'Z@@', 'au@@', 'ber@@', '.', '</s>']
2024-05-23 23:24:03,809 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:24:03,809 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:24:03,809 - INFO - joeynmt.training - 	Hypothesis: Es gibt auch Wind und Kontrakte in der Sommer-Sommer-Zauber.
2024-05-23 23:24:03,809 - INFO - joeynmt.training - Example #4
2024-05-23 23:24:03,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:24:03,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:24:03,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'das', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ah', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'ah', 'über', 'die', 'letzten', '25', 'Jahre', 'gesch@@', 'ah', 'gesch@@', 'ah', 'über', 'die', 'letzten', '25', 'Jahre', 'gesch@@', 'ah', 'gesch@@', 'ah', 'ist.', '</s>']
2024-05-23 23:24:03,809 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:24:03,809 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:24:03,809 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen zeigen werde, das in den letzten 25 Jahren geschah über die letzten 25 Jahren geschah über die letzten 25 Jahre geschah geschah über die letzten 25 Jahre geschah geschah ist.
2024-05-23 23:24:26,975 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     2.006080, Batch Acc: 0.481796, Tokens per Sec:     3152, Lr: 0.000300
2024-05-23 23:24:48,683 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.887023, Batch Acc: 0.481698, Tokens per Sec:     3378, Lr: 0.000300
2024-05-23 23:25:10,933 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.788068, Batch Acc: 0.477172, Tokens per Sec:     3420, Lr: 0.000300
2024-05-23 23:25:32,397 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.858862, Batch Acc: 0.480012, Tokens per Sec:     3397, Lr: 0.000300
2024-05-23 23:25:54,385 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.759678, Batch Acc: 0.482967, Tokens per Sec:     3390, Lr: 0.000300
2024-05-23 23:25:54,386 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:25:54,386 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:27:11,648 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.94, acc:   0.47, generation: 77.2545[sec], evaluation: 0.0000[sec]
2024-05-23 23:27:11,972 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/16500.ckpt
2024-05-23 23:27:12,128 - INFO - joeynmt.training - Example #0
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en,', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'is', 'is', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'alt', 'wur@@', 'den.', '</s>']
2024-05-23 23:27:12,129 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:27:12,129 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:27:12,129 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folien, so dass die arktische Eis is is für die meisten der letzten drei Millionen Jahre lang der letzten drei Millionen Jahre alt wurden.
2024-05-23 23:27:12,129 - INFO - joeynmt.training - Example #1
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'An@@', 's@@', 'ät@@', 'ze', 'dieses', 'spezi@@', 'ellen', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 23:27:12,129 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:27:12,129 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:27:12,129 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser Ansätze dieses speziellen Problem ist, weil es nicht die Dicksal des Eiss.
2024-05-23 23:27:12,129 - INFO - joeynmt.training - Example #2
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:27:12,129 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'im', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist im Sinne des globalen Klimasystem.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - Example #3
2024-05-23 23:27:12,130 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:27:12,130 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:27:12,130 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'ver@@', 'fol@@', 'g@@', 't.', '</s>']
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte in der Sommer und verfolgt.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - Example #4
2024-05-23 23:27:12,130 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:27:12,130 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:27:12,130 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigt', 'Ihnen', 'eine', 'Re@@', 'gi@@', 'k', 'ver@@', 'gew@@', 'al@@', 'tigt', 'werden,', 'was', 'über', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:27:12,130 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigt Ihnen eine Regik vergewaltigt werden, was über den letzten 25 Jahren passiert ist.
2024-05-23 23:27:34,152 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     1.857074, Batch Acc: 0.488259, Tokens per Sec:     3327, Lr: 0.000300
2024-05-23 23:27:55,614 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.896804, Batch Acc: 0.477251, Tokens per Sec:     3463, Lr: 0.000300
2024-05-23 23:28:17,470 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     2.117052, Batch Acc: 0.480791, Tokens per Sec:     3322, Lr: 0.000300
2024-05-23 23:28:36,589 - INFO - joeynmt.training - Epoch   5: total training loss 7255.22
2024-05-23 23:28:36,589 - INFO - joeynmt.training - EPOCH 6
2024-05-23 23:28:39,684 - INFO - joeynmt.training - Epoch   6, Step:    19400, Batch Loss:     1.796954, Batch Acc: 0.489904, Tokens per Sec:     3185, Lr: 0.000300
2024-05-23 23:29:01,833 - INFO - joeynmt.training - Epoch   6, Step:    19500, Batch Loss:     1.849173, Batch Acc: 0.504243, Tokens per Sec:     3363, Lr: 0.000300
2024-05-23 23:29:01,834 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:29:01,834 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:30:13,062 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.84, acc:   0.47, generation: 71.2193[sec], evaluation: 0.0000[sec]
2024-05-23 23:30:13,063 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:30:13,411 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/17000.ckpt
2024-05-23 23:30:13,561 - INFO - joeynmt.training - Example #0
2024-05-23 23:30:13,561 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:30:13,561 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:30:13,561 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeigt,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'E@@', 'is', 'is', 'is', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'aten', 'gew@@', 'esen', 'wurde.', '</s>']
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigt, dass die arktische Eis Eis is is für die meisten drei Millionen Jahre lang der letzten drei Millionen Jahre lang der Lower 48 Staaten gewesen wurde.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - Example #1
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'S@@', 'eri@@', 'ous@@', 's', 'dieses', 'spezi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is@@', 'b@@', 'ah@@', 'n', 'ist.', '</s>']
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des Seriouss dieses speziellen Problem, weil es nicht das Dickheit der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eisbahn ist.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - Example #2
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'im', 'Sin@@', 'ne', 'des', 'Klima@@', 'system', 'im', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - 	Hypothesis: Die arktische Eis ist im Sinne des Klimasystem im globalen Klimasystem des Klimasystems des Klimasystem.
2024-05-23 23:30:13,562 - INFO - joeynmt.training - Example #3
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:30:13,562 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'in', 'der', 'S@@', 'omm@@', 'er@@', 'f@@', 'ä@@', 'hi@@', 'g', 'und', 'Kon@@', 'tra@@', 'k@@', 'ten', 'in', 'S@@', 'omm@@', 'er@@', 'f@@', 'ä@@', 'hi@@', 'g', 'sind.', '</s>']
2024-05-23 23:30:13,563 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:30:13,563 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:30:13,563 - INFO - joeynmt.training - 	Hypothesis: Es gibt es in Winter und Kontrakten in Sommer und Kontrag in Sommer und Kontrag in der Sommerfähig und Kontrakten in Sommerfähig sind.
2024-05-23 23:30:13,563 - INFO - joeynmt.training - Example #4
2024-05-23 23:30:13,563 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:30:13,563 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:30:13,563 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'werde', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', '-@@', 'Wal@@', 'd', 'sein,', 'was', 'in', 'den', 'letzten', '25', 'Jahre', 'passier@@', 't.', '</s>']
2024-05-23 23:30:13,563 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:30:13,563 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:30:13,563 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen werde ich Ihnen ein schnelles schnell-Wald sein, was in den letzten 25 Jahre passiert.
2024-05-23 23:30:36,590 - INFO - joeynmt.training - Epoch   6, Step:    19600, Batch Loss:     1.872425, Batch Acc: 0.508011, Tokens per Sec:     3099, Lr: 0.000300
2024-05-23 23:30:59,344 - INFO - joeynmt.training - Epoch   6, Step:    19700, Batch Loss:     1.790490, Batch Acc: 0.507259, Tokens per Sec:     3288, Lr: 0.000300
2024-05-23 23:31:20,874 - INFO - joeynmt.training - Epoch   6, Step:    19800, Batch Loss:     1.723253, Batch Acc: 0.506543, Tokens per Sec:     3382, Lr: 0.000300
2024-05-23 23:31:42,850 - INFO - joeynmt.training - Epoch   6, Step:    19900, Batch Loss:     1.895903, Batch Acc: 0.500679, Tokens per Sec:     3383, Lr: 0.000300
2024-05-23 23:32:06,030 - INFO - joeynmt.training - Epoch   6, Step:    20000, Batch Loss:     1.781427, Batch Acc: 0.501300, Tokens per Sec:     3252, Lr: 0.000300
2024-05-23 23:32:06,031 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:32:06,031 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:33:30,454 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.87, acc:   0.47, generation: 84.4157[sec], evaluation: 0.0000[sec]
2024-05-23 23:33:30,797 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/17500.ckpt
2024-05-23 23:33:30,932 - INFO - joeynmt.training - Example #0
2024-05-23 23:33:30,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:33:30,932 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:33:30,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en,', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'e,', 'die', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'b@@', 'eig@@', ',', 'die', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Prozent', 'ist.', '</s>']
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien, so dass die arktische Eise, die die arktische Eisbeig, die für die meisten letzten drei Millionen Jahre lang die Größe 48 Staaten, die Größe der unteren 48 Prozent ist.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - Example #1
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'ä@@', 'tig@@', 'keit', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'enk@@', 'ar@@', 'tige', 'zu', 'zeigen,', 'weil', 'es', 'nicht', 'das', 'D@@', 'enk@@', 'ar@@', 'z@@', 't.', '</s>']
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied ist die ernsthätigkeit dieses Problem ist, weil es nicht die Denkartige zu zeigen, weil es nicht das Denkarzt.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - Example #2
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'et@@', 'e,', 'in', 'einem', 'Sinn@@', ',', 'das', 'G@@', 'an@@', 'ze', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiszete, in einem Sinn, das Ganze des globalen Klimasystem des Klimasystem
2024-05-23 23:33:30,933 - INFO - joeynmt.training - Example #3
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:33:30,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'ä@@', 'be', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'g@@', 'ung', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'g@@', 'ung', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'g@@', 'ung', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'kehr@@', 's@@', 'eit@@', 'ung.', '</s>']
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:33:30,933 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verfolgsäbe in der Sommer und Verfolgung in der Sommer und Verfolgung in der Sommer und Verfolgung in der Sommer und Verkehrseitung.
2024-05-23 23:33:30,934 - INFO - joeynmt.training - Example #4
2024-05-23 23:33:30,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:33:30,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:33:30,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'wird,', 'dass', 'ich', 'Ihnen', 'ein', 'schnell', 'schnell', 'schnell', 'schnell', 'vor@@', 'wär@@', 'ts', 'ist,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-23 23:33:30,934 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:33:30,934 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:33:30,934 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen wird, dass ich Ihnen ein schnell schnell schnell schnell vorwärts ist, was in den letzten 25 Jahren geschehen ist.
2024-05-23 23:33:54,932 - INFO - joeynmt.training - Epoch   6, Step:    20100, Batch Loss:     1.640213, Batch Acc: 0.501503, Tokens per Sec:     3072, Lr: 0.000300
2024-05-23 23:34:18,397 - INFO - joeynmt.training - Epoch   6, Step:    20200, Batch Loss:     1.817965, Batch Acc: 0.502572, Tokens per Sec:     3173, Lr: 0.000300
2024-05-23 23:34:41,230 - INFO - joeynmt.training - Epoch   6, Step:    20300, Batch Loss:     1.632285, Batch Acc: 0.502139, Tokens per Sec:     3204, Lr: 0.000300
2024-05-23 23:35:04,482 - INFO - joeynmt.training - Epoch   6, Step:    20400, Batch Loss:     1.697443, Batch Acc: 0.500095, Tokens per Sec:     3158, Lr: 0.000300
2024-05-23 23:35:27,717 - INFO - joeynmt.training - Epoch   6, Step:    20500, Batch Loss:     1.900381, Batch Acc: 0.501283, Tokens per Sec:     3170, Lr: 0.000300
2024-05-23 23:35:27,718 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:35:27,718 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:36:50,019 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.88, acc:   0.47, generation: 82.2943[sec], evaluation: 0.0000[sec]
2024-05-23 23:36:50,356 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/18000.ckpt
2024-05-23 23:36:50,531 - INFO - joeynmt.training - Example #0
2024-05-23 23:36:50,531 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:36:50,531 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:36:50,531 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'e,', 'die', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'such@@', 'enden', '4@@', '8', 'Sta@@', 'ats@@', 'b@@', 'ild', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folie gezeigt habe, dass die arktische Eise, die für die meisten letzten drei Millionen Jahren die Größe der letzten drei Millionen Jahren die Größe der untersuchenden 48 Staatsbild von 40 Prozent.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - Example #1
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'Ern@@', 'st,', 'die', 'ern@@', 's@@', 'th@@', 'eit', 'dieser', 'spezi@@', 'elle', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's', 'zeigen.', '</s>']
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des Ernst, die ernstheit dieser spezielle Probleme, weil es nicht die Dicksal des Eiss zeigen.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - Example #2
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'tik@@', 'tik@@', 'tik@@', 'el', 'ist', 'in', 'einem', 'Sinn@@', ',', 'das', 'Sch@@', 'n@@', 'it@@', 'te,', 'das', 'G@@', 'lob@@', 'al', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Hypothesis: Die Artiktiktikel ist in einem Sinn, das Schnitte, das Global des Klimasystem.
2024-05-23 23:36:50,532 - INFO - joeynmt.training - Example #3
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:36:50,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'w@@', 'äch@@', 'st', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 's@@', 'ä@@', 'ß@@', '.', '</s>']
2024-05-23 23:36:50,532 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:36:50,533 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:36:50,533 - INFO - joeynmt.training - 	Hypothesis: Es wächst in Winter und Verfolgssäß.
2024-05-23 23:36:50,533 - INFO - joeynmt.training - Example #4
2024-05-23 23:36:50,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:36:50,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:36:50,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'schn@@', 'eller', 'schn@@', 'eller', 'auf', 'die', 'letzten', '25', 'Jahren', 'passier@@', 't,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 23:36:50,533 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:36:50,533 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:36:50,533 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen ein schneller schneller auf die letzten 25 Jahren passiert, was in den letzten 25 Jahren passiert.
2024-05-23 23:37:14,427 - INFO - joeynmt.training - Epoch   6, Step:    20600, Batch Loss:     1.828564, Batch Acc: 0.495939, Tokens per Sec:     3011, Lr: 0.000300
2024-05-23 23:37:36,830 - INFO - joeynmt.training - Epoch   6, Step:    20700, Batch Loss:     1.726517, Batch Acc: 0.501269, Tokens per Sec:     3341, Lr: 0.000300
2024-05-23 23:37:59,945 - INFO - joeynmt.training - Epoch   6, Step:    20800, Batch Loss:     1.985403, Batch Acc: 0.500986, Tokens per Sec:     3182, Lr: 0.000300
2024-05-23 23:38:22,398 - INFO - joeynmt.training - Epoch   6, Step:    20900, Batch Loss:     1.606817, Batch Acc: 0.498521, Tokens per Sec:     3283, Lr: 0.000300
2024-05-23 23:38:45,018 - INFO - joeynmt.training - Epoch   6, Step:    21000, Batch Loss:     2.018636, Batch Acc: 0.501387, Tokens per Sec:     3331, Lr: 0.000300
2024-05-23 23:38:45,019 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:38:45,019 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:40:14,857 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.78, acc:   0.47, generation: 89.8301[sec], evaluation: 0.0000[sec]
2024-05-23 23:40:14,859 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:40:15,197 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/19000.ckpt
2024-05-23 23:40:15,337 - INFO - joeynmt.training - Example #0
2024-05-23 23:40:15,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:40:15,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:40:15,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en,', 'die', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'b@@', 'ah@@', 'n@@', 'b@@', 'ah@@', 'n@@', 'b@@', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'hat', 'die', 'S@@', 'hr@@', 'un@@', 'k@@', 'ün@@', 'sch@@', 't', 'die', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:40:15,337 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:40:15,337 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:40:15,337 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien, die die arktischen Eisbahnbahnb, die für die meisten drei Millionen Jahre der unteren 48 Staaten, hat die Shrunkünscht die von 40 Prozent.
2024-05-23 23:40:15,337 - INFO - joeynmt.training - Example #1
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'Ver@@', 'b@@', 'un@@', 'den@@', 's@@', 'theor@@', 'et@@', 'isch', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'b@@', 'ah@@', 'n@@', 'b@@', 'ild', 'ist.', '</s>']
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Hypothesis: Aber das Verbundenstheoretisch dieses Problem ist, weil es nicht die Dicksal des Eisbahnbild ist.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - Example #2
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'isch@@', 'e,', 'die', 'Ar@@', 'kt@@', 'is', 'ist', 'im', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Hypothesis: Die Arktische, die Arktis ist im Sinne des globalen Klimasystem des globalen Klimasystem.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - Example #3
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'den', 'Sch@@', 'n@@', 'it@@', 't@@', 'eil', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'die', 'B@@', 'enz@@', 'in@@', '-@@', 'Z@@', 'un@@', 'äch@@', 'st', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'die', 'B@@', 'enz@@', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'den', 'S@@', 'omm@@', 'er', 'im', 'S@@', 'omm@@', 'er', 'S@@', 'omm@@', 'er', 'und', 'den', 'Sch@@', 'mer@@', '.', '</s>']
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und den Schnitteil in der Sommer und die Benzin-Zunächst in der Sommer und die Benzin der Sommer und den Sommer im Sommer Sommer und den Schmer.
2024-05-23 23:40:15,338 - INFO - joeynmt.training - Example #4
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:40:15,338 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'li@@', 'en', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'Re@@', 'chn@@', 'er', 'passier@@', 't.', '</s>']
2024-05-23 23:40:15,339 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:40:15,339 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:40:15,339 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folien zeige ich Ihnen ein schnelles schnelles schnelles Rechner passiert.
2024-05-23 23:40:39,151 - INFO - joeynmt.training - Epoch   6, Step:    21100, Batch Loss:     1.836565, Batch Acc: 0.499577, Tokens per Sec:     3017, Lr: 0.000300
2024-05-23 23:41:01,943 - INFO - joeynmt.training - Epoch   6, Step:    21200, Batch Loss:     1.842426, Batch Acc: 0.499700, Tokens per Sec:     3223, Lr: 0.000300
2024-05-23 23:41:25,134 - INFO - joeynmt.training - Epoch   6, Step:    21300, Batch Loss:     1.869300, Batch Acc: 0.504965, Tokens per Sec:     3252, Lr: 0.000300
2024-05-23 23:41:47,289 - INFO - joeynmt.training - Epoch   6, Step:    21400, Batch Loss:     1.696797, Batch Acc: 0.494652, Tokens per Sec:     3355, Lr: 0.000300
2024-05-23 23:42:10,824 - INFO - joeynmt.training - Epoch   6, Step:    21500, Batch Loss:     1.832926, Batch Acc: 0.497800, Tokens per Sec:     3187, Lr: 0.000300
2024-05-23 23:42:10,824 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:42:10,824 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:43:38,375 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.75, acc:   0.47, generation: 87.5436[sec], evaluation: 0.0000[sec]
2024-05-23 23:43:38,377 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:43:38,726 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/18500.ckpt
2024-05-23 23:43:38,912 - INFO - joeynmt.training - Example #0
2024-05-23 23:43:38,912 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:43:38,912 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:43:38,912 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeig@@', 'te,', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'e,', 'die', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'h@@', 'ne', 'Geb@@', 'i@@', 'ete', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'hat', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Prozent', 'ist.', '</s>']
2024-05-23 23:43:38,912 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:43:38,912 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:43:38,912 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigte, dass die arktischen Eise, die die meisten der letzten drei Millionen Jahren die Größe 48 Stahne Gebiete der Größe 48 Staaten, hat Shrunk von 40 Prozent ist.
2024-05-23 23:43:38,912 - INFO - joeynmt.training - Example #1
2024-05-23 23:43:38,912 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:43:38,912 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:43:38,912 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'tr@@', 'äum@@', 'e', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'ell', 'Problem@@', 's', 'ist', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'des', 'E@@', 'is@@', 's', 'der', 'E@@', 'is@@', 's', 'des', 'E@@', 'is@@', 's', 'der', 'E@@', 'is', 'des', 'E@@', 'is', 'der', 'E@@', 'is@@', 'en@@', 's.', '</s>']
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterträume die ernsthaft dieses speziell Problems ist der Eis des Eis der Eis der Eis der Eis der Eis der Eis der Eis der Eis des Eiss der Eiss des Eiss der Eis des Eis der Eisens.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - Example #2
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'et@@', 'e,', 'in', 'einem', 'Sinn@@', ',', 'der', 'das', 'glob@@', 'alen', 'Klima@@', 'wan@@', 'del', 'des', 'Klima@@', 'systems', '</s>']
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiszete, in einem Sinn, der das globalen Klimawandel des Klimasystems
2024-05-23 23:43:38,913 - INFO - joeynmt.training - Example #3
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'tr@@', 'it@@', 't', 'in', 'S@@', 'omm@@', 'er', 'und', 'den', 'B@@', 'enz@@', '.', '</s>']
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Vertritt in Sommer und den Benz.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - Example #4
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:43:38,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', 'as@@', 'se', 'zei@@', 'ge', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'dass', 'ich', 'die', 'letzten', '25', 'Jahren', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:43:38,913 - INFO - joeynmt.training - 	Hypothesis: Die nächste Diasse zeige ich Ihnen zeigen werde, dass ich die letzten 25 Jahren in den letzten 25 Jahren passiert.
2024-05-23 23:44:02,859 - INFO - joeynmt.training - Epoch   6, Step:    21600, Batch Loss:     1.966315, Batch Acc: 0.495424, Tokens per Sec:     3017, Lr: 0.000300
2024-05-23 23:44:26,711 - INFO - joeynmt.training - Epoch   6, Step:    21700, Batch Loss:     1.953752, Batch Acc: 0.497034, Tokens per Sec:     3074, Lr: 0.000300
2024-05-23 23:44:50,202 - INFO - joeynmt.training - Epoch   6, Step:    21800, Batch Loss:     1.748331, Batch Acc: 0.501405, Tokens per Sec:     3196, Lr: 0.000300
2024-05-23 23:45:13,216 - INFO - joeynmt.training - Epoch   6, Step:    21900, Batch Loss:     1.868051, Batch Acc: 0.498932, Tokens per Sec:     3276, Lr: 0.000300
2024-05-23 23:45:36,608 - INFO - joeynmt.training - Epoch   6, Step:    22000, Batch Loss:     1.725899, Batch Acc: 0.496560, Tokens per Sec:     3138, Lr: 0.000300
2024-05-23 23:45:36,609 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:45:36,609 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:46:55,473 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.71, acc:   0.48, generation: 78.8557[sec], evaluation: 0.0000[sec]
2024-05-23 23:46:55,474 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:46:55,831 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/20500.ckpt
2024-05-23 23:46:55,966 - INFO - joeynmt.training - Example #0
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'ist,', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'en@@', 'ord@@', 'ent@@', 'lichen', '4@@', '8', 'Sta@@', 'aten', 'Sta@@', 'aten', 'ges@@', 'tal@@', 'ten,', 'hat', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'e', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:46:55,966 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:46:55,966 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:46:55,966 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien so dass die arktischen Eis ist, dass die meisten drei Millionen Jahre lang der letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größenordentlichen 48 Staaten Staaten gestalten, hat die Größe der untere von 40 Prozent.
2024-05-23 23:46:55,966 - INFO - joeynmt.training - Example #1
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'S@@', 'eri@@', 'en@@', 's,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'en@@', 'heit', 'des', 'E@@', 'is@@', 's', 'zeigen,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'en@@', 'heit', 'des', 'E@@', 'is@@', 's', 'zeigen.', '</s>']
2024-05-23 23:46:55,966 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:46:55,966 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:46:55,966 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des Seriens, weil es nicht die Dickenheit des Eiss zeigen, weil es nicht die Dickenheit des Eiss zeigen.
2024-05-23 23:46:55,966 - INFO - joeynmt.training - Example #2
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:46:55,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'ist', 'in', 'einem', 'Sin@@', 'ne', 'des', 'Klima@@', 'system', 'ist', 'das', 'G@@', 'lob@@', 'al', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Hypothesis: Die Arktis ist in einem Sinne des Klimasystem ist das Global des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem
2024-05-23 23:46:55,967 - INFO - joeynmt.training - Example #3
2024-05-23 23:46:55,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:46:55,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:46:55,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'auch', 'auch', 'auch', 'das', 'Ge@@', 'win@@', 'n', 'und', 'das', 'Ver@@', 'fol@@', 'ge', 'in', 'der', 'S@@', 'omm@@', 'er@@', 'en.', '</s>']
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Hypothesis: Es gibt auch auch auch das Gewinn und das Verfolge in der Sommeren.
2024-05-23 23:46:55,967 - INFO - joeynmt.training - Example #4
2024-05-23 23:46:55,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:46:55,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:46:55,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Di@@', 'a', 'zeigt', 'Ihnen', 'ein', 'R@@', 'is@@', 'ik@@', 'o', 'schn@@', 'ell@@', '-@@', 'St@@', 're@@', 'm@@', 'ung', 'von', 'der', 'letzten', '25', 'Jahr@@', 'en.', '</s>']
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:46:55,967 - INFO - joeynmt.training - 	Hypothesis: Die nächste Dia zeigt Ihnen ein Risiko schnell-Stremung von der letzten 25 Jahren.
2024-05-23 23:47:20,253 - INFO - joeynmt.training - Epoch   6, Step:    22100, Batch Loss:     1.799510, Batch Acc: 0.494858, Tokens per Sec:     2908, Lr: 0.000300
2024-05-23 23:47:42,957 - INFO - joeynmt.training - Epoch   6, Step:    22200, Batch Loss:     1.913816, Batch Acc: 0.495518, Tokens per Sec:     3317, Lr: 0.000300
2024-05-23 23:48:05,570 - INFO - joeynmt.training - Epoch   6, Step:    22300, Batch Loss:     1.651029, Batch Acc: 0.495956, Tokens per Sec:     3265, Lr: 0.000300
2024-05-23 23:48:28,018 - INFO - joeynmt.training - Epoch   6, Step:    22400, Batch Loss:     1.626868, Batch Acc: 0.498092, Tokens per Sec:     3211, Lr: 0.000300
2024-05-23 23:48:50,486 - INFO - joeynmt.training - Epoch   6, Step:    22500, Batch Loss:     1.644425, Batch Acc: 0.498094, Tokens per Sec:     3257, Lr: 0.000300
2024-05-23 23:48:50,486 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:48:50,486 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:50:03,399 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.69, acc:   0.47, generation: 72.9054[sec], evaluation: 0.0000[sec]
2024-05-23 23:50:03,401 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:50:03,741 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/20000.ckpt
2024-05-23 23:50:03,892 - INFO - joeynmt.training - Example #0
2024-05-23 23:50:03,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:50:03,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:50:03,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'en@@', 'ord@@', 'nung', '4@@', '8', 'Sta@@', 'aten', 'Sta@@', 'aten', 'ver@@', 'di@@', 'en@@', 'en.', '</s>']
2024-05-23 23:50:03,892 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:50:03,892 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:50:03,892 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Eis der letzten drei Millionen Jahre lang der letzten drei Millionen Jahre der Lower 48 Staaten die Größenordnung 48 Staaten Staaten verdienen.
2024-05-23 23:50:03,892 - INFO - joeynmt.training - Example #1
2024-05-23 23:50:03,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:50:03,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:50:03,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'spezi@@', 'ellen', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ün@@', 'dung', 'der', 'E@@', 'is@@', 'se', 'der', 'E@@', 'is@@', 'se', 'der', 'E@@', 'is@@', 'sionen', 'der', 'E@@', 'is@@', 'sionen', 'der', 'E@@', 'is@@', 'se', 'zeigen.', '</s>']
2024-05-23 23:50:03,892 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:50:03,892 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:50:03,892 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser speziellen Probleme, weil es nicht die Dündung der Eisse der Eisse der Eissionen der Eissionen der Eisse zeigen.
2024-05-23 23:50:03,892 - INFO - joeynmt.training - Example #2
2024-05-23 23:50:03,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', ',', 'das', 'Sch@@', 'ei@@', 'ß', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einem Sinn, das Scheiß des globalen Klimasystem des Klimasystem des Klimasystem des Klimasystem
2024-05-23 23:50:03,893 - INFO - joeynmt.training - Example #3
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'auch', 'W@@', 'en@@', 'ter', 'und', 'die', 'S@@', 'omm@@', 'er', 'in', 'S@@', 'omm@@', 'er', 'und', 'die', 'Ver@@', 'fol@@', 'ge', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'gen', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'gen.', '</s>']
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Hypothesis: Es gibt auch Wenter und die Sommer in Sommer und die Verfolge in den Sommer und Verfolgen in den Sommer und Verfolgen.
2024-05-23 23:50:03,893 - INFO - joeynmt.training - Example #4
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:50:03,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', '-@@', 'Wal@@', 'd', 'der', 'passier@@', 'te.', '</s>']
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:50:03,893 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen ein schnelles schnell-Wald der passierte.
2024-05-23 23:50:27,932 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.794056, Batch Acc: 0.502641, Tokens per Sec:     3017, Lr: 0.000300
2024-05-23 23:50:50,459 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.892407, Batch Acc: 0.497718, Tokens per Sec:     3201, Lr: 0.000300
2024-05-23 23:51:12,915 - INFO - joeynmt.training - Epoch   6, Step:    22800, Batch Loss:     1.847071, Batch Acc: 0.500463, Tokens per Sec:     3271, Lr: 0.000300
2024-05-23 23:51:35,722 - INFO - joeynmt.training - Epoch   6, Step:    22900, Batch Loss:     1.753711, Batch Acc: 0.493138, Tokens per Sec:     3182, Lr: 0.000300
2024-05-23 23:51:58,630 - INFO - joeynmt.training - Epoch   6, Step:    23000, Batch Loss:     1.805783, Batch Acc: 0.491447, Tokens per Sec:     3241, Lr: 0.000300
2024-05-23 23:51:58,631 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:51:58,631 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:53:25,536 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.63, acc:   0.47, generation: 86.8964[sec], evaluation: 0.0000[sec]
2024-05-23 23:53:25,538 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-23 23:53:25,866 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/19500.ckpt
2024-05-23 23:53:26,041 - INFO - joeynmt.training - Example #0
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'in', 'den', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'Sta@@', 'at@@', 'en,', 'die', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:53:26,041 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:53:26,041 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:53:26,041 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt, dass die arktische Eis in den meisten drei Millionen Jahre lang die meisten drei Millionen Jahre die Größe der letzten drei Millionen Jahre lang die Größe 48 Staaten, die Größe der niedriger Staaten, die Shrunk von 40 Prozent.
2024-05-23 23:53:26,041 - INFO - joeynmt.training - Example #1
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'das', 'unter@@', 'schei@@', 'det', 'die', 'S@@', 'eri@@', 'en@@', 'heit', 'dieses', 'spezi@@', 'elle', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is@@', 'se', 'der', 'E@@', 'is@@', 'se', 'zeigen.', '</s>']
2024-05-23 23:53:26,041 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:53:26,041 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:53:26,041 - INFO - joeynmt.training - 	Hypothesis: Aber das unterscheidet die Serienheit dieses spezielle Problem ist, weil es nicht die Dickheit der Eisse der Eisse zeigen.
2024-05-23 23:53:26,041 - INFO - joeynmt.training - Example #2
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:53:26,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', ',', 'das', 'B@@', 'ier@@', ',', 'der', 'den', 'Klima@@', 'wan@@', 'del', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system.', '</s>']
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eis Eis ist in einem Sinn, das Bier, der den Klimawandel des Klimasystem des Klimasystem.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - Example #3
2024-05-23 23:53:26,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:53:26,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:53:26,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'im', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'dem', 'S@@', 'omm@@', 'er@@', 'in.', '</s>']
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich im Winter und Kontrakte in dem Sommerin.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - Example #4
2024-05-23 23:53:26,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:53:26,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:53:26,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fo@@', 'li@@', 'en', 'wird', 'Ihnen', 'ein', 'ra@@', 'pi@@', 'd', 'schn@@', 'ell@@', '-@@', 'Wal@@', 'd', 'zu', 'sein,', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'passier@@', 'te.', '</s>']
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:53:26,042 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folien wird Ihnen ein rapid schnell-Wald zu sein, was über die letzten 25 Jahren passierte.
2024-05-23 23:53:49,471 - INFO - joeynmt.training - Epoch   6, Step:    23100, Batch Loss:     1.555711, Batch Acc: 0.496161, Tokens per Sec:     3048, Lr: 0.000300
2024-05-23 23:54:12,527 - INFO - joeynmt.training - Epoch   6, Step:    23200, Batch Loss:     1.982022, Batch Acc: 0.501128, Tokens per Sec:     3229, Lr: 0.000300
2024-05-23 23:54:25,332 - INFO - joeynmt.training - Epoch   6: total training loss 6955.39
2024-05-23 23:54:25,332 - INFO - joeynmt.training - EPOCH 7
2024-05-23 23:54:35,592 - INFO - joeynmt.training - Epoch   7, Step:    23300, Batch Loss:     1.931973, Batch Acc: 0.524891, Tokens per Sec:     3252, Lr: 0.000300
2024-05-23 23:54:59,130 - INFO - joeynmt.training - Epoch   7, Step:    23400, Batch Loss:     1.878426, Batch Acc: 0.523453, Tokens per Sec:     3099, Lr: 0.000300
2024-05-23 23:55:22,587 - INFO - joeynmt.training - Epoch   7, Step:    23500, Batch Loss:     1.706216, Batch Acc: 0.520152, Tokens per Sec:     3140, Lr: 0.000300
2024-05-23 23:55:22,588 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:55:22,588 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:56:28,995 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.65, acc:   0.48, generation: 66.3978[sec], evaluation: 0.0000[sec]
2024-05-23 23:56:29,371 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/21000.ckpt
2024-05-23 23:56:29,541 - INFO - joeynmt.training - Example #0
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en,', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'u@@', 'e,', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'meisten', 'n@@', 'ie@@', 'dri@@', 'ger', 'sind', 'und', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:56:29,541 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:56:29,541 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:56:29,541 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich diese zwei Folien, so dass die arktischen Eisue, die meisten drei Millionen Jahre lang die meisten drei Millionen Jahre lang die Größe 48 Staaten, die meisten niedriger sind und Shrunk von 40 Prozent.
2024-05-23 23:56:29,541 - INFO - joeynmt.training - Example #1
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieses', 'spezi@@', 'ellen', 'Problem', 'dieses', 'spezi@@', 'ellen', 'Problem', 'dieses', 'Problem', 'ist,', 'denn', 'es', 'zeigt', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'zeigen.', '</s>']
2024-05-23 23:56:29,541 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:56:29,541 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:56:29,541 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieses speziellen Problem dieses speziellen Problem dieses Problem ist, denn es zeigt die Dickheit der Eis der Eis zeigen.
2024-05-23 23:56:29,541 - INFO - joeynmt.training - Example #2
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:56:29,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:56:29,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', ',', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eissch, in einem Sinn des globalen Klimasystem des globalen Klimasystem.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - Example #3
2024-05-23 23:56:29,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:56:29,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:56:29,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'en.', '</s>']
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte in Sommer und Kontraken.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - Example #4
2024-05-23 23:56:29,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:56:29,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:56:29,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'wird', 'ein', 'ra@@', 'pi@@', 'd', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'St@@', 'elle', 'zu', 'sein,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:56:29,542 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen wird ein rapid schnelles schnelles Stelle zu sein, was in den letzten 25 Jahren geschehen ist.
2024-05-23 23:56:53,113 - INFO - joeynmt.training - Epoch   7, Step:    23600, Batch Loss:     1.774083, Batch Acc: 0.516747, Tokens per Sec:     2973, Lr: 0.000300
2024-05-23 23:57:16,633 - INFO - joeynmt.training - Epoch   7, Step:    23700, Batch Loss:     1.760658, Batch Acc: 0.518273, Tokens per Sec:     3153, Lr: 0.000300
2024-05-23 23:57:39,678 - INFO - joeynmt.training - Epoch   7, Step:    23800, Batch Loss:     1.697159, Batch Acc: 0.522482, Tokens per Sec:     3110, Lr: 0.000300
2024-05-23 23:58:02,571 - INFO - joeynmt.training - Epoch   7, Step:    23900, Batch Loss:     1.787522, Batch Acc: 0.520227, Tokens per Sec:     3123, Lr: 0.000300
2024-05-23 23:58:25,542 - INFO - joeynmt.training - Epoch   7, Step:    24000, Batch Loss:     1.955928, Batch Acc: 0.516317, Tokens per Sec:     3228, Lr: 0.000300
2024-05-23 23:58:25,542 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 23:58:25,542 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-23 23:59:50,520 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.66, acc:   0.48, generation: 84.9698[sec], evaluation: 0.0000[sec]
2024-05-23 23:59:50,840 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/21500.ckpt
2024-05-23 23:59:51,008 - INFO - joeynmt.training - Example #0
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeig@@', 'te', 'so', 'dass', 'der', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'Größ@@', 'e', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'en@@', 'ster@@', 'ben', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-23 23:59:51,008 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-23 23:59:51,008 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-23 23:59:51,008 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigte so dass der arktischen Eis für die meisten der letzten drei Millionen Jahren die Größe der Größe der Lower 48 Staaten die Größensterben 48 Staaten, Shrunk von 40 Prozent.
2024-05-23 23:59:51,008 - INFO - joeynmt.training - Example #1
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'ern@@', 'ens', 'ist', 'das', 'ern@@', 's@@', 'th@@', 'ick@@', 'te', 'Problem@@', ',', 'weil', 'es', 'nicht', 'der', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is@@', 'b@@', 'ah@@', 'n', 'des', 'E@@', 'is@@', 's.', '</s>']
2024-05-23 23:59:51,008 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-23 23:59:51,008 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-23 23:59:51,008 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des ernens ist das ernsthickte Problem, weil es nicht der Dickheit der Eisbahn des Eiss.
2024-05-23 23:59:51,008 - INFO - joeynmt.training - Example #2
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-23 23:59:51,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'et@@', 'ische', 'E@@', 'is@@', 'k@@', 'ra@@', 'ft,', 'das', 'Sch@@', 'ei@@', 'ß@@', 'en@@', 'den@@', 'z', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'des', 'Klima@@', 'system', 'ist.', '</s>']
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisketische Eiskraft, das Scheißendenz des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem des Klimasystem ist.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - Example #3
2024-05-23 23:59:51,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-23 23:59:51,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-23 23:59:51,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'fen', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'fen', 'in', 'der', 'S@@', 'omm@@', 'er@@', 'fahr@@', 'en.', '</s>']
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Hypothesis: Es erweitert im Sommer und Kontrafen im Sommer und Kontrafen in der Sommerfahren.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - Example #4
2024-05-23 23:59:51,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-23 23:59:51,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-23 23:59:51,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'wird,', 'wird', 'das', 'eine', 'R@@', 'i@@', 'elle', 'des', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', '-@@', 'Wal@@', 'd', 'der', 'passier@@', 'te.', '</s>']
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-23 23:59:51,009 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen wird, wird das eine Rielle des schnelles schnell-Wald der passierte.
2024-05-24 00:00:15,679 - INFO - joeynmt.training - Epoch   7, Step:    24100, Batch Loss:     1.756039, Batch Acc: 0.520455, Tokens per Sec:     3003, Lr: 0.000300
2024-05-24 00:00:38,776 - INFO - joeynmt.training - Epoch   7, Step:    24200, Batch Loss:     1.712738, Batch Acc: 0.519990, Tokens per Sec:     3180, Lr: 0.000300
2024-05-24 00:01:01,482 - INFO - joeynmt.training - Epoch   7, Step:    24300, Batch Loss:     1.613189, Batch Acc: 0.509472, Tokens per Sec:     3139, Lr: 0.000300
2024-05-24 00:01:25,261 - INFO - joeynmt.training - Epoch   7, Step:    24400, Batch Loss:     1.814199, Batch Acc: 0.515063, Tokens per Sec:     3194, Lr: 0.000300
2024-05-24 00:01:48,453 - INFO - joeynmt.training - Epoch   7, Step:    24500, Batch Loss:     1.715196, Batch Acc: 0.512848, Tokens per Sec:     3182, Lr: 0.000300
2024-05-24 00:01:48,454 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:01:48,454 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:03:02,370 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.62, acc:   0.48, generation: 73.9088[sec], evaluation: 0.0000[sec]
2024-05-24 00:03:02,372 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:03:02,717 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/22000.ckpt
2024-05-24 00:03:02,928 - INFO - joeynmt.training - Example #0
2024-05-24 00:03:02,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:03:02,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:03:02,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'E@@', 'is', 'im', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'gew@@', 'esen', 'ist,', 'die', 'meisten', 'der', 'n@@', 'ie@@', 'dri@@', 'gen', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'die', 'meisten', 'n@@', 'ie@@', 'dri@@', 'gen', '4@@', '8', 'Sta@@', 'aten', 'und', 'S@@', 'hr@@', 'un@@', 'k@@', 'ün@@', 'dig@@', 'en.', '</s>']
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die arktische Eis Eis im letzten drei Millionen Jahre der letzten drei Millionen Jahre gewesen ist, die meisten der niedrigen 48 Staaten, die die meisten niedrigen 48 Staaten und Shrunkündigen.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - Example #1
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'der', 'ern@@', 'sten', 'S@@', 'eri@@', 'en@@', 'heit', 'dieser', 'spezi@@', 'elle', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'der', 'E@@', 'is@@', 'se.', '</s>']
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied der ernsten Serienheit dieser spezielle Probleme, weil es nicht die Dicke der Eis der Eis der Eis der Eis der Eis der Eisse.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - Example #2
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'im', 'Sin@@', 'ne', 'der', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist im Sinne der globalen Klimasystem.
2024-05-24 00:03:02,929 - INFO - joeynmt.training - Example #3
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:03:02,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'W@@', 'ind', 'und', 'das', 'Sch@@', 'mer@@', '.', '</s>']
2024-05-24 00:03:02,929 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:03:02,930 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:03:02,930 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Wind und das Schmer.
2024-05-24 00:03:02,930 - INFO - joeynmt.training - Example #4
2024-05-24 00:03:02,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:03:02,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:03:02,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigen', 'die', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'dass', 'Sie', 'ein', 'R@@', 'is@@', 'ik@@', 'o', 'ver@@', 'k@@', 'ehr', 'sein', 'wird,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ah', 'ist.', '</s>']
2024-05-24 00:03:02,930 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:03:02,930 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:03:02,930 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigen die ich Ihnen zeigen werde, dass Sie ein Risiko verkehr sein wird, was in den letzten 25 Jahren geschah ist.
2024-05-24 00:03:26,960 - INFO - joeynmt.training - Epoch   7, Step:    24600, Batch Loss:     1.716862, Batch Acc: 0.517135, Tokens per Sec:     3043, Lr: 0.000300
2024-05-24 00:03:50,327 - INFO - joeynmt.training - Epoch   7, Step:    24700, Batch Loss:     1.795798, Batch Acc: 0.513070, Tokens per Sec:     3060, Lr: 0.000300
2024-05-24 00:04:13,862 - INFO - joeynmt.training - Epoch   7, Step:    24800, Batch Loss:     1.673563, Batch Acc: 0.513322, Tokens per Sec:     3143, Lr: 0.000300
2024-05-24 00:04:36,531 - INFO - joeynmt.training - Epoch   7, Step:    24900, Batch Loss:     1.766695, Batch Acc: 0.513071, Tokens per Sec:     3186, Lr: 0.000300
2024-05-24 00:04:59,274 - INFO - joeynmt.training - Epoch   7, Step:    25000, Batch Loss:     1.593991, Batch Acc: 0.508095, Tokens per Sec:     3205, Lr: 0.000300
2024-05-24 00:04:59,275 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:04:59,275 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:06:20,237 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.58, acc:   0.48, generation: 80.9535[sec], evaluation: 0.0000[sec]
2024-05-24 00:06:20,238 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:06:20,634 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/22500.ckpt
2024-05-24 00:06:20,818 - INFO - joeynmt.training - Example #0
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'en@@', 'ster@@', 'schaf@@', 'ten', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'hat', 'die', 'S@@', 'hr@@', 'un@@', 'k@@', 'ün@@', 'dig@@', 'en.', '</s>']
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Eis der letzten drei Millionen Jahre lang die Größe der letzten drei Millionen Jahre lang die Größensterschaften 48 Staaten, hat die Shrunkündigen.
2024-05-24 00:06:20,819 - INFO - joeynmt.training - Example #1
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'sta@@', 'at@@', 'lichen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'sionen', 'der', 'E@@', 'is@@', 'sionen', 'zeigen.', '</s>']
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstaatlichen Problem, weil es nicht die Dicke des Eissionen der Eissionen zeigen.
2024-05-24 00:06:20,819 - INFO - joeynmt.training - Example #2
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:06:20,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'c@@', 'a@@', 'p', 'ist,', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:06:20,819 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:06:20,820 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiscap ist, in einem Sinn des globalen Klimasystem.
2024-05-24 00:06:20,820 - INFO - joeynmt.training - Example #3
2024-05-24 00:06:20,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:06:20,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:06:20,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'ger@@', 'ung', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'ger@@', 'ung', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-24 00:06:20,820 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:06:20,820 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:06:20,820 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verfolge in Sommer und Verfolgerung in der Sommer und Kontrakte in der Sommer und Kontrakte in der Sommer und Verfolgerung in Sommer und Kontrakte in den Sommer und Kontrag in den Sommer und Kontrakte in der Sommer.
2024-05-24 00:06:20,820 - INFO - joeynmt.training - Example #4
2024-05-24 00:06:20,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:06:20,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:06:20,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'eller', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-24 00:06:20,820 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:06:20,820 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:06:20,820 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass Sie ein schnelles schneller in den letzten 25 Jahren passiert.
2024-05-24 00:06:45,211 - INFO - joeynmt.training - Epoch   7, Step:    25100, Batch Loss:     1.555879, Batch Acc: 0.515587, Tokens per Sec:     2947, Lr: 0.000300
2024-05-24 00:07:09,287 - INFO - joeynmt.training - Epoch   7, Step:    25200, Batch Loss:     1.706369, Batch Acc: 0.512369, Tokens per Sec:     3021, Lr: 0.000300
2024-05-24 00:07:33,720 - INFO - joeynmt.training - Epoch   7, Step:    25300, Batch Loss:     1.732000, Batch Acc: 0.515723, Tokens per Sec:     3060, Lr: 0.000300
2024-05-24 00:07:56,574 - INFO - joeynmt.training - Epoch   7, Step:    25400, Batch Loss:     1.676942, Batch Acc: 0.509830, Tokens per Sec:     3196, Lr: 0.000300
2024-05-24 00:08:19,724 - INFO - joeynmt.training - Epoch   7, Step:    25500, Batch Loss:     1.758545, Batch Acc: 0.509688, Tokens per Sec:     3197, Lr: 0.000300
2024-05-24 00:08:19,725 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:08:19,725 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:09:43,676 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.57, acc:   0.48, generation: 83.9438[sec], evaluation: 0.0000[sec]
2024-05-24 00:09:43,677 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:09:43,971 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/24000.ckpt
2024-05-24 00:09:44,121 - INFO - joeynmt.training - Example #0
2024-05-24 00:09:44,121 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:09:44,121 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:09:44,121 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'und', 'S@@', 'hr@@', 'un@@', 'k@@', 'un@@', 'sch@@', 'ar@@', 'fen', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:09:44,121 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:09:44,121 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:09:44,121 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Eis für die meisten der letzten drei Millionen Jahre lang die Größe 48 Staaten die Größe 48 Staaten und Shrunkunscharfen von 40 Prozent.
2024-05-24 00:09:44,121 - INFO - joeynmt.training - Example #1
2024-05-24 00:09:44,121 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:09:44,121 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:09:44,121 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'äd@@', 'te', 'der', 'S@@', 'eri@@', 'en@@', 'heit', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'des', 'I@@', 'kon@@', 's', 'zeigen.', '</s>']
2024-05-24 00:09:44,121 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:09:44,121 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:09:44,121 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstädte der Serienheit dieses Problem ist, weil es nicht die Dickheit des Ikons zeigen.
2024-05-24 00:09:44,121 - INFO - joeynmt.training - Example #2
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'en@@', 'r@@', 'ung', 'ist', 'in', 'einem', 'Sinn@@', 'e,', 'das', 'G@@', 'lob@@', 'us', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Hypothesis: Die arktische Eiszenrung ist in einem Sinne, das Globus des Klimasystems des Klimasystems
2024-05-24 00:09:44,122 - INFO - joeynmt.training - Example #3
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'en.', '</s>']
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich in Winter und Kontrakte im Sommer und Kontraken.
2024-05-24 00:09:44,122 - INFO - joeynmt.training - Example #4
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:09:44,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fo@@', 'li@@', 'en', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', '-@@', 'Vor@@', 'wär@@', 'ts', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'wird.', '</s>']
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:09:44,122 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folien zeige ich Ihnen ein schnelles schnell-Vorwärts von dem, was in den letzten 25 Jahren geschehen wird.
2024-05-24 00:10:07,441 - INFO - joeynmt.training - Epoch   7, Step:    25600, Batch Loss:     1.792461, Batch Acc: 0.513878, Tokens per Sec:     3085, Lr: 0.000300
2024-05-24 00:10:31,756 - INFO - joeynmt.training - Epoch   7, Step:    25700, Batch Loss:     1.949138, Batch Acc: 0.514544, Tokens per Sec:     2972, Lr: 0.000300
2024-05-24 00:10:54,429 - INFO - joeynmt.training - Epoch   7, Step:    25800, Batch Loss:     1.785048, Batch Acc: 0.513909, Tokens per Sec:     3285, Lr: 0.000300
2024-05-24 00:11:18,882 - INFO - joeynmt.training - Epoch   7, Step:    25900, Batch Loss:     1.827141, Batch Acc: 0.512169, Tokens per Sec:     2991, Lr: 0.000300
2024-05-24 00:11:41,957 - INFO - joeynmt.training - Epoch   7, Step:    26000, Batch Loss:     1.688236, Batch Acc: 0.509084, Tokens per Sec:     3280, Lr: 0.000300
2024-05-24 00:11:41,959 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:11:41,959 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:13:02,044 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.50, acc:   0.48, generation: 80.0773[sec], evaluation: 0.0000[sec]
2024-05-24 00:13:02,046 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:13:02,387 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/23500.ckpt
2024-05-24 00:13:02,596 - INFO - joeynmt.training - Example #0
2024-05-24 00:13:02,596 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:13:02,596 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:13:02,596 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as@@', 'se', 'ge@@', 'zeigt', 'hat,', 'dass', 'das', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ge@@', 'zeigt,', 'dass', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'die', 'meisten', 'meisten', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'gew@@', 'esen', 'wurde,', 'hat', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Prozent', 'ges@@', 'tor@@', 'ben', 'ist.', '</s>']
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Diasse gezeigt hat, dass das arktische Eis gezeigt, dass die meisten drei Millionen Jahre die meisten meisten unteren 48 Staaten die Größe 48 Staaten gewesen wurde, hat Shrunk von 40 Prozent gestorben ist.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - Example #1
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'Jahr@@', 'es', 'ist', 'das', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'den', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 'en@@', 's.', '</s>']
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des Jahres ist das ernsthaft dieses Problem ist, weil es nicht den Dicksal des Eisens.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - Example #2
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', ',', 'das', 'G@@', 'lob@@', 'al', 'des', 'Klima@@', 'es', 'im', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eis ist in einem Sinn, das Global des Klimaes im globalen Klimasystem.
2024-05-24 00:13:02,597 - INFO - joeynmt.training - Example #3
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:13:02,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:13:02,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'in', 'Win@@', 'ter', 'und', 'B@@', 'est@@', 'and@@', 't@@', 'eile', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 00:13:02,598 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:13:02,598 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:13:02,598 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich in Winter und Bestandteile im Sommer
2024-05-24 00:13:02,598 - INFO - joeynmt.training - Example #4
2024-05-24 00:13:02,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:13:02,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:13:02,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'zeigen,', 'dass', 'Sie', 'ein', 'schnell', 'schnell', 'vor@@', 'wär@@', 'ts', 'des', 'letzten', '25', 'Jahre', 'passier@@', 't.', '</s>']
2024-05-24 00:13:02,598 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:13:02,598 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:13:02,598 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen zeigen, dass Sie ein schnell schnell vorwärts des letzten 25 Jahre passiert.
2024-05-24 00:13:27,071 - INFO - joeynmt.training - Epoch   7, Step:    26100, Batch Loss:     1.977694, Batch Acc: 0.507048, Tokens per Sec:     2923, Lr: 0.000300
2024-05-24 00:13:50,127 - INFO - joeynmt.training - Epoch   7, Step:    26200, Batch Loss:     1.904965, Batch Acc: 0.509546, Tokens per Sec:     3131, Lr: 0.000300
2024-05-24 00:14:12,757 - INFO - joeynmt.training - Epoch   7, Step:    26300, Batch Loss:     1.824819, Batch Acc: 0.508671, Tokens per Sec:     3279, Lr: 0.000300
2024-05-24 00:14:35,648 - INFO - joeynmt.training - Epoch   7, Step:    26400, Batch Loss:     1.826045, Batch Acc: 0.511621, Tokens per Sec:     3289, Lr: 0.000300
2024-05-24 00:14:58,602 - INFO - joeynmt.training - Epoch   7, Step:    26500, Batch Loss:     1.870136, Batch Acc: 0.510466, Tokens per Sec:     3207, Lr: 0.000300
2024-05-24 00:14:58,603 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:14:58,604 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:16:24,323 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.44, acc:   0.48, generation: 85.7117[sec], evaluation: 0.0000[sec]
2024-05-24 00:16:24,323 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:16:24,728 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/23000.ckpt
2024-05-24 00:16:24,875 - INFO - joeynmt.training - Example #0
2024-05-24 00:16:24,875 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ge@@', 'zeigt', 'hat,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'gew@@', 'esen', 'hat,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'ges@@', 'tor@@', 'ben', 'ist.', '</s>']
2024-05-24 00:16:24,876 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:16:24,876 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:16:24,876 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt dass die arktische Eis gezeigt hat, dass die arktische Eis der letzten drei Millionen Jahre gewesen hat, die Größe 48 Staaten gestorben ist.
2024-05-24 00:16:24,876 - INFO - joeynmt.training - Example #1
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'sta@@', 'un@@', 'en', 'des', 'Problem@@', 's', 'dieser', 'bestimm@@', 'ten', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'I@@', 's.', '</s>']
2024-05-24 00:16:24,876 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:16:24,876 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:16:24,876 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstaunen des Problems dieser bestimmten Problem ist, weil es nicht die Dicksal des Is.
2024-05-24 00:16:24,876 - INFO - joeynmt.training - Example #2
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:16:24,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'm@@', 'utz@@', 'ung', 'ist', 'im', 'Sin@@', 'ne', 'des', 'Klima@@', 'wandel@@', 's', 'des', 'Klima@@', 'wandel@@', 's', '</s>']
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisschmutzung ist im Sinne des Klimawandels des Klimawandels
2024-05-24 00:16:24,877 - INFO - joeynmt.training - Example #3
2024-05-24 00:16:24,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:16:24,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:16:24,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'g', 'im', 'S@@', 'omm@@', 'er', 'und', 'die', 'sich', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'g', 'im', 'S@@', 'omm@@', 'er', 'w@@', 'äch@@', 'st.', '</s>']
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich im Sommer und Vertrag im Sommer und die sich im Sommer und Kontrag im Sommer und Kontrag im Sommer und Kontrag im Sommer und Kontrag im Sommer wächst.
2024-05-24 00:16:24,877 - INFO - joeynmt.training - Example #4
2024-05-24 00:16:24,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:16:24,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:16:24,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'li@@', 'en', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'Auf@@', 'wär@@', 'ts', 'der', 'letzten', '25', 'Jahre', 'passier@@', 't.', '</s>']
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:16:24,877 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folien zeige ich Ihnen ein schnelles schnelles schnelles Aufwärts der letzten 25 Jahre passiert.
2024-05-24 00:16:49,094 - INFO - joeynmt.training - Epoch   7, Step:    26600, Batch Loss:     1.760628, Batch Acc: 0.508550, Tokens per Sec:     2987, Lr: 0.000300
2024-05-24 00:17:12,143 - INFO - joeynmt.training - Epoch   7, Step:    26700, Batch Loss:     1.776441, Batch Acc: 0.514764, Tokens per Sec:     3231, Lr: 0.000300
2024-05-24 00:17:35,819 - INFO - joeynmt.training - Epoch   7, Step:    26800, Batch Loss:     1.709247, Batch Acc: 0.508264, Tokens per Sec:     3189, Lr: 0.000300
2024-05-24 00:17:58,165 - INFO - joeynmt.training - Epoch   7, Step:    26900, Batch Loss:     1.775907, Batch Acc: 0.512254, Tokens per Sec:     3258, Lr: 0.000300
2024-05-24 00:18:21,119 - INFO - joeynmt.training - Epoch   7, Step:    27000, Batch Loss:     1.841271, Batch Acc: 0.510515, Tokens per Sec:     3275, Lr: 0.000300
2024-05-24 00:18:21,120 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:18:21,120 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:19:41,649 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.49, generation: 80.5223[sec], evaluation: 0.0000[sec]
2024-05-24 00:19:41,651 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:19:41,979 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/24500.ckpt
2024-05-24 00:19:42,140 - INFO - joeynmt.training - Example #0
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'dass', 'die', 'Ar@@', 'kt@@', 'isch@@', 'e,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'der', 'Größ@@', 'e', '4@@', '8', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'auf@@', 'ge@@', 'zeich@@', 'net', 'wird.', '</s>']
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt dass die Arktische, dass die arktische Eis der Größe 48 Millionen Jahre die Größe 48 Staaten die Größe 48 Staaten aufgezeichnet wird.
2024-05-24 00:19:42,141 - INFO - joeynmt.training - Example #1
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'An@@', 'nah@@', 'me', 'dieses', 'bestimm@@', 'te', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's', 'zeigen.', '</s>']
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser Annahme dieses bestimmte Problem ist, weil es nicht die Dicksal des Eiss zeigen.
2024-05-24 00:19:42,141 - INFO - joeynmt.training - Example #2
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:19:42,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', ',', 'in', 'einem', 'Sinn@@', ',', 'das', 'B@@', 'ier', 'des', 'Klima@@', 'wandel@@', 's', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:19:42,141 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:19:42,142 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eissch, in einem Sinn, das Bier des Klimawandels des globalen Klimasystem.
2024-05-24 00:19:42,142 - INFO - joeynmt.training - Example #3
2024-05-24 00:19:42,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:19:42,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:19:42,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 00:19:42,142 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:19:42,142 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:19:42,142 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte in Sommer und Kontrakte in Sommer
2024-05-24 00:19:42,142 - INFO - joeynmt.training - Example #4
2024-05-24 00:19:42,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:19:42,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:19:42,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'schnell', 'schnell', 'schnell', 'zu', 'sein,', 'was', 'über', 'die', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-24 00:19:42,142 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:19:42,142 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:19:42,142 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie zeige zeige ich Ihnen eine schnell schnell schnell zu sein, was über die letzten 25 Jahren geschehen ist.
2024-05-24 00:20:06,171 - INFO - joeynmt.training - Epoch   7, Step:    27100, Batch Loss:     1.627587, Batch Acc: 0.511500, Tokens per Sec:     2958, Lr: 0.000300
2024-05-24 00:20:15,981 - INFO - joeynmt.training - Epoch   7: total training loss 6758.86
2024-05-24 00:20:15,981 - INFO - joeynmt.training - EPOCH 8
2024-05-24 00:20:29,334 - INFO - joeynmt.training - Epoch   8, Step:    27200, Batch Loss:     1.466903, Batch Acc: 0.545500, Tokens per Sec:     3308, Lr: 0.000300
2024-05-24 00:20:52,025 - INFO - joeynmt.training - Epoch   8, Step:    27300, Batch Loss:     1.586394, Batch Acc: 0.533381, Tokens per Sec:     3255, Lr: 0.000300
2024-05-24 00:21:16,023 - INFO - joeynmt.training - Epoch   8, Step:    27400, Batch Loss:     1.713397, Batch Acc: 0.540857, Tokens per Sec:     3068, Lr: 0.000300
2024-05-24 00:21:39,207 - INFO - joeynmt.training - Epoch   8, Step:    27500, Batch Loss:     1.722357, Batch Acc: 0.533946, Tokens per Sec:     3184, Lr: 0.000300
2024-05-24 00:21:39,208 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:21:39,208 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:23:00,768 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.46, acc:   0.49, generation: 81.5516[sec], evaluation: 0.0000[sec]
2024-05-24 00:23:01,119 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/25000.ckpt
2024-05-24 00:23:01,319 - INFO - joeynmt.training - Example #0
2024-05-24 00:23:01,319 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:23:01,319 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:23:01,319 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'a', 'Di@@', 'a', 'ge@@', 'zeigt', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'er', 'in', 'den', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'der', 'Lo@@', 'wer', '4@@', '8', 'Sta@@', 'aten', 'in', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:23:01,319 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:23:01,319 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:23:01,319 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dia Dia gezeigt dass die arktischen Eiser in den letzten drei Millionen Jahre lang die letzten drei Millionen Jahre lang der Lower 48 Staaten in der Größe 48 Staaten, Shrunk von 40 Prozent.
2024-05-24 00:23:01,319 - INFO - joeynmt.training - Example #1
2024-05-24 00:23:01,319 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:23:01,319 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:23:01,319 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'spezi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'en@@', 'heit', 'der', 'E@@', 'is@@', 'en@@', 'b@@', 'ah@@', 'n', 'zeig@@', 't.', '</s>']
2024-05-24 00:23:01,319 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:23:01,319 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:23:01,319 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied ist die ernsthaft dieses speziellen Problem, weil es nicht die Dickenheit der Eisenbahn zeigt.
2024-05-24 00:23:01,319 - INFO - joeynmt.training - Example #2
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'et@@', 'e,', 'die', 'schl@@', 'ä@@', 'gt', 'im', 'Sin@@', 'ne', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiszete, die schlägt im Sinne des Klimasystems
2024-05-24 00:23:01,320 - INFO - joeynmt.training - Example #3
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Sie', 'w@@', 'äch@@', 'st', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'halten', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'halten', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'g@@', 'ern.']
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Hypothesis: Sie wächst und Verträge in Sommer und Verträge in Sommer und Verträge in Sommer und Verträge in Sommer und Verträge in Sommer und Verträge in Sommer und Verträge in Sommer und Verträge in den Sommer und Verhalten in den Sommer und Verhalten in den Sommer und Verträgern.
2024-05-24 00:23:01,320 - INFO - joeynmt.training - Example #4
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:23:01,320 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Di@@', 'a', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'R@@', 'i@@', 'de', 'der', 'Zeit', 'der', 'letzten', '25', 'Jahre', 'lang@@', 'sam', 'ist,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:23:01,320 - INFO - joeynmt.training - 	Hypothesis: Der nächste Dia zeige ich Ihnen eine Ride der Zeit der letzten 25 Jahre langsam ist, was in den letzten 25 Jahren passiert ist.
2024-05-24 00:23:25,468 - INFO - joeynmt.training - Epoch   8, Step:    27600, Batch Loss:     1.595685, Batch Acc: 0.532725, Tokens per Sec:     2989, Lr: 0.000300
2024-05-24 00:23:49,103 - INFO - joeynmt.training - Epoch   8, Step:    27700, Batch Loss:     1.390180, Batch Acc: 0.531116, Tokens per Sec:     3070, Lr: 0.000300
2024-05-24 00:24:13,401 - INFO - joeynmt.training - Epoch   8, Step:    27800, Batch Loss:     1.745440, Batch Acc: 0.530883, Tokens per Sec:     3063, Lr: 0.000300
2024-05-24 00:24:39,459 - INFO - joeynmt.training - Epoch   8, Step:    27900, Batch Loss:     1.712081, Batch Acc: 0.531895, Tokens per Sec:     2798, Lr: 0.000300
2024-05-24 00:25:03,028 - INFO - joeynmt.training - Epoch   8, Step:    28000, Batch Loss:     1.634532, Batch Acc: 0.531115, Tokens per Sec:     3102, Lr: 0.000300
2024-05-24 00:25:03,029 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:25:03,029 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:26:13,174 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.44, acc:   0.49, generation: 70.1359[sec], evaluation: 0.0000[sec]
2024-05-24 00:26:13,599 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/25500.ckpt
2024-05-24 00:26:13,739 - INFO - joeynmt.training - Example #0
2024-05-24 00:26:13,739 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:26:13,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:26:13,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Di@@', 'as', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'er', 'Kap@@', ',', 'die', 'für', 'meisten', 'meisten', 'drei', 'Millionen', 'Jahre', 'gew@@', 'esen', 'hat,', 'die', 'meisten', 'von', '40', 'Millionen', 'Jahre', 'gew@@', 'esen', 'hat,', 'die', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Dias gezeigt habe, dass die arktische Eiser Kap, die für meisten meisten drei Millionen Jahre gewesen hat, die meisten von 40 Millionen Jahre gewesen hat, die unteren 48 Staaten, Shrunk von 40 Prozent.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - Example #1
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'äd@@', 't@@', 'ische', 'S@@', 'er@@', 'ie', 'dieses', 'spezi@@', 'elle', 'Problem@@', ',', 'denn', 'es', 'zeigt', 'die', 'D@@', 'ic@@', 'e.', '</s>']
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstädtische Serie dieses spezielle Problem, denn es zeigt die Dice.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - Example #2
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'et@@', 'e,', 'in', 'einem', 'Sin@@', 'ne', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:26:13,740 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiszete, in einem Sinne des Klimasystems des Klimasystems
2024-05-24 00:26:13,740 - INFO - joeynmt.training - Example #3
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:26:13,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:26:13,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'ä@@', 'he', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 00:26:13,741 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:26:13,741 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:26:13,741 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verfolgsähe im Sommer und Kontrakte im Sommer
2024-05-24 00:26:13,741 - INFO - joeynmt.training - Example #4
2024-05-24 00:26:13,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:26:13,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:26:13,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'wird.', '</s>']
2024-05-24 00:26:13,741 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:26:13,741 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:26:13,741 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen ein schnelles schnelles schnelles in den letzten 25 Jahren geschehen wird.
2024-05-24 00:26:36,773 - INFO - joeynmt.training - Epoch   8, Step:    28100, Batch Loss:     1.720343, Batch Acc: 0.532248, Tokens per Sec:     3074, Lr: 0.000300
2024-05-24 00:26:58,358 - INFO - joeynmt.training - Epoch   8, Step:    28200, Batch Loss:     1.783211, Batch Acc: 0.527009, Tokens per Sec:     3396, Lr: 0.000300
2024-05-24 00:27:20,629 - INFO - joeynmt.training - Epoch   8, Step:    28300, Batch Loss:     1.732331, Batch Acc: 0.529659, Tokens per Sec:     3416, Lr: 0.000300
2024-05-24 00:27:42,421 - INFO - joeynmt.training - Epoch   8, Step:    28400, Batch Loss:     1.693560, Batch Acc: 0.531599, Tokens per Sec:     3388, Lr: 0.000300
2024-05-24 00:28:04,383 - INFO - joeynmt.training - Epoch   8, Step:    28500, Batch Loss:     1.733812, Batch Acc: 0.524866, Tokens per Sec:     3384, Lr: 0.000300
2024-05-24 00:28:04,384 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:28:04,385 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:29:25,582 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.41, acc:   0.49, generation: 81.1889[sec], evaluation: 0.0000[sec]
2024-05-24 00:29:25,892 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/26000.ckpt
2024-05-24 00:29:26,035 - INFO - joeynmt.training - Example #0
2024-05-24 00:29:26,035 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:29:26,035 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:29:26,035 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'ich', 'habe', 'diese', 'zwei', 'Di@@', 'a', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'ä@@', 'ub@@', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'di@@', 'en@@', 'st', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:29:26,035 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:29:26,035 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:29:26,035 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr ich habe diese zwei Dia gezeigt habe, dass die arktische Eisschäub, die für die meisten drei Millionen Jahre lang die Größe 48 Stadienst 48 Staaten, die Shrunk von 40 Prozent.
2024-05-24 00:29:26,035 - INFO - joeynmt.training - Example #1
2024-05-24 00:29:26,035 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:29:26,035 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:29:26,035 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'äd@@', 'te', 'die', 'ern@@', 'ste', 'dieser', 'spezi@@', 'ellen', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is@@', 'en@@', 'b@@', 'ah@@', 'n@@', 'b@@', 'rech@@', 't.', '</s>']
2024-05-24 00:29:26,035 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:29:26,035 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:29:26,035 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstädte die ernste dieser speziellen Probleme, weil es nicht die Dickheit der Eisenbahnbrecht.
2024-05-24 00:29:26,035 - INFO - joeynmt.training - Example #2
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'isch@@', 'e,', 'E@@', 'is@@', 'z@@', 'et@@', 'e,', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'glob@@', 'alen', 'Klima@@', 'system', 'des', 'Klima@@', 'system', '</s>']
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Hypothesis: Der Arktische, Eiszete, in einem Sinn des globalen Klimasystem des globalen Klimasystem des Klimasystem
2024-05-24 00:29:26,036 - INFO - joeynmt.training - Example #3
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'aus.', '</s>']
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Kontrakte im Sommer und Kontrakte im Sommer und Kontrakte im Sommer und Kontrakte im Sommer und Kontrakte im Sommer aus.
2024-05-24 00:29:26,036 - INFO - joeynmt.training - Example #4
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:29:26,036 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Di@@', 'a', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'schnell', 'schnell', 'schnell', 'zu', 'ver@@', 'st@@', 'är@@', 'kt', 'ist,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:29:26,036 - INFO - joeynmt.training - 	Hypothesis: Der nächste Dia zeige zeige ich Ihnen ein schnell schnell schnell zu verstärkt ist, was in den letzten 25 Jahren passiert.
2024-05-24 00:29:50,153 - INFO - joeynmt.training - Epoch   8, Step:    28600, Batch Loss:     1.712190, Batch Acc: 0.528949, Tokens per Sec:     3024, Lr: 0.000300
2024-05-24 00:30:13,713 - INFO - joeynmt.training - Epoch   8, Step:    28700, Batch Loss:     1.566479, Batch Acc: 0.527338, Tokens per Sec:     3139, Lr: 0.000300
2024-05-24 00:30:35,831 - INFO - joeynmt.training - Epoch   8, Step:    28800, Batch Loss:     1.728372, Batch Acc: 0.520195, Tokens per Sec:     3360, Lr: 0.000300
2024-05-24 00:30:58,164 - INFO - joeynmt.training - Epoch   8, Step:    28900, Batch Loss:     1.891472, Batch Acc: 0.524011, Tokens per Sec:     3265, Lr: 0.000300
2024-05-24 00:31:20,800 - INFO - joeynmt.training - Epoch   8, Step:    29000, Batch Loss:     1.672852, Batch Acc: 0.523853, Tokens per Sec:     3230, Lr: 0.000300
2024-05-24 00:31:20,801 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:31:20,801 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:32:31,357 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.48, generation: 70.5487[sec], evaluation: 0.0000[sec]
2024-05-24 00:32:31,359 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:32:31,702 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/27500.ckpt
2024-05-24 00:32:31,939 - INFO - joeynmt.training - Example #0
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'e,', 'die', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'e,', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'Sta@@', 'ats@@', 'b@@', 'ild', 'der', 'n@@', 'ie@@', 'dri@@', 'ger', 'Sta@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:32:31,939 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:32:31,939 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:32:31,939 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie so dass die arktische Eise, die die arktische Eise, die meisten der letzten drei Millionen Jahre der niedriger Staatsbild der niedriger Staaten, Shrunk von 40 Prozent.
2024-05-24 00:32:31,939 - INFO - joeynmt.training - Example #1
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'tr@@', 'äum@@', 'e', 'die', 'ern@@', 'sten', 'Probleme', 'dieses', 'spezi@@', 'ellen', 'Problem@@', ',', 'weil', 'es', 'nicht', 'das', 'D@@', 'ick@@', 'er@@', 'do@@', 'pp@@', 'elt', 'nicht', 'zeigen.', '</s>']
2024-05-24 00:32:31,939 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:32:31,939 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:32:31,939 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterträume die ernsten Probleme dieses speziellen Problem, weil es nicht das Dickerdoppelt nicht zeigen.
2024-05-24 00:32:31,939 - INFO - joeynmt.training - Example #2
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:32:31,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'e,', 'die', 'Bi@@', 'en@@', 'en@@', 'en@@', '"', 'im', 'Au@@', 'gen@@', 'bli@@', 'ck', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisze, die Bienenen" im Augenblick des Klimasystems des Klimasystems des Klimasystems des Klimasystems
2024-05-24 00:32:31,940 - INFO - joeynmt.training - Example #3
2024-05-24 00:32:31,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:32:31,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:32:31,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'el', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'en.', '</s>']
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich in Winter und Kontrakel im Sommer und Kontraken.
2024-05-24 00:32:31,940 - INFO - joeynmt.training - Example #4
2024-05-24 00:32:31,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:32:31,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:32:31,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'zei@@', 'ge', 'Ihnen', 'eine', 'schn@@', 'ell@@', '-@@', 'Wal@@', 'd', 'der', 'der', 'letzten', '25', 'Jahre', 'passier@@', 'te.', '</s>']
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:32:31,940 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige zeige Ihnen eine schnell-Wald der der letzten 25 Jahre passierte.
2024-05-24 00:32:55,114 - INFO - joeynmt.training - Epoch   8, Step:    29100, Batch Loss:     1.770397, Batch Acc: 0.524557, Tokens per Sec:     3107, Lr: 0.000300
2024-05-24 00:33:17,795 - INFO - joeynmt.training - Epoch   8, Step:    29200, Batch Loss:     1.619918, Batch Acc: 0.526975, Tokens per Sec:     3284, Lr: 0.000300
2024-05-24 00:33:39,846 - INFO - joeynmt.training - Epoch   8, Step:    29300, Batch Loss:     1.632740, Batch Acc: 0.518169, Tokens per Sec:     3343, Lr: 0.000300
2024-05-24 00:34:02,112 - INFO - joeynmt.training - Epoch   8, Step:    29400, Batch Loss:     1.555040, Batch Acc: 0.521924, Tokens per Sec:     3311, Lr: 0.000300
2024-05-24 00:34:24,692 - INFO - joeynmt.training - Epoch   8, Step:    29500, Batch Loss:     1.517251, Batch Acc: 0.526160, Tokens per Sec:     3248, Lr: 0.000300
2024-05-24 00:34:24,693 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:34:24,693 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:35:44,486 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.35, acc:   0.49, generation: 79.7849[sec], evaluation: 0.0000[sec]
2024-05-24 00:35:44,488 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:35:44,821 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/28000.ckpt
2024-05-24 00:35:45,029 - INFO - joeynmt.training - Example #0
2024-05-24 00:35:45,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:35:45,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:35:45,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'von', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'ver@@', 'lang@@', 'sam@@', 'er', 'St@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:35:45,029 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:35:45,029 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:35:45,029 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die arktischen Eis der letzten drei Millionen Jahre die Größe der letzten drei Millionen Jahre die Größe von unteren 48 Staaten verlangsamer Stunk von 40 Prozent.
2024-05-24 00:35:45,029 - INFO - joeynmt.training - Example #1
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'spezi@@', 'ellen', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'des', 'E@@', 'is', 'zeigen.', '</s>']
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser speziellen Probleme, weil es nicht die Dickheit des Eis zeigen.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - Example #2
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'in', 'einem', 'Sinn@@', ',', 'das', 'den', 'Sch@@', 'ei@@', 't,', 'das', 'den', 'Herz@@', 'ens', 'des', 'glob@@', 'alen', 'Klima@@', 'system.', '</s>']
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist in einem Sinn, das den Scheit, das den Herzens des globalen Klimasystem.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - Example #3
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'w@@', 'äch@@', 'st', 'und', 'das', 'ist', 'im', 'S@@', 'omm@@', 'er@@', 'f@@', 'ähr@@', 't', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - 	Hypothesis: Es wächst und das ist im Sommerfährt und Kontrakte im Sommer.
2024-05-24 00:35:45,030 - INFO - joeynmt.training - Example #4
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:35:45,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:35:45,031 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'schnell', 'schnell', 'ver@@', 'gew@@', 'al@@', 'tigt', 'werden', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-24 00:35:45,031 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:35:45,031 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:35:45,031 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie zeige zeige ich Ihnen eine schnell schnell vergewaltigt werden von dem, was in den letzten 25 Jahren passiert.
2024-05-24 00:36:08,386 - INFO - joeynmt.training - Epoch   8, Step:    29600, Batch Loss:     1.510306, Batch Acc: 0.522357, Tokens per Sec:     3102, Lr: 0.000300
2024-05-24 00:36:32,091 - INFO - joeynmt.training - Epoch   8, Step:    29700, Batch Loss:     1.580800, Batch Acc: 0.522509, Tokens per Sec:     3096, Lr: 0.000300
2024-05-24 00:36:55,358 - INFO - joeynmt.training - Epoch   8, Step:    29800, Batch Loss:     1.649818, Batch Acc: 0.523730, Tokens per Sec:     3162, Lr: 0.000300
2024-05-24 00:37:19,712 - INFO - joeynmt.training - Epoch   8, Step:    29900, Batch Loss:     1.811881, Batch Acc: 0.523580, Tokens per Sec:     3027, Lr: 0.000300
2024-05-24 00:37:42,573 - INFO - joeynmt.training - Epoch   8, Step:    30000, Batch Loss:     1.603367, Batch Acc: 0.525699, Tokens per Sec:     3324, Lr: 0.000300
2024-05-24 00:37:42,574 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:37:42,574 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:39:02,407 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.33, acc:   0.49, generation: 79.8259[sec], evaluation: 0.0000[sec]
2024-05-24 00:39:02,409 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:39:02,779 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/26500.ckpt
2024-05-24 00:39:03,007 - INFO - joeynmt.training - Example #0
2024-05-24 00:39:03,007 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:39:03,007 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:39:03,007 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'ge@@', 'zeigt,', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'und', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die arktischen Eis gezeigt, dass die arktischen Eis der letzten drei Millionen Jahre die Größe der untere 48 Staaten und Shrunk von 40 Prozent.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - Example #1
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'st@@', 'ün@@', 'de', 'des', 'A@@', 'spek@@', 'tive', 'Problem@@', 's', 'dieses', 'Problem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'zeigen.', '</s>']
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstünde des Aspektive Problems dieses Problem weil es nicht die Dickheit der Eis zeigen.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - Example #2
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'op@@', ',', 'in', 'einem', 'Sinn@@', 'e,', 'der', 'die', 'Herz@@', 'en.', '</s>']
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiskop, in einem Sinne, der die Herzen.
2024-05-24 00:39:03,008 - INFO - joeynmt.training - Example #3
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:39:03,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 00:39:03,009 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:39:03,009 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:39:03,009 - INFO - joeynmt.training - 	Hypothesis: Es erwartet in Winter und Kontrakte im Sommer und Kontrakte im Sommer
2024-05-24 00:39:03,009 - INFO - joeynmt.training - Example #4
2024-05-24 00:39:03,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:39:03,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:39:03,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fol@@', 'ie', 'zeigen', 'wird', 'ein', 'schn@@', 'eller', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'ver@@', 'st@@', 'är@@', 'kt', 'zu', 'sein,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-24 00:39:03,009 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:39:03,009 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:39:03,009 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie zeigen wird ein schneller schnelles schnelles verstärkt zu sein, was in den letzten 25 Jahren geschehen ist.
2024-05-24 00:39:26,415 - INFO - joeynmt.training - Epoch   8, Step:    30100, Batch Loss:     1.817737, Batch Acc: 0.523423, Tokens per Sec:     3041, Lr: 0.000300
2024-05-24 00:39:48,515 - INFO - joeynmt.training - Epoch   8, Step:    30200, Batch Loss:     1.578542, Batch Acc: 0.513244, Tokens per Sec:     3290, Lr: 0.000300
2024-05-24 00:40:11,420 - INFO - joeynmt.training - Epoch   8, Step:    30300, Batch Loss:     1.865212, Batch Acc: 0.523610, Tokens per Sec:     3179, Lr: 0.000300
2024-05-24 00:40:33,638 - INFO - joeynmt.training - Epoch   8, Step:    30400, Batch Loss:     1.541856, Batch Acc: 0.525894, Tokens per Sec:     3311, Lr: 0.000300
2024-05-24 00:40:56,420 - INFO - joeynmt.training - Epoch   8, Step:    30500, Batch Loss:     1.799791, Batch Acc: 0.520391, Tokens per Sec:     3227, Lr: 0.000300
2024-05-24 00:40:56,421 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:40:56,421 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:42:19,404 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.32, acc:   0.49, generation: 82.9753[sec], evaluation: 0.0000[sec]
2024-05-24 00:42:19,406 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:42:19,828 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/28500.ckpt
2024-05-24 00:42:19,974 - INFO - joeynmt.training - Example #0
2024-05-24 00:42:19,974 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:42:19,974 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:42:19,974 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'in', 'den', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:42:19,974 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:42:19,974 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:42:19,974 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Eis in den meisten letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe der unteren 48 Staaten, Shrunk von 40 Prozent.
2024-05-24 00:42:19,975 - INFO - joeynmt.training - Example #1
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'tr@@', 'auen', 'des', 'Er@@', 'kenn@@', 'en@@', 's,', 'weil', 'es', 'nicht', 'den', 'D@@', 'ick@@', 's@@', 'al', 'des', 'E@@', 'is@@', 's', 'zeigen.', '</s>']
2024-05-24 00:42:19,975 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:42:19,975 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:42:19,975 - INFO - joeynmt.training - 	Hypothesis: Aber diese Untertrauen des Erkennens, weil es nicht den Dicksal des Eiss zeigen.
2024-05-24 00:42:19,975 - INFO - joeynmt.training - Example #2
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'c@@', 'a@@', 'p', 'ist', 'in', 'einem', 'Sinn@@', 'e,', 'der', 'Sch@@', 'ei@@', 't,', 'der', 'Sch@@', 'ei@@', 'ter', 'des', 'Klima@@', 'system.', '</s>']
2024-05-24 00:42:19,975 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:42:19,975 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:42:19,975 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiscap ist in einem Sinne, der Scheit, der Scheiter des Klimasystem.
2024-05-24 00:42:19,975 - INFO - joeynmt.training - Example #3
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:42:19,975 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'die', 'W@@', 'en@@', '-', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'tion@@', 'en.', '</s>']
2024-05-24 00:42:19,976 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:42:19,976 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:42:19,976 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich die Wen- und Kontrakte im Sommer und Kontraktionen.
2024-05-24 00:42:19,976 - INFO - joeynmt.training - Example #4
2024-05-24 00:42:19,976 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:42:19,976 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:42:19,976 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zeigt', 'die', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'dass', 'wir', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-24 00:42:19,976 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:42:19,976 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:42:19,976 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeigt die ich Ihnen zeigen werde, dass wir in den letzten 25 Jahren passiert ist.
2024-05-24 00:42:43,419 - INFO - joeynmt.training - Epoch   8, Step:    30600, Batch Loss:     1.623677, Batch Acc: 0.516663, Tokens per Sec:     3048, Lr: 0.000300
2024-05-24 00:43:05,773 - INFO - joeynmt.training - Epoch   8, Step:    30700, Batch Loss:     1.672019, Batch Acc: 0.518784, Tokens per Sec:     3351, Lr: 0.000300
2024-05-24 00:43:28,930 - INFO - joeynmt.training - Epoch   8, Step:    30800, Batch Loss:     1.989218, Batch Acc: 0.521088, Tokens per Sec:     3179, Lr: 0.000300
2024-05-24 00:43:52,654 - INFO - joeynmt.training - Epoch   8, Step:    30900, Batch Loss:     1.608689, Batch Acc: 0.516615, Tokens per Sec:     3110, Lr: 0.000300
2024-05-24 00:44:16,009 - INFO - joeynmt.training - Epoch   8, Step:    31000, Batch Loss:     1.706174, Batch Acc: 0.524499, Tokens per Sec:     3207, Lr: 0.000300
2024-05-24 00:44:16,010 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:44:16,011 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:45:42,747 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.30, acc:   0.49, generation: 86.7288[sec], evaluation: 0.0000[sec]
2024-05-24 00:45:42,748 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 00:45:43,088 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/27000.ckpt
2024-05-24 00:45:43,233 - INFO - joeynmt.training - Example #0
2024-05-24 00:45:43,233 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:45:43,233 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:45:43,233 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ge@@', 'zeigt', 'hatte,', 'was', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'gen', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:45:43,233 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:45:43,233 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:45:43,233 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese beiden Folien gezeigt dass die arktische Eis gezeigt hatte, was für die meisten der letzten drei Millionen Jahre die Größe der niedrigen 48 Staaten die Größe 48 Staaten von 40 Prozent.
2024-05-24 00:45:43,233 - INFO - joeynmt.training - Example #1
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'spezi@@', 'ellen', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'des', 'E@@', 'is@@', 'es', 'zeigt', 'nicht', 'das', 'D@@', 'ick@@', 'er@@', 'heit', 'des', 'E@@', 'is@@', 'es', 'zeigen.', '</s>']
2024-05-24 00:45:43,234 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:45:43,234 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:45:43,234 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser speziellen Probleme, weil es nicht die Dickheit des Eises zeigt nicht das Dickerheit des Eises zeigen.
2024-05-24 00:45:43,234 - INFO - joeynmt.training - Example #2
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'la@@', 'g', 'ist', 'in', 'einem', 'Sin@@', 'ne', 'des', 'glob@@', 'alen', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 00:45:43,234 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:45:43,234 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:45:43,234 - INFO - joeynmt.training - 	Hypothesis: Der Arktische Eisschlag ist in einem Sinne des globalen Klimasystems des Klimasystems des Klimasystems
2024-05-24 00:45:43,234 - INFO - joeynmt.training - Example #3
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:45:43,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'aus@@', 'wei@@', 'ch@@', 'lich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 00:45:43,235 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:45:43,235 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:45:43,235 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich ausweichlich in Winter und Kontrakte im Sommer
2024-05-24 00:45:43,235 - INFO - joeynmt.training - Example #4
2024-05-24 00:45:43,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:45:43,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:45:43,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fo@@', 'li@@', 'e,', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'R@@', 'i@@', 'd', 'schn@@', 'ell@@', 'es', 'Wal@@', 'd@@', 'rei@@', 'ch', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-24 00:45:43,235 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:45:43,235 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:45:43,235 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie, zeige ich Ihnen ein Rid schnelles Waldreich in den letzten 25 Jahren passiert.
2024-05-24 00:45:46,995 - INFO - joeynmt.training - Epoch   8: total training loss 6550.45
2024-05-24 00:45:46,995 - INFO - joeynmt.training - EPOCH 9
2024-05-24 00:46:05,688 - INFO - joeynmt.training - Epoch   9, Step:    31100, Batch Loss:     1.509801, Batch Acc: 0.554284, Tokens per Sec:     3357, Lr: 0.000300
2024-05-24 00:46:28,056 - INFO - joeynmt.training - Epoch   9, Step:    31200, Batch Loss:     1.566327, Batch Acc: 0.548682, Tokens per Sec:     3408, Lr: 0.000300
2024-05-24 00:46:50,251 - INFO - joeynmt.training - Epoch   9, Step:    31300, Batch Loss:     1.567950, Batch Acc: 0.550613, Tokens per Sec:     3278, Lr: 0.000300
2024-05-24 00:47:13,576 - INFO - joeynmt.training - Epoch   9, Step:    31400, Batch Loss:     1.401132, Batch Acc: 0.546666, Tokens per Sec:     3223, Lr: 0.000300
2024-05-24 00:47:36,680 - INFO - joeynmt.training - Epoch   9, Step:    31500, Batch Loss:     1.561917, Batch Acc: 0.546040, Tokens per Sec:     3191, Lr: 0.000300
2024-05-24 00:47:36,680 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:47:36,680 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:48:49,320 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.49, generation: 72.6316[sec], evaluation: 0.0000[sec]
2024-05-24 00:48:49,323 - INFO - joeynmt.training - Example #0
2024-05-24 00:48:49,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:48:49,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:48:49,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'in', 'den', 'meisten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'von', 'ger@@', 'ing@@', 'eren', '4@@', '8', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien so dass die arktische Eis in den meisten drei Millionen Jahre lang die meisten letzten drei Millionen Jahre die Größe von geringeren 48 Prozent.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - Example #1
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'ick@@', 's@@', 'al', 'dieses', 'Problem', 'nicht', 'mehr', 'zeig@@', 't.', '</s>']
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied ist die ernsthicksal dieses Problem nicht mehr zeigt.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - Example #2
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'et@@', 'e,', 'die', 'schl@@', 'ä@@', 'gt', 'Herz@@', 'ens', 'des', 'glob@@', 'alen', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'system.', '</s>']
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - 	Hypothesis: Die arktische Eiszete, die schlägt Herzens des globalen Klimasystems des Klimasystems des Klimasystem.
2024-05-24 00:48:49,324 - INFO - joeynmt.training - Example #3
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:48:49,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'auch', 'die', 'w@@', 'äch@@', 'st', 'und', 'Ver@@', 'fol@@', 'ge', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge.', '</s>']
2024-05-24 00:48:49,325 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:48:49,325 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:48:49,325 - INFO - joeynmt.training - 	Hypothesis: Es gibt auch die wächst und Verfolge in den Sommer und Verträge.
2024-05-24 00:48:49,325 - INFO - joeynmt.training - Example #4
2024-05-24 00:48:49,325 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:48:49,325 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:48:49,325 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Di@@', 'a', 'zeigt', 'Ihnen', 'eine', 'R@@', 'is@@', 'ik@@', 'd', 'schn@@', 'ell@@', '-@@', 'Wal@@', 'd', 'ist', 'der', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'wird.', '</s>']
2024-05-24 00:48:49,325 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:48:49,325 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:48:49,325 - INFO - joeynmt.training - 	Hypothesis: Der nächste Dia zeigt Ihnen eine Risikd schnell-Wald ist der letzten 25 Jahren geschehen wird.
2024-05-24 00:49:13,527 - INFO - joeynmt.training - Epoch   9, Step:    31600, Batch Loss:     1.771021, Batch Acc: 0.545657, Tokens per Sec:     3092, Lr: 0.000300
2024-05-24 00:49:35,969 - INFO - joeynmt.training - Epoch   9, Step:    31700, Batch Loss:     1.727648, Batch Acc: 0.540476, Tokens per Sec:     3187, Lr: 0.000300
2024-05-24 00:49:58,772 - INFO - joeynmt.training - Epoch   9, Step:    31800, Batch Loss:     1.570347, Batch Acc: 0.541777, Tokens per Sec:     3270, Lr: 0.000300
2024-05-24 00:50:22,223 - INFO - joeynmt.training - Epoch   9, Step:    31900, Batch Loss:     1.792738, Batch Acc: 0.538326, Tokens per Sec:     3126, Lr: 0.000300
2024-05-24 00:50:44,278 - INFO - joeynmt.training - Epoch   9, Step:    32000, Batch Loss:     1.601016, Batch Acc: 0.540625, Tokens per Sec:     3211, Lr: 0.000300
2024-05-24 00:50:44,279 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:50:44,279 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:52:04,514 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.35, acc:   0.49, generation: 80.2275[sec], evaluation: 0.0000[sec]
2024-05-24 00:52:04,864 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/29000.ckpt
2024-05-24 00:52:05,054 - INFO - joeynmt.training - Example #0
2024-05-24 00:52:05,054 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:52:05,054 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:52:05,054 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die Arktische Eis gezeigt habe, dass die Arktis für die meisten letzten drei Millionen Jahren die Größe 48 Staaten, die Größe 48 Staaten, die von 40 Prozent.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - Example #1
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'Ver@@', 'trau@@', 't@@', 'ungs@@', 'z@@', 'ept', 'dieser', 'spezi@@', 'ellen', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'enk@@', 'enk@@', 'el', 'der', 'E@@', 'is@@', 'en@@', 'heit', 'zeig@@', 't.', '</s>']
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser Vertrautungszept dieser speziellen Problem ist, weil es nicht die Denkenkel der Eisenheit zeigt.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - Example #2
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'la@@', 'g', 'ist,', 'in', 'einem', 'Sin@@', 'n', 'des', 'Klima@@', 'system@@', 's.', '</s>']
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisschlag ist, in einem Sinn des Klimasystems.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - Example #3
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:52:05,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'w@@', 'äch@@', 'st', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'in', 'der', 'S@@', 'omm@@', 'er@@', 'ei@@', '.', '</s>']
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:52:05,055 - INFO - joeynmt.training - 	Hypothesis: Es wächst sich in Winter und Kontrakte in der Sommerei.
2024-05-24 00:52:05,056 - INFO - joeynmt.training - Example #4
2024-05-24 00:52:05,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:52:05,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:52:05,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'schnell', 'schnell', 'schnell', 'die', 'Ver@@', 'w@@', 'and@@', 'l@@', 'eit@@', 'ung', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-24 00:52:05,056 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:52:05,056 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:52:05,056 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen eine schnell schnell schnell die Verwandleitung von dem, was in den letzten 25 Jahren passiert.
2024-05-24 00:52:28,917 - INFO - joeynmt.training - Epoch   9, Step:    32100, Batch Loss:     1.894642, Batch Acc: 0.533812, Tokens per Sec:     2982, Lr: 0.000300
2024-05-24 00:52:52,054 - INFO - joeynmt.training - Epoch   9, Step:    32200, Batch Loss:     1.717316, Batch Acc: 0.535835, Tokens per Sec:     3321, Lr: 0.000300
2024-05-24 00:53:15,721 - INFO - joeynmt.training - Epoch   9, Step:    32300, Batch Loss:     1.896267, Batch Acc: 0.533830, Tokens per Sec:     3073, Lr: 0.000300
2024-05-24 00:53:38,424 - INFO - joeynmt.training - Epoch   9, Step:    32400, Batch Loss:     1.572302, Batch Acc: 0.534911, Tokens per Sec:     3289, Lr: 0.000300
2024-05-24 00:54:00,761 - INFO - joeynmt.training - Epoch   9, Step:    32500, Batch Loss:     1.634563, Batch Acc: 0.535507, Tokens per Sec:     3293, Lr: 0.000300
2024-05-24 00:54:00,762 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:54:00,762 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:55:27,496 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.33, acc:   0.49, generation: 86.7262[sec], evaluation: 0.0000[sec]
2024-05-24 00:55:27,761 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/29500.ckpt
2024-05-24 00:55:27,912 - INFO - joeynmt.training - Example #0
2024-05-24 00:55:27,912 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:55:27,912 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:55:27,912 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'e@@', 'di@@', 'en', 'für', 'die', 'meisten', 'letzten', 'drei', 'Millionen', 'Jahre', 'gew@@', 'esen', 'ist,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Prozent', 'ist.', '</s>']
2024-05-24 00:55:27,912 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:55:27,912 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:55:27,912 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Eisschedien für die meisten letzten drei Millionen Jahre gewesen ist, die Größe 48 Staaten, die Shrunk von 40 Prozent ist.
2024-05-24 00:55:27,912 - INFO - joeynmt.training - Example #1
2024-05-24 00:55:27,912 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:55:27,912 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:55:27,912 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'stat@@', 't@@', 'dessen', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'ick@@', 'te', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'des', 'E@@', 'is@@', 'es', 'zeigen.', '</s>']
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstattdessen ist die ernsthickte Problem, weil es nicht die Dickheit des Eises zeigen.
2024-05-24 00:55:27,913 - INFO - joeynmt.training - Example #2
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'e@@', 'f', 'in', 'einem', 'Sinn@@', 'e,', 'der', 'Sch@@', 'ei@@', 't,', 'das', 'schl@@', 'ä@@', 'gt', 'Herz@@', '-@@', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisschef in einem Sinne, der Scheit, das schlägt Herz-des Klimasystems
2024-05-24 00:55:27,913 - INFO - joeynmt.training - Example #3
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'w@@', 'äch@@', 'st', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:55:27,913 - INFO - joeynmt.training - 	Hypothesis: Es wächst sich in Winter und Kontrakte im Sommer
2024-05-24 00:55:27,913 - INFO - joeynmt.training - Example #4
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:55:27,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Di@@', 'a', 'zei@@', 'ge', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'R@@', 'oll@@', 'en@@', '-', 'der', 'letzten', '25', 'Jahre', 'passier@@', 'te.', '</s>']
2024-05-24 00:55:27,914 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:55:27,914 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:55:27,914 - INFO - joeynmt.training - 	Hypothesis: Der nächste Dia zeige zeige zeige ich Ihnen eine Rollen- der letzten 25 Jahre passierte.
2024-05-24 00:55:50,784 - INFO - joeynmt.training - Epoch   9, Step:    32600, Batch Loss:     1.586590, Batch Acc: 0.534317, Tokens per Sec:     3087, Lr: 0.000300
2024-05-24 00:56:13,301 - INFO - joeynmt.training - Epoch   9, Step:    32700, Batch Loss:     1.747564, Batch Acc: 0.536073, Tokens per Sec:     3336, Lr: 0.000300
2024-05-24 00:56:34,942 - INFO - joeynmt.training - Epoch   9, Step:    32800, Batch Loss:     1.684574, Batch Acc: 0.533119, Tokens per Sec:     3300, Lr: 0.000300
2024-05-24 00:56:56,630 - INFO - joeynmt.training - Epoch   9, Step:    32900, Batch Loss:     1.771903, Batch Acc: 0.535701, Tokens per Sec:     3441, Lr: 0.000300
2024-05-24 00:57:20,202 - INFO - joeynmt.training - Epoch   9, Step:    33000, Batch Loss:     1.683414, Batch Acc: 0.533482, Tokens per Sec:     3144, Lr: 0.000300
2024-05-24 00:57:20,202 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 00:57:20,202 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 00:58:35,142 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.31, acc:   0.49, generation: 74.9323[sec], evaluation: 0.0000[sec]
2024-05-24 00:58:35,391 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/32000.ckpt
2024-05-24 00:58:35,556 - INFO - joeynmt.training - Example #0
2024-05-24 00:58:35,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 00:58:35,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 00:58:35,556 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'K@@', 'ä@@', 'se', 'in', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', 'von', 'n@@', 'ie@@', 'dri@@', 'ger', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'hat', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 00:58:35,556 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 00:58:35,556 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 00:58:35,556 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die arktische Käse in der letzten drei Millionen Jahre lang die Größe von niedriger 48 Staaten, die Größe 48 Staaten, hat Shrunk von 40 Prozent.
2024-05-24 00:58:35,556 - INFO - joeynmt.training - Example #1
2024-05-24 00:58:35,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 00:58:35,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 00:58:35,556 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'stat@@', 'te', 'ist', 'die', 'ern@@', 'st', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'dieses', 'Problem', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'zeigen.', '</s>']
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstatte ist die ernst dieses speziellen Problems dieses Problem weil es nicht die Dickheit der Eis zeigen.
2024-05-24 00:58:35,557 - INFO - joeynmt.training - Example #2
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'ist', 'in', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist ist in gewissem Sinne des Klimasystems des Klimasystems des Klimasystems
2024-05-24 00:58:35,557 - INFO - joeynmt.training - Example #3
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'die', 'W@@', 'ind', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 00:58:35,557 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich die Wind und Kontrakte im Sommer
2024-05-24 00:58:35,557 - INFO - joeynmt.training - Example #4
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 00:58:35,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'R@@', 'is@@', 'ik@@', 'd', 'schn@@', 'ell@@', 'es', 'über', 'die', 'letzten', '25', 'Jahre', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-24 00:58:35,558 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 00:58:35,558 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 00:58:35,558 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie zeige zeige ich Ihnen ein Risikd schnelles über die letzten 25 Jahre geschehen ist.
2024-05-24 00:59:00,261 - INFO - joeynmt.training - Epoch   9, Step:    33100, Batch Loss:     1.679777, Batch Acc: 0.533772, Tokens per Sec:     2986, Lr: 0.000300
2024-05-24 00:59:24,052 - INFO - joeynmt.training - Epoch   9, Step:    33200, Batch Loss:     1.653300, Batch Acc: 0.539598, Tokens per Sec:     3123, Lr: 0.000300
2024-05-24 00:59:46,671 - INFO - joeynmt.training - Epoch   9, Step:    33300, Batch Loss:     1.693091, Batch Acc: 0.532860, Tokens per Sec:     3341, Lr: 0.000300
2024-05-24 01:00:09,667 - INFO - joeynmt.training - Epoch   9, Step:    33400, Batch Loss:     1.709084, Batch Acc: 0.537517, Tokens per Sec:     3186, Lr: 0.000300
2024-05-24 01:00:32,704 - INFO - joeynmt.training - Epoch   9, Step:    33500, Batch Loss:     1.738014, Batch Acc: 0.534181, Tokens per Sec:     3093, Lr: 0.000300
2024-05-24 01:00:32,705 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:00:32,705 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:01:51,945 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.27, acc:   0.49, generation: 79.2328[sec], evaluation: 0.0000[sec]
2024-05-24 01:01:51,946 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 01:01:52,146 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/32500.ckpt
2024-05-24 01:01:52,180 - INFO - joeynmt.training - Example #0
2024-05-24 01:01:52,180 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:01:52,180 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:01:52,180 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'K@@', 'raft', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 01:01:52,180 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:01:52,180 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:01:52,180 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Kraft der letzten drei Millionen Jahren die Größe der letzten drei Millionen Jahren die Größe 48 Staaten, die Größe 48 Staaten, Shrunk von 40 Prozent.
2024-05-24 01:01:52,180 - INFO - joeynmt.training - Example #1
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'des', 'Aus@@', 'ma@@', 'ß', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'zeigen.', '</s>']
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied des Ausmaß dieses Problem ist, weil es nicht die Dickheit der Eis zeigen.
2024-05-24 01:01:52,181 - INFO - joeynmt.training - Example #2
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'e,', 'in', 'einem', 'Sinn@@', 'e,', 'das', 'schl@@', 'us@@', 's@@', 'fol@@', 'g@@', 'ung', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eissche, in einem Sinne, das schlussfolgung des Klimasystems
2024-05-24 01:01:52,181 - INFO - joeynmt.training - Example #3
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:01:52,181 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich im Sommer und Kontrakte im Sommer
2024-05-24 01:01:52,181 - INFO - joeynmt.training - Example #4
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:01:52,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'ein', 'R@@', 'ap@@', 'i@@', 'd', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-24 01:01:52,182 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:01:52,182 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:01:52,182 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen ein Rapid schnelles schnelles in den letzten 25 Jahren passiert ist.
2024-05-24 01:02:16,941 - INFO - joeynmt.training - Epoch   9, Step:    33600, Batch Loss:     1.609434, Batch Acc: 0.532514, Tokens per Sec:     2888, Lr: 0.000300
2024-05-24 01:02:39,754 - INFO - joeynmt.training - Epoch   9, Step:    33700, Batch Loss:     1.589985, Batch Acc: 0.533860, Tokens per Sec:     3267, Lr: 0.000300
2024-05-24 01:03:02,598 - INFO - joeynmt.training - Epoch   9, Step:    33800, Batch Loss:     1.911932, Batch Acc: 0.533148, Tokens per Sec:     3244, Lr: 0.000300
2024-05-24 01:03:25,248 - INFO - joeynmt.training - Epoch   9, Step:    33900, Batch Loss:     1.584289, Batch Acc: 0.528929, Tokens per Sec:     3243, Lr: 0.000300
2024-05-24 01:03:47,958 - INFO - joeynmt.training - Epoch   9, Step:    34000, Batch Loss:     1.682070, Batch Acc: 0.531014, Tokens per Sec:     3229, Lr: 0.000300
2024-05-24 01:03:47,959 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:03:47,959 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:05:10,669 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.21, acc:   0.50, generation: 82.7021[sec], evaluation: 0.0000[sec]
2024-05-24 01:05:10,670 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 01:05:10,943 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/30000.ckpt
2024-05-24 01:05:11,143 - INFO - joeynmt.training - Example #0
2024-05-24 01:05:11,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:05:11,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:05:11,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ver@@', 't@@', 'et,', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is', 'in', 'den', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'und', 'S@@', 'hr@@', 'un@@', 'k', 'durch', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien vertet, dass die arktischen Eis in den meisten der letzten drei Millionen Jahre die Größe 48 Staaten die Größe 48 Staaten und Shrunk durch 40 Prozent.
2024-05-24 01:05:11,144 - INFO - joeynmt.training - Example #1
2024-05-24 01:05:11,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:05:11,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:05:11,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'ist', 'die', 'Ern@@', 'st', 'dieses', 'Problem', 'dieser', 'bestimm@@', 'te', 'Problem@@', ',', 'weil', 'es', 'nicht', 'die', 'd@@', 'ün@@', 'ne', 'des', 'E@@', 'is@@', 'es', 'zeigen.', '</s>']
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied ist die Ernst dieses Problem dieser bestimmte Problem, weil es nicht die dünne des Eises zeigen.
2024-05-24 01:05:11,144 - INFO - joeynmt.training - Example #2
2024-05-24 01:05:11,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:05:11,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:05:11,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'E@@', 'is@@', 'sch@@', ',', 'in', 'einem', 'Sinn@@', ',', 'das', 'schl@@', 'ag@@', 'ende', 'Herz@@', 'en.', '</s>']
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:05:11,144 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:05:11,145 - INFO - joeynmt.training - 	Hypothesis: Die Arktis Eissch, in einem Sinn, das schlagende Herzen.
2024-05-24 01:05:11,145 - INFO - joeynmt.training - Example #3
2024-05-24 01:05:11,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:05:11,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:05:11,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'im', 'S@@', 'omm@@', 'er@@', 'hö@@', 'h@@', 't', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er@@', 'in.', '</s>']
2024-05-24 01:05:11,145 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:05:11,145 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:05:11,145 - INFO - joeynmt.training - 	Hypothesis: Es gibt es im Sommerhöht und Kontrakte im Sommerin.
2024-05-24 01:05:11,145 - INFO - joeynmt.training - Example #4
2024-05-24 01:05:11,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:05:11,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:05:11,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'Ihnen', 'eine', 'R@@', 'is@@', 'ik@@', 'd', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'ist', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-24 01:05:11,145 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:05:11,145 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:05:11,145 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie zeige zeige Ihnen eine Risikd schnelles schnelles ist in den letzten 25 Jahren passiert.
2024-05-24 01:05:34,677 - INFO - joeynmt.training - Epoch   9, Step:    34100, Batch Loss:     1.554857, Batch Acc: 0.531404, Tokens per Sec:     3077, Lr: 0.000300
2024-05-24 01:05:57,385 - INFO - joeynmt.training - Epoch   9, Step:    34200, Batch Loss:     1.686059, Batch Acc: 0.527855, Tokens per Sec:     3342, Lr: 0.000300
2024-05-24 01:06:20,393 - INFO - joeynmt.training - Epoch   9, Step:    34300, Batch Loss:     1.890718, Batch Acc: 0.532390, Tokens per Sec:     3148, Lr: 0.000300
2024-05-24 01:06:42,826 - INFO - joeynmt.training - Epoch   9, Step:    34400, Batch Loss:     1.805348, Batch Acc: 0.528193, Tokens per Sec:     3218, Lr: 0.000300
2024-05-24 01:07:05,567 - INFO - joeynmt.training - Epoch   9, Step:    34500, Batch Loss:     1.568876, Batch Acc: 0.532450, Tokens per Sec:     3234, Lr: 0.000300
2024-05-24 01:07:05,568 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:07:05,568 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:08:10,296 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.20, acc:   0.50, generation: 64.7208[sec], evaluation: 0.0000[sec]
2024-05-24 01:08:10,297 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 01:08:10,523 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/30500.ckpt
2024-05-24 01:08:10,748 - INFO - joeynmt.training - Example #0
2024-05-24 01:08:10,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:08:10,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:08:10,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'so', 'dass', 'die', 'ar@@', 'kt@@', 'ischen', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'die', 'die', 'meisten', 'dieser', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'letzten', 'drei', 'Millionen', 'Jahre', 'alt', 'war,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'und', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Prozent', 'ist.', '</s>']
2024-05-24 01:08:10,748 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:08:10,748 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:08:10,748 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien so dass die arktischen Eiskap, die die meisten dieser drei Millionen Jahre lang die letzten drei Millionen Jahre alt war, die Größe 48 Staaten und Shrunk von 40 Prozent ist.
2024-05-24 01:08:10,748 - INFO - joeynmt.training - Example #1
2024-05-24 01:08:10,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'Ver@@', 'stän@@', 'dn@@', 'is', 'dieses', 'Problem', 'der', 'S@@', 'eri@@', 'en@@', 'heit', 'des', 'Problem@@', 's', 'zu', 'zeigen.', '</s>']
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser Verständnis dieses Problem der Serienheit des Problems zu zeigen.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - Example #2
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'Ar@@', 'kt@@', 'is', 'E@@', 'is@@', 'sch@@', ',', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'wandel@@', 's.', '</s>']
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Hypothesis: Der Arktis Eissch, in einem Sinn des globalen Klimawandels.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - Example #3
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'er@@', 'weiter@@', 't', 'sich', 'im', 'W@@', 'inter@@', 'inter@@', 'grund', 'und', 'Ver@@', 'fol@@', 'g@@', 's@@', 'ä@@', 'tt@@', 'e.', '</s>']
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - 	Hypothesis: Es erweitert sich im Winterintergrund und Verfolgsätte.
2024-05-24 01:08:10,749 - INFO - joeynmt.training - Example #4
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:08:10,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fo@@', 'li@@', 'en', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'R@@', 'is@@', 'ik@@', 'd', 'schn@@', 'eller', 'zu', 'sein,', 'was', 'über', 'die', 'letzten', '25', 'Jahre', 'passier@@', 't.', '</s>']
2024-05-24 01:08:10,750 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:08:10,750 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:08:10,750 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folien zeige ich Ihnen eine Risikd schneller zu sein, was über die letzten 25 Jahre passiert.
2024-05-24 01:08:34,807 - INFO - joeynmt.training - Epoch   9, Step:    34600, Batch Loss:     1.729103, Batch Acc: 0.530596, Tokens per Sec:     2977, Lr: 0.000300
2024-05-24 01:08:57,137 - INFO - joeynmt.training - Epoch   9, Step:    34700, Batch Loss:     1.544626, Batch Acc: 0.533420, Tokens per Sec:     3297, Lr: 0.000300
2024-05-24 01:09:21,399 - INFO - joeynmt.training - Epoch   9, Step:    34800, Batch Loss:     1.659213, Batch Acc: 0.524580, Tokens per Sec:     3042, Lr: 0.000300
2024-05-24 01:09:43,244 - INFO - joeynmt.training - Epoch   9: total training loss 6406.59
2024-05-24 01:09:43,245 - INFO - joeynmt.training - EPOCH 10
2024-05-24 01:09:44,607 - INFO - joeynmt.training - Epoch  10, Step:    34900, Batch Loss:     1.588777, Batch Acc: 0.538056, Tokens per Sec:     2924, Lr: 0.000300
2024-05-24 01:10:06,943 - INFO - joeynmt.training - Epoch  10, Step:    35000, Batch Loss:     1.687957, Batch Acc: 0.559845, Tokens per Sec:     3258, Lr: 0.000300
2024-05-24 01:10:06,944 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:10:06,944 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:11:17,464 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.21, acc:   0.50, generation: 70.5120[sec], evaluation: 0.0000[sec]
2024-05-24 01:11:17,830 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/33000.ckpt
2024-05-24 01:11:17,855 - INFO - joeynmt.training - Example #0
2024-05-24 01:11:17,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:11:17,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:11:17,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Prozent', 'war.', '</s>']
2024-05-24 01:11:17,855 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:11:17,855 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:11:17,855 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Eis der letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre der Größe 48 Staaten, die Größe 48 Staaten, die Shrunk von 40 Prozent war.
2024-05-24 01:11:17,855 - INFO - joeynmt.training - Example #1
2024-05-24 01:11:17,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:11:17,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:11:17,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'schied', 'dieser', 'bestimm@@', 'ten', 'Problem@@', 'e,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'des', 'E@@', 'is@@', 's', 'zeigen,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'des', 'E@@', 'is@@', 'es', 'zeigen.', '</s>']
2024-05-24 01:11:17,855 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:11:17,855 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterschied dieser bestimmten Probleme, weil es nicht die Dickheit des Eiss zeigen, weil es nicht die Dickheit des Eises zeigen.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - Example #2
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is', 'ist', 'im', 'Sinn@@', 'e,', 'der', 'Sch@@', 'ick@@', 's@@', 'al', 'des', 'Klima@@', 'system@@', 's.', '</s>']
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis ist im Sinne, der Schicksal des Klimasystems.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - Example #3
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'sich', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'tra@@', 'g', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'sich', 'auf', 'W@@', 'ind', 'aus.', '</s>']
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Hypothesis: Es gibt es sich im Sommer und Vertrag im Sommer und Kontrakte im Sommer und Kontrakte im Sommer und Kontrakte im Sommer und Kontrakte im Sommer und Kontrakte sich auf Wind aus.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - Example #4
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:11:17,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'schn@@', 'ell@@', '-@@', 'Vor@@', 'wär@@', 'ts', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:11:17,856 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen eine schnell-Vorwärts von dem, was in den letzten 25 Jahren geschehen ist.
2024-05-24 01:11:42,291 - INFO - joeynmt.training - Epoch  10, Step:    35100, Batch Loss:     1.550889, Batch Acc: 0.559535, Tokens per Sec:     2929, Lr: 0.000300
2024-05-24 01:12:04,878 - INFO - joeynmt.training - Epoch  10, Step:    35200, Batch Loss:     1.453755, Batch Acc: 0.558839, Tokens per Sec:     3314, Lr: 0.000300
2024-05-24 01:12:27,993 - INFO - joeynmt.training - Epoch  10, Step:    35300, Batch Loss:     1.631959, Batch Acc: 0.550937, Tokens per Sec:     3133, Lr: 0.000300
2024-05-24 01:12:50,441 - INFO - joeynmt.training - Epoch  10, Step:    35400, Batch Loss:     1.549984, Batch Acc: 0.553384, Tokens per Sec:     3277, Lr: 0.000300
2024-05-24 01:13:13,336 - INFO - joeynmt.training - Epoch  10, Step:    35500, Batch Loss:     1.566173, Batch Acc: 0.553884, Tokens per Sec:     3264, Lr: 0.000300
2024-05-24 01:13:13,336 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:13:13,336 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:14:24,286 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.28, acc:   0.50, generation: 70.9429[sec], evaluation: 0.0000[sec]
2024-05-24 01:14:24,532 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/31000.ckpt
2024-05-24 01:14:24,701 - INFO - joeynmt.training - Example #0
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'zeig@@', 'te,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'in', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'größ@@', 'eren', '4@@', '8', 'Sta@@', 'aten', 'und', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien zeigte, dass die arktische Eis in der letzten drei Millionen Jahren die Größe der letzten drei Millionen Jahren die Größe der größeren 48 Staaten und Shrunk von 40 Prozent.
2024-05-24 01:14:24,702 - INFO - joeynmt.training - Example #1
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'tr@@', 'it@@', 't', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem@@', 's', 'weil', 'es', 'nicht', 'so', 'so', 'wie', 'das', 'E@@', 'is@@', 'es', 'zeig@@', 't.', '</s>']
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Hypothesis: Aber diese Untertritt ist die ernsthaft dieses Problems weil es nicht so so wie das Eises zeigt.
2024-05-24 01:14:24,702 - INFO - joeynmt.training - Example #2
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:14:24,702 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'c@@', 'a@@', 'p', 'ist', 'in', 'einem', 'Sin@@', 'ne', 'des', 'Klima@@', 'system.', '</s>']
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:14:24,702 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:14:24,703 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiscap ist in einem Sinne des Klimasystem.
2024-05-24 01:14:24,703 - INFO - joeynmt.training - Example #3
2024-05-24 01:14:24,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:14:24,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:14:24,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'in', 'Win@@', 'ter', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-24 01:14:24,703 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:14:24,703 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:14:24,703 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich in Winter und Kontrakte im Sommer und Kontrakte im Sommer.
2024-05-24 01:14:24,703 - INFO - joeynmt.training - Example #4
2024-05-24 01:14:24,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:14:24,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:14:24,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'dass', 'Sie', 'ein', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'schn@@', 'ell@@', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'passier@@', 't.', '</s>']
2024-05-24 01:14:24,703 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:14:24,703 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:14:24,703 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie, die ich Ihnen zeigen werde, dass Sie ein schnelles schnelles schnelles in den letzten 25 Jahren passiert.
2024-05-24 01:14:48,451 - INFO - joeynmt.training - Epoch  10, Step:    35600, Batch Loss:     1.780312, Batch Acc: 0.553823, Tokens per Sec:     3119, Lr: 0.000300
2024-05-24 01:15:11,376 - INFO - joeynmt.training - Epoch  10, Step:    35700, Batch Loss:     1.710192, Batch Acc: 0.550595, Tokens per Sec:     3268, Lr: 0.000300
2024-05-24 01:15:33,867 - INFO - joeynmt.training - Epoch  10, Step:    35800, Batch Loss:     1.635270, Batch Acc: 0.546506, Tokens per Sec:     3224, Lr: 0.000300
2024-05-24 01:15:56,389 - INFO - joeynmt.training - Epoch  10, Step:    35900, Batch Loss:     1.596646, Batch Acc: 0.545650, Tokens per Sec:     3261, Lr: 0.000300
2024-05-24 01:16:19,527 - INFO - joeynmt.training - Epoch  10, Step:    36000, Batch Loss:     1.717888, Batch Acc: 0.542807, Tokens per Sec:     3137, Lr: 0.000300
2024-05-24 01:16:19,528 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:16:19,528 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:17:29,657 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.25, acc:   0.50, generation: 70.1212[sec], evaluation: 0.0000[sec]
2024-05-24 01:17:29,956 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/35500.ckpt
2024-05-24 01:17:29,981 - INFO - joeynmt.helpers - delete /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_bpe_5000/35500.ckpt
2024-05-24 01:17:29,981 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_bpe_5000/35500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_bpe_5000/35500.ckpt')
2024-05-24 01:17:29,981 - INFO - joeynmt.training - Example #0
2024-05-24 01:17:29,981 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:17:29,981 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:17:29,981 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'zeig@@', 'te,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'von', 'der', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'hat', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien zeigte, dass die arktische Eis der letzten drei Millionen Jahre die Größe der letzten drei Millionen Jahre die Größe von der Größe 48 Staaten, hat Shrunk von 40 Prozent.
2024-05-24 01:17:29,982 - INFO - joeynmt.training - Example #1
2024-05-24 01:17:29,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:17:29,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:17:29,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'wä@@', 'hr@@', 'ung', 'der', 'Ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'zeigen.', '</s>']
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterwährung der Ernsthaft dieses Problem ist, weil es nicht die Dickheit der Eis zeigen.
2024-05-24 01:17:29,982 - INFO - joeynmt.training - Example #2
2024-05-24 01:17:29,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:17:29,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:17:29,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', '-@@', 'K@@', 'a@@', 'p', 'ist', 'in', 'einem', 'Sinn@@', ',', 'der', 'B@@', 'ier', 'des', 'Klima@@', 'system.', '</s>']
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:17:29,982 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:17:29,983 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eis-Kap ist in einem Sinn, der Bier des Klimasystem.
2024-05-24 01:17:29,983 - INFO - joeynmt.training - Example #3
2024-05-24 01:17:29,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:17:29,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:17:29,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'trä@@', 'ge', 'im', 'S@@', 'omm@@', 'er', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-24 01:17:29,983 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:17:29,983 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:17:29,983 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verträge im Sommer und Kontrakte im Sommer.
2024-05-24 01:17:29,983 - INFO - joeynmt.training - Example #4
2024-05-24 01:17:29,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:17:29,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:17:29,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Di@@', 'as@@', 'se', 'zei@@', 'ge', 'ich', 'Ihnen', 'Ihnen', 'eine', 'R@@', 'is@@', 'ik@@', 'o', 'des', 'schn@@', 'ell@@', 'es', 'über', 'den', 'letzten', '25', 'Jah@@', 're.', '</s>']
2024-05-24 01:17:29,983 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:17:29,983 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:17:29,983 - INFO - joeynmt.training - 	Hypothesis: Der nächste Diasse zeige ich Ihnen Ihnen eine Risiko des schnelles über den letzten 25 Jahre.
2024-05-24 01:17:53,499 - INFO - joeynmt.training - Epoch  10, Step:    36100, Batch Loss:     1.670859, Batch Acc: 0.549270, Tokens per Sec:     3050, Lr: 0.000300
2024-05-24 01:18:17,279 - INFO - joeynmt.training - Epoch  10, Step:    36200, Batch Loss:     1.682297, Batch Acc: 0.549152, Tokens per Sec:     3120, Lr: 0.000300
2024-05-24 01:18:39,856 - INFO - joeynmt.training - Epoch  10, Step:    36300, Batch Loss:     1.473794, Batch Acc: 0.543077, Tokens per Sec:     3265, Lr: 0.000300
2024-05-24 01:19:02,333 - INFO - joeynmt.training - Epoch  10, Step:    36400, Batch Loss:     1.574455, Batch Acc: 0.539359, Tokens per Sec:     3292, Lr: 0.000300
2024-05-24 01:19:24,860 - INFO - joeynmt.training - Epoch  10, Step:    36500, Batch Loss:     1.527529, Batch Acc: 0.545403, Tokens per Sec:     3261, Lr: 0.000300
2024-05-24 01:19:24,861 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:19:24,861 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:20:35,705 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.26, acc:   0.50, generation: 70.8368[sec], evaluation: 0.0000[sec]
2024-05-24 01:20:35,944 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/33500.ckpt
2024-05-24 01:20:35,957 - INFO - joeynmt.training - Example #0
2024-05-24 01:20:35,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:20:35,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:20:35,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is', 'in', 'den', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'lang', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'aus@@', 'sieh@@', 't.', '</s>']
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Eis in den meisten der letzten drei Millionen Jahre der Größe der letzten drei Millionen Jahre lang die Größe 48 Staaten aussieht.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - Example #1
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'stütz@@', 'ung', 'des', 'A@@', 'spek@@', 't', 'dieses', 'spezi@@', 'elle', 'Problem@@', 's,', 'weil', 'es', 'nicht', 'den', 'D@@', 'ick@@', 'heit', 'des', 'E@@', 'is@@', 'es', 'zeigen.', '</s>']
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstützung des Aspekt dieses spezielle Problems, weil es nicht den Dickheit des Eises zeigen.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - Example #2
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'z@@', 'et@@', 'ab@@', 'er,', 'der', 'schl@@', 'us@@', 's@@', 'fol@@', 'gen@@', 'des', 'Klima@@', 'system.', '</s>']
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eiszetaber, der schlussfolgendes Klimasystem.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - Example #3
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:20:35,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'Ver@@', 'trä@@', 'ger@@', 'ung', 'im', 'S@@', 'omm@@', 'er.', '</s>']
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:20:35,958 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:20:35,959 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und Verträgerung im Sommer.
2024-05-24 01:20:35,959 - INFO - joeynmt.training - Example #4
2024-05-24 01:20:35,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:20:35,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:20:35,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fo@@', 'li@@', 'e,', 'zei@@', 'ge', 'zei@@', 'ge', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'schn@@', 'elle', 'Geschichte', 'von', 'dem,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.', '</s>']
2024-05-24 01:20:35,959 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:20:35,959 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:20:35,959 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie, zeige zeige zeige zeige ich Ihnen eine schnelle Geschichte von dem, was in den letzten 25 Jahren passiert ist.
2024-05-24 01:20:58,861 - INFO - joeynmt.training - Epoch  10, Step:    36600, Batch Loss:     1.492949, Batch Acc: 0.543074, Tokens per Sec:     3260, Lr: 0.000300
2024-05-24 01:21:21,875 - INFO - joeynmt.training - Epoch  10, Step:    36700, Batch Loss:     1.655648, Batch Acc: 0.547435, Tokens per Sec:     3283, Lr: 0.000300
2024-05-24 01:21:44,133 - INFO - joeynmt.training - Epoch  10, Step:    36800, Batch Loss:     1.683915, Batch Acc: 0.544698, Tokens per Sec:     3246, Lr: 0.000300
2024-05-24 01:22:06,676 - INFO - joeynmt.training - Epoch  10, Step:    36900, Batch Loss:     1.563108, Batch Acc: 0.543275, Tokens per Sec:     3252, Lr: 0.000300
2024-05-24 01:22:31,154 - INFO - joeynmt.training - Epoch  10, Step:    37000, Batch Loss:     1.655794, Batch Acc: 0.544607, Tokens per Sec:     3080, Lr: 0.000300
2024-05-24 01:22:31,155 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:22:31,155 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:23:45,314 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.19, acc:   0.50, generation: 74.1509[sec], evaluation: 0.0000[sec]
2024-05-24 01:23:45,315 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 01:23:45,547 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/36500.ckpt
2024-05-24 01:23:45,564 - INFO - joeynmt.training - Example #0
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeig@@', 'te,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'Ar@@', 'kt@@', 'is', 'in', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'at@@', 'en,', 'die', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 01:23:45,564 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:23:45,564 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:23:45,564 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folie gezeigte, dass die arktische Arktis in der letzten drei Millionen Jahre die Größe der letzten drei Millionen Jahre die Größe 48 Staaten, die Größe 48 Staaten, die Shrunk von 40 Prozent.
2024-05-24 01:23:45,564 - INFO - joeynmt.training - Example #1
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'grund', 'des', 'A@@', 'bl@@', 'as@@', 'sen@@', 's', 'ern@@', 's@@', 'th@@', 'af@@', 't', 'dieses', 'Problem@@', 's', 'zeigt', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'zei@@', 'ge', 'nicht', 'das', 'D@@', 'ick@@', 'heit', 'zeigen.', '</s>']
2024-05-24 01:23:45,564 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:23:45,564 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:23:45,564 - INFO - joeynmt.training - 	Hypothesis: Aber diese Untergrund des Ablassens ernsthaft dieses Problems zeigt weil es nicht die Dickheit zeige nicht das Dickheit zeigen.
2024-05-24 01:23:45,564 - INFO - joeynmt.training - Example #2
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:23:45,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'e@@', 'p@@', 'ft', 'ist,', 'in', 'einem', 'Sin@@', 'n', 'des', 'glob@@', 'alen', 'Klima@@', 'systems', '</s>']
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisschepft ist, in einem Sinn des globalen Klimasystems
2024-05-24 01:23:45,565 - INFO - joeynmt.training - Example #3
2024-05-24 01:23:45,565 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:23:45,565 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:23:45,565 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'es', 'W@@', 'inter@@', 'grund', 'und', 'B@@', 'ah@@', 'n', 'in', 'S@@', 'omm@@', 'er', 'und', 'B@@', 'ah@@', 'n', 'in', 'den', 'S@@', 'omm@@', 'er', 'und', 'B@@', 're@@', 'k@@', 'en.', '</s>']
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Hypothesis: Es gibt es Wintergrund und Bahn in Sommer und Bahn in den Sommer und Breken.
2024-05-24 01:23:45,565 - INFO - joeynmt.training - Example #4
2024-05-24 01:23:45,565 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:23:45,565 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:23:45,565 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'ich', 'Ihnen', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'schn@@', 'elle', 'L@@', 'oh@@', 'n', 'von', 'dem', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:23:45,565 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige ich Ihnen zeige zeige ich Ihnen eine schnelle Lohn von dem letzten 25 Jahren geschehen ist.
2024-05-24 01:24:09,143 - INFO - joeynmt.training - Epoch  10, Step:    37100, Batch Loss:     1.665136, Batch Acc: 0.548260, Tokens per Sec:     3033, Lr: 0.000300
2024-05-24 01:24:33,354 - INFO - joeynmt.training - Epoch  10, Step:    37200, Batch Loss:     1.769972, Batch Acc: 0.541173, Tokens per Sec:     3015, Lr: 0.000300
2024-05-24 01:24:56,522 - INFO - joeynmt.training - Epoch  10, Step:    37300, Batch Loss:     1.643325, Batch Acc: 0.538504, Tokens per Sec:     3192, Lr: 0.000300
2024-05-24 01:25:18,868 - INFO - joeynmt.training - Epoch  10, Step:    37400, Batch Loss:     1.585176, Batch Acc: 0.539572, Tokens per Sec:     3293, Lr: 0.000300
2024-05-24 01:25:41,904 - INFO - joeynmt.training - Epoch  10, Step:    37500, Batch Loss:     1.426538, Batch Acc: 0.543759, Tokens per Sec:     3230, Lr: 0.000300
2024-05-24 01:25:41,905 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:25:41,905 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:26:46,388 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.19, acc:   0.50, generation: 64.4750[sec], evaluation: 0.0000[sec]
2024-05-24 01:26:46,742 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/36000.ckpt
2024-05-24 01:26:46,867 - INFO - joeynmt.training - Example #0
2024-05-24 01:26:46,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:26:46,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:26:46,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'Kap@@', ',', 'die', 'für', 'die', 'meisten', 'drei', 'Millionen', 'Jahre', 'der', 'Größ@@', 'e', 'der', 'n@@', 'ie@@', 'dri@@', 'gen', '4@@', '8', 'Sta@@', 'aten', 'die', 'Größ@@', 'e', '4@@', '8', 'Sta@@', 'aten', 'hat', 'S@@', 'hr@@', 'un@@', 'k', 'von', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folien gezeigt habe, dass die arktische Kap, die für die meisten drei Millionen Jahre der Größe der niedrigen 48 Staaten die Größe 48 Staaten hat Shrunk von 40 Prozent.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - Example #1
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'stat@@', 'tung', 'des', 'Problem@@', 's', 'des', 'Problem@@', 's,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'nicht', 'zeigen', 'ist.', '</s>']
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstattung des Problems des Problems, weil es nicht die Dickheit nicht zeigen ist.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - Example #2
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'sch@@', 'ar@@', 'kt@@', 'ische', 'K@@', 'er@@', 'l@@', 'age', 'des', 'Klima@@', 'wandel@@', 's.', '</s>']
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - 	Hypothesis: Die Arktische Eisscharktische Kerlage des Klimawandels.
2024-05-24 01:26:46,868 - INFO - joeynmt.training - Example #3
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:26:46,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'sich', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ge', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'g@@', 'ung', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'fol@@', 'g@@', 'ung', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'g@@', 'ung', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'g@@', 't.', '</s>']
2024-05-24 01:26:46,869 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:26:46,869 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:26:46,869 - INFO - joeynmt.training - 	Hypothesis: Es gibt sich im Sommer und Verträge im Sommer und Verfolgung im Sommer und Verfolgung im Sommer und Verträgung im Sommer und Verträgt.
2024-05-24 01:26:46,869 - INFO - joeynmt.training - Example #4
2024-05-24 01:26:46,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:26:46,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:26:46,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeigen', 'wer@@', 'de,', 'wird', 'eine', 'schn@@', 'elle', 'Zeit', 'zu', 'sein,', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'i@@', 'eh@@', 't.', '</s>']
2024-05-24 01:26:46,869 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:26:46,869 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:26:46,869 - INFO - joeynmt.training - 	Hypothesis: Der nächste Folie, die ich Ihnen zeigen werde, wird eine schnelle Zeit zu sein, was in den letzten 25 Jahren geschieht.
2024-05-24 01:27:09,900 - INFO - joeynmt.training - Epoch  10, Step:    37600, Batch Loss:     1.582232, Batch Acc: 0.541176, Tokens per Sec:     3105, Lr: 0.000300
2024-05-24 01:27:32,398 - INFO - joeynmt.training - Epoch  10, Step:    37700, Batch Loss:     1.794033, Batch Acc: 0.542554, Tokens per Sec:     3226, Lr: 0.000300
2024-05-24 01:27:54,686 - INFO - joeynmt.training - Epoch  10, Step:    37800, Batch Loss:     1.835053, Batch Acc: 0.540034, Tokens per Sec:     3319, Lr: 0.000300
2024-05-24 01:28:16,694 - INFO - joeynmt.training - Epoch  10, Step:    37900, Batch Loss:     1.620802, Batch Acc: 0.544892, Tokens per Sec:     3262, Lr: 0.000300
2024-05-24 01:28:38,209 - INFO - joeynmt.training - Epoch  10, Step:    38000, Batch Loss:     1.516735, Batch Acc: 0.538359, Tokens per Sec:     3367, Lr: 0.000300
2024-05-24 01:28:38,210 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:28:38,210 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:29:50,034 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.17, acc:   0.50, generation: 71.8170[sec], evaluation: 0.0000[sec]
2024-05-24 01:29:50,036 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 01:29:50,343 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/35000.ckpt
2024-05-24 01:29:50,512 - INFO - joeynmt.training - Example #0
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'zeig@@', 'te', 'ich', 'diese', 'zwei', 'Fol@@', 'ie', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'für', 'die', 'meisten', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahre', 'die', 'L@@', 'en@@', 'ere', '4@@', '8', 'Sta@@', 'aten', 'die', 'S@@', 'at@@', 'z', '40', 'Proz@@', 'ent.', '</s>']
2024-05-24 01:29:50,513 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:29:50,513 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:29:50,513 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr zeigte ich diese zwei Folie gezeigt habe, dass die Arktis für die meisten der letzten drei Millionen Jahre die Größe der letzten drei Millionen Jahre die Lenere 48 Staaten die Satz 40 Prozent.
2024-05-24 01:29:50,513 - INFO - joeynmt.training - Example #1
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'richt', 'der', 'ern@@', 's@@', 'th@@', 'af@@', 'te', 'dieses', 'Problem', 'ist,', 'weil', 'es', 'nicht', 'die', 'D@@', 'ick@@', 'heit', 'der', 'E@@', 'is', 'der', 'E@@', 'is', 'zeigen.', '</s>']
2024-05-24 01:29:50,513 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:29:50,513 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:29:50,513 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterricht der ernsthafte dieses Problem ist, weil es nicht die Dickheit der Eis der Eis zeigen.
2024-05-24 01:29:50,513 - INFO - joeynmt.training - Example #2
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:29:50,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'auf', 'eine', 'Art', 'und', 'Weise', 'ist', 'in', 'einem', 'Sinn@@', ',', 'das', 'Sch@@', 'ei@@', 't,', 'das', 'Sch@@', 'ei@@', 'ß', 'des', 'Klima@@', 'system.', '</s>']
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Hypothesis: Die Arktis auf eine Art und Weise ist in einem Sinn, das Scheit, das Scheiß des Klimasystem.
2024-05-24 01:29:50,514 - INFO - joeynmt.training - Example #3
2024-05-24 01:29:50,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:29:50,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:29:50,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'gibt', 'in', 'Win@@', 'ter', 'und', 'die', 'B@@', 'ör@@', 'se', 'und', 'Kon@@', 'tra@@', 'k@@', 'te', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Hypothesis: Es gibt in Winter und die Börse und Kontrakte im Sommer
2024-05-24 01:29:50,514 - INFO - joeynmt.training - Example #4
2024-05-24 01:29:50,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:29:50,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:29:50,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'nächste', 'Fol@@', 'ie', 'zei@@', 'ge', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'schn@@', 'elle', 'Vor@@', 'stellung', 'der', 'letzten', '25', 'Jahre', 'gesch@@', 'ehen', 'wird.', '</s>']
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:29:50,514 - INFO - joeynmt.training - 	Hypothesis: Die nächste Folie zeige zeige ich Ihnen eine schnelle Vorstellung der letzten 25 Jahre geschehen wird.
2024-05-24 01:30:15,041 - INFO - joeynmt.training - Epoch  10, Step:    38100, Batch Loss:     1.693406, Batch Acc: 0.540818, Tokens per Sec:     3021, Lr: 0.000300
2024-05-24 01:30:36,642 - INFO - joeynmt.training - Epoch  10, Step:    38200, Batch Loss:     1.539863, Batch Acc: 0.535095, Tokens per Sec:     3350, Lr: 0.000300
2024-05-24 01:30:59,798 - INFO - joeynmt.training - Epoch  10, Step:    38300, Batch Loss:     1.604201, Batch Acc: 0.535879, Tokens per Sec:     3231, Lr: 0.000300
2024-05-24 01:31:22,968 - INFO - joeynmt.training - Epoch  10, Step:    38400, Batch Loss:     1.601014, Batch Acc: 0.542319, Tokens per Sec:     3208, Lr: 0.000300
2024-05-24 01:31:45,564 - INFO - joeynmt.training - Epoch  10, Step:    38500, Batch Loss:     1.620356, Batch Acc: 0.536928, Tokens per Sec:     3273, Lr: 0.000300
2024-05-24 01:31:45,565 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:31:45,565 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:32:52,914 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.17, acc:   0.50, generation: 67.3420[sec], evaluation: 0.0000[sec]
2024-05-24 01:32:52,915 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2024-05-24 01:32:53,287 - INFO - joeynmt.helpers - delete ../models/model_bpe_5000/34000.ckpt
2024-05-24 01:32:53,313 - INFO - joeynmt.training - Example #0
2024-05-24 01:32:53,313 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'ast', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'stat@@', 'es,', 'has', 's@@', 'hr@@', 'un@@', 'k', 'by', '40', 'perc@@', 'ent.']
2024-05-24 01:32:53,313 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt,', 'um', 'zu', 'ver@@', 'anschau@@', 'lich@@', 'en,', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e,', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'nd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'hatte,', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'ump@@', 'ft', 'ist.']
2024-05-24 01:32:53,313 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'zwei', 'Fo@@', 'li@@', 'en', 'ge@@', 'zeigt', 'habe,', 'dass', 'die', 'Ar@@', 'kt@@', 'is', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'meisten', 'dieser', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'letzten', 'drei', 'Millionen', 'Jahren', 'die', 'Größ@@', 'e', 'der', 'unter@@', 'en', '4@@', '8', 'Sta@@', 'aten', 'gew@@', 'esen', 'ist.', '</s>']
2024-05-24 01:32:53,313 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that  demonstrate that the arctic ice cap,  which for most of the last three million years  has been the size of the lower 48 states,  has shrunk by 40 percent.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Reference:  Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Hypothesis: Letztes Jahr habe ich diese zwei Folien gezeigt habe, dass die Arktis der letzten drei Millionen Jahren die meisten dieser drei Millionen Jahren die Größe der letzten drei Millionen Jahren die Größe der unteren 48 Staaten gewesen ist.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - Example #1
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'stat@@', 'es', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'particular', 'problem', 'because', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Aber', 'dies', 'drück@@', 't', 'nicht', 'star@@', 'k', 'genug', 'die', 'Ern@@', 's@@', 'th@@', 'af@@', 'tig@@', 'keit', 'dieses', 'spezi@@', 'ellen', 'Problem@@', 's', 'aus,', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zeig@@', 't.']
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Aber', 'diese', 'Unter@@', 'sta@@', 'aten', 'ist', 'die', 'ern@@', 's@@', 'th@@', 'ick@@', 'en@@', 'ne', 'dieses', 'Problem', 'nicht', 'zeigen', 'wird.', '</s>']
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Reference:  Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Hypothesis: Aber diese Unterstaaten ist die ernsthickenne dieses Problem nicht zeigen wird.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - Example #2
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'c@@', 'a@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system.']
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['In', 'gewis@@', 'se@@', 'm', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'ka@@', 'pp@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'y@@', 'st@@', 'em@@', 's.']
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Die', 'Ar@@', 'kt@@', 'is', 'E@@', 'is@@', 'sch@@', 'e@@', 'tik@@', 'el', 'ist,', 'in', 'einem', 'Sinn@@', 'e,', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'des', 'Klima@@', 'systems', '</s>']
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Source:     The arctic ice cap is, in a sense,  the beating heart of the global climate system.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Reference:  In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Hypothesis: Die Arktis Eisschetikel ist, in einem Sinne, das schlagende Herz des Klimasystems
2024-05-24 01:32:53,314 - INFO - joeynmt.training - Example #3
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Sie', 'w@@', 'äch@@', 'st', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'ump@@', 'ft', 'im', 'S@@', 'omm@@', 'er.']
2024-05-24 01:32:53,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Es', 'erwar@@', 'tet', 'Win@@', 'ter', 'und', 'Ver@@', 'trä@@', 'ge', 'im', 'S@@', 'omm@@', 'er', 'und', 'Ver@@', 'trä@@', 'ger', 'im', 'S@@', 'omm@@', 'er', '</s>']
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Reference:  Sie wächst im Winter und schrumpft im Sommer.
2024-05-24 01:32:53,314 - INFO - joeynmt.training - 	Hypothesis: Es erwartet Winter und Verträge im Sommer und Verträger im Sommer
2024-05-24 01:32:53,314 - INFO - joeynmt.training - Example #4
2024-05-24 01:32:53,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'fast@@', '-@@', 'for@@', 'ward', 'of', "what's", 'happened', 'over', 'the', 'last', '25', 'years.']
2024-05-24 01:32:53,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Die', 'nächste', 'Fo@@', 'li@@', 'e,', 'die', 'ich', 'Ihnen', 'zeig@@', 'e,', 'ist', 'eine', 'Zeit@@', 'ra@@', 'ffer@@', 'auf@@', 'nah@@', 'me', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist.']
2024-05-24 01:32:53,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Der', 'nächste', 'Di@@', 'a', 'zei@@', 'ge', 'ich', 'Ihnen', 'eine', 'R@@', 'as@@', 'che', 'schn@@', 'ell@@', 'es', 'in', 'den', 'letzten', '25', 'Jahren', 'gesch@@', 'ehen', 'ist.', '</s>']
2024-05-24 01:32:53,315 - INFO - joeynmt.training - 	Source:     The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.
2024-05-24 01:32:53,315 - INFO - joeynmt.training - 	Reference:  Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2024-05-24 01:32:53,315 - INFO - joeynmt.training - 	Hypothesis: Der nächste Dia zeige ich Ihnen eine Rasche schnelles in den letzten 25 Jahren geschehen ist.
2024-05-24 01:33:16,491 - INFO - joeynmt.training - Epoch  10, Step:    38600, Batch Loss:     1.476479, Batch Acc: 0.539189, Tokens per Sec:     3168, Lr: 0.000300
2024-05-24 01:33:38,929 - INFO - joeynmt.training - Epoch  10, Step:    38700, Batch Loss:     1.656116, Batch Acc: 0.533076, Tokens per Sec:     3287, Lr: 0.000300
2024-05-24 01:33:55,932 - INFO - joeynmt.training - Epoch  10: total training loss 6275.27
2024-05-24 01:33:55,933 - INFO - joeynmt.training - Training ended after  10 epochs.
2024-05-24 01:33:55,933 - INFO - joeynmt.training - Best validation result (greedy) at step    38500:   6.17 ppl.
2024-05-24 01:33:55,944 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-24 01:33:55,994 - INFO - joeynmt.model - Enc-dec model built.
2024-05-24 01:33:56,390 - INFO - joeynmt.helpers - Load model from /Users/janinehindermann/Documents/workarea/mt-exercise-5/models/model_bpe_5000/38500.ckpt.
2024-05-24 01:33:56,393 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4998),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4998),
	loss_function=None)
2024-05-24 01:33:56,396 - INFO - joeynmt.prediction - Decoding on dev set...
2024-05-24 01:33:56,396 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:33:56,396 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:37:29,905 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 213.5016[sec], evaluation: 0.0000[sec]
2024-05-24 01:37:29,907 - INFO - joeynmt.prediction - Translations saved to: /Users/janinehindermann/Documents/workarea/mt-exercise-5/scripts/../models/model_bpe_5000/00038500.hyps.dev.
2024-05-24 01:37:29,907 - INFO - joeynmt.prediction - Decoding on test set...
2024-05-24 01:37:29,907 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-24 01:37:29,907 - INFO - joeynmt.prediction - Predicting 1568 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-24 01:41:42,237 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 252.3174[sec], evaluation: 0.0000[sec]
2024-05-24 01:41:42,240 - INFO - joeynmt.prediction - Translations saved to: /Users/janinehindermann/Documents/workarea/mt-exercise-5/scripts/../models/model_bpe_5000/00038500.hyps.test.
