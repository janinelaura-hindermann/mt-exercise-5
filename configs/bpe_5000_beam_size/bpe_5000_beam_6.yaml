name: bpe_5000
joeynmt_version: 2.0.0


data:
  # take the sampled training data
  train: data_sampled/sampled_train.en-de
  # take the original dev data
  dev: data/dev.en-de
  # take the original test data
  test: data/test.en-de
  dataset_type: plain
  src:
    lang: en
    # bpe
    level: bpe
    # no lowercase is needed
    lowercase: false
    # inspired by exercise 4
    max_sent_length: 100
    # use of "voc_file" recommended for bpe NMT
    # TODO file
    voc_file: data_bpe_5000/joint_vocab_clean.txt
    # inspired by exercise 4
    tokenizer_type: subword-nmt
    # inspired by exercise 4
    tokenizer_cfg:
      pretokenizer: none
      num_merges: 5000
      codes: data_bpe_5000/bpe_code_5000.bpe
  trg:
    lang: de
    # bpe
    level: bpe
    # no lowercase is needed
    lowercase: false
    # inspired by exercise 4
    max_sent_length: 100
    # use of "voc_file" recommended for bpe NMT
    # TODO file
    voc_file: data_bpe_5000/joint_vocab_clean.txt
    # inspired by exercise 4
    tokenizer_type: subword-nmt
    # inspired by exercise 4
    tokenizer_cfg:
      pretokenizer: none
      num_merges: 5000
      codes: data_bpe_5000/bpe_code_5000.bpe

testing:
  # inspired by exercise 4
  beam_size: 6
  alpha: 1.0

training:
  random_seed: 42
  optimizer: adam
  normalization: tokens
  learning_rate: 0.0003
  batch_size: 2048
  batch_type: token
  eval_batch_size: 1024
  eval_batch_type: token
  scheduling: plateau
  patience: 8
  weight_decay: 0.0
  decrease_factor: 0.7
  early_stopping_metric: ppl
  epochs: 10
  validation_freq: 500
  logging_freq: 100
  eval_metric: bleu
  # added model path
  model_dir: ../models/model_bpe_5000
  overwrite: false
  shuffle: true
  use_cuda: false
  max_output_length: 100
  print_valid_sents: [0, 1, 2, 3, 4]
  label_smoothing: 0.3

model:
  initializer: xavier_uniform
  bias_initializer: zeros
  init_gain: 1.0
  embed_initializer: xavier_uniform
  embed_init_gain: 1.0
  tied_embeddings: true
  tied_softmax: true
  encoder:
    type: transformer
    num_layers: 4
    num_heads: 2
    embeddings:
      embedding_dim: 256
      scale: true
      dropout: 0
    hidden_size: 256
    ff_size: 512
    dropout: 0
  decoder:
    type: transformer
    num_layers: 1
    num_heads: 2
    embeddings:
      embedding_dim: 256
      scale: true
      dropout: 0
    hidden_size: 256
    ff_size: 512
    dropout: 0
