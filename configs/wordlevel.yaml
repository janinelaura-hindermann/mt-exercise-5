name: "wordlevel"
joeynmt_version: "2.0.0"


data:
  # take the sampled training data
  train: "data_sampled/sampled_train.en-de"
  # take the original dev data
  dev: "data/dev.en-de"
  # take the original test data
  test: "data/test.en-de"
  dataset_type: "plain"
  src:
    lang: "en"
    # word level
    level: "word"
    # no lowercase is needed
    lowercase: False
    # inspired by exercise 4
    max_sent_length: 100
    # use of "voc_limit" recommended for word-level NMT, take 2000 as mentioned in the exercise
    voc_limit: 2000
    # inspired by exercise 4
    tokenizer_type: "moses"
  trg:
    lang: "de"
    # word level
    level: "word"
    # no lowercase is needed
    lowercase: False
    # inspired by exercise 4
    max_sent_length: 100
    # use of "voc_limit" recommended for word-level NMT, take 2000 as mentioned in the exercise
    voc_limit: 2000
    # inspired by exercise 4
    tokenizer_type: "moses"

testing:
  # inspired by exercise 4
  beam_size: 5
  alpha: 1.0

training:
  random_seed: 42
  optimizer: "adam"
  normalization: "tokens"
  learning_rate: 0.0003
  batch_size: 2048
  batch_type: "token"
  eval_batch_size: 1024
  eval_batch_type: "token"
  scheduling: "plateau"
  patience: 8
  weight_decay: 0.0
  decrease_factor: 0.7
  early_stopping_metric: "ppl"
  epochs: 10
  validation_freq: 500
  logging_freq: 100
  eval_metric: "bleu"
  # added model path
  model_dir: "../models/model_wordlevel"
  overwrite: False
  shuffle: True
  use_cuda: False
  max_output_length: 100
  print_valid_sents: [ 0, 1, 2, 3, 4 ]
  label_smoothing: 0.3

model:
  initializer: "xavier_uniform"
  bias_initializer: "zeros"
  init_gain: 1.0
  embed_initializer: "xavier_uniform"
  embed_init_gain: 1.0
  # word level is recommended to set this to False
  tied_embeddings: False
  tied_softmax: True
  encoder:
    type: "transformer"
    num_layers: 4
    num_heads: 2
    embeddings:
      embedding_dim: 256
      scale: True
      dropout: 0
    hidden_size: 256
    ff_size: 512
    dropout: 0
  decoder:
    type: "transformer"
    num_layers: 1
    num_heads: 2
    embeddings:
      embedding_dim: 256
      scale: True
      dropout: 0
    hidden_size: 256
    ff_size: 512
    dropout: 0
