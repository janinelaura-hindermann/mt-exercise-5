2024-05-23 21:01:44,429 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                           cfg.name : bpe_5000
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                     cfg.data.train : data_bpe_5000/train.en-de
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                       cfg.data.dev : data_bpe_5000/dev.en-de
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                      cfg.data.test : data_bpe_5000/test.en-de
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : data_bpe_5000/joint_vocab_clean.txt
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 5000
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data_bpe_5000/bpe_code_5000.bpe
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : de
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : data_bpe_5000/joint_vocab_clean.txt
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 5000
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data_bpe_5000/bpe_code_5000.bpe
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2024-05-23 21:01:44,430 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -             cfg.training.model_dir : ../models/model_bpe_5000
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2024-05-23 21:01:44,431 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2024-05-23 21:01:44,432 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2024-05-23 21:01:44,434 - INFO - joeynmt.data - Building tokenizer...
2024-05-23 21:01:44,445 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-23 21:01:44,445 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-23 21:01:44,445 - INFO - joeynmt.data - Loading train set...
2024-05-23 21:01:44,624 - INFO - joeynmt.data - Building vocabulary...
2024-05-23 21:01:44,904 - INFO - joeynmt.data - Loading dev set...
2024-05-23 21:01:44,907 - INFO - joeynmt.data - Loading test set...
2024-05-23 21:01:44,911 - INFO - joeynmt.data - Data loaded.
2024-05-23 21:01:44,911 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 21:01:44,911 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=888, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 21:01:44,911 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1568, src_lang=en, trg_lang=de, has_trg=True, random_subset=-1)
2024-05-23 21:01:44,911 - INFO - joeynmt.data - First training example:
	[SRC] A@@ @@@ @ l G@@ @@@ @ or@@ @@@ @ e: A@@ @@@ @ ver@@ @@@ @ ting the climate c@@ @@@ @ ris@@ @@@ @ is
	[TRG] A@@ @@@ @ l G@@ @@@ @ or@@ @@@ @ e: Die Ab@@ @@@ @ wen@@ @@@ @ dung der Klima@@ @@@ @ k@@ @@@ @ at@@ @@@ @ ast@@ @@@ @ rop@@ @@@ @ he
2024-05-23 21:01:44,911 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) !" (6) !@@ (7) " (8) ", (9) ".
2024-05-23 21:01:44,911 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) !" (6) !@@ (7) " (8) ", (9) ".
2024-05-23 21:01:44,912 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4998
2024-05-23 21:01:44,912 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4998
2024-05-23 21:01:44,914 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-23 21:01:45,008 - INFO - joeynmt.model - Enc-dec model built.
2024-05-23 21:01:45,010 - INFO - joeynmt.model - Total params: 4178688
2024-05-23 21:01:45,011 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4998),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4998),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2024-05-23 21:01:45,011 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2024-05-23 21:01:45,011 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2024-05-23 21:01:45,011 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2024-05-23 21:01:45,011 - INFO - joeynmt.training - EPOCH 1
2024-05-23 21:02:15,404 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     2.847211, Batch Acc: 0.356228, Tokens per Sec:     2787, Lr: 0.000300
2024-05-23 21:02:45,858 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.712137, Batch Acc: 0.443742, Tokens per Sec:     2749, Lr: 0.000300
2024-05-23 21:03:16,098 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.679386, Batch Acc: 0.456188, Tokens per Sec:     2691, Lr: 0.000300
2024-05-23 21:03:47,972 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.601613, Batch Acc: 0.455322, Tokens per Sec:     2607, Lr: 0.000300
2024-05-23 21:04:16,508 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.434597, Batch Acc: 0.459009, Tokens per Sec:     2967, Lr: 0.000300
2024-05-23 21:04:16,509 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-23 21:04:16,509 - INFO - joeynmt.prediction - Predicting 888 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
Predicting...:   0%|          | 0/888 [00:00<?, ?it/s]Predicting...:   2%|▏         | 16/888 [00:04<03:57,  3.68it/s]Predicting...:   4%|▍         | 39/888 [00:11<04:02,  3.50it/s]Predicting...:   6%|▋         | 56/888 [00:13<03:00,  4.61it/s]Predicting...:   9%|▊         | 77/888 [00:16<02:40,  5.07it/s]Predicting...:  12%|█▏        | 103/888 [00:20<02:14,  5.84it/s]Predicting...:  14%|█▎        | 120/888 [00:23<02:10,  5.87it/s]Predicting...:  14%|█▍        | 128/888 [00:32<04:04,  3.10it/s]Predicting...:  15%|█▌        | 137/888 [00:40<05:32,  2.26it/s]Predicting...:  17%|█▋        | 153/888 [00:43<04:18,  2.84it/s]Predicting...:  19%|█▉        | 172/888 [00:46<03:24,  3.51it/s]Predicting...:  22%|██▏       | 192/888 [00:48<02:35,  4.47it/s]Predicting...:  23%|██▎       | 204/888 [00:53<03:08,  3.64it/s]Predicting...:  25%|██▍       | 219/888 [00:57<02:58,  3.75it/s]Predicting...:  26%|██▌       | 233/888 [01:04<03:36,  3.03it/s]Predicting...:  28%|██▊       | 246/888 [01:10<03:54,  2.74it/s]Predicting...:  29%|██▉       | 258/888 [01:15<04:01,  2.61it/s]Predicting...:  31%|███       | 271/888 [01:24<04:55,  2.09it/s]Predicting...:  32%|███▏      | 283/888 [01:33<05:25,  1.86it/s]Predicting...:  34%|███▎      | 299/888 [01:37<04:26,  2.21it/s]Predicting...:  35%|███▌      | 313/888 [01:41<03:52,  2.47it/s]Predicting...:  37%|███▋      | 327/888 [01:46<03:29,  2.68it/s]Predicting...:  39%|███▊      | 342/888 [01:59<04:48,  1.90it/s]Predicting...:  40%|████      | 359/888 [02:04<04:01,  2.19it/s]Predicting...:  42%|████▏     | 376/888 [02:08<03:19,  2.56it/s]Predicting...:  44%|████▍     | 395/888 [02:16<03:10,  2.58it/s]Predicting...:  46%|████▌     | 409/888 [02:18<02:40,  2.99it/s]Predicting...:  48%|████▊     | 425/888 [02:22<02:24,  3.21it/s]Predicting...:  49%|████▉     | 435/888 [02:30<03:07,  2.41it/s]Predicting...:  51%|█████▏    | 457/888 [02:34<02:19,  3.08it/s]Predicting...:  53%|█████▎    | 474/888 [02:40<02:17,  3.01it/s]Predicting...:  56%|█████▌    | 493/888 [02:44<01:52,  3.51it/s]Predicting...:  58%|█████▊    | 513/888 [02:50<01:48,  3.46it/s]Predicting...:  59%|█████▉    | 523/888 [02:57<02:13,  2.74it/s]Predicting...:  61%|██████    | 542/888 [03:09<02:40,  2.16it/s]